{"text_id": "8000", "text": "docstring: async def app_delete(self, args=None): if args is None: args = {} config = { 'app_delete': { 'required': 1, 'type': 'integer' }, 'passthrough': {}, 'req_id': { 'type': 'integer' } } all_args = { 'method': 'app_delete', 'needs_method_arg': '1', 'args': args, 'config': config, } return await self.process_request(all_args)"}
{"text_id": "8001", "text": "docstring: def user_save_image_label(file_name, image_path_name, scene_name, labels): return user_label_image_inst.save_label2file(file_name, image_path_name, scene_name, labels)"}
{"text_id": "8002", "text": "docstring: def listen_file(local_ip, local_port, save_position): try: print('listening files') s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) s.bind((local_ip, local_port)) s.listen(10) except socket.error as msg: print('file listener error') print(msg) sys.exit(1) while True: conn, addr = s.accept() t = threading.Thread(target=receive_a_file, args=(conn, addr, save_position)) t.start()"}
{"text_id": "8003", "text": "docstring: def create_tv_service_with_target_media_state(accessory): service = create_tv_service(accessory) tms = service.add_char(CharacteristicsTypes.TARGET_MEDIA_STATE) tms.value = None tms.perms.append(CharacteristicPermissions.paired_write) return service"}
{"text_id": "8004", "text": "docstring: def flatten_merge_history(merge_history): if len(merge_history) > 2: prev_merged_segments = merge_history[-2] for next_merged_segments in merge_history[::-1][2:]: new_merged_segments = [] for s in prev_merged_segments: new_segment = [] for j in s: new_segment.extend(next_merged_segments[j]) new_merged_segments.append(new_segment) prev_merged_segments = new_merged_segments.copy() merge_history_flat = new_merged_segments else: merge_history_flat = merge_history[0] return merge_history_flat"}
{"text_id": "8005", "text": "docstring: def json_to_file(self, file_to_write): json_path = path.join( path.dirname(path.realpath('__file__')), self._config_folder, self._city_list_filename_json ) with open(json_path, 'w') as json_file: json_file.write(file_to_write) return json_path"}
{"text_id": "8006", "text": "docstring: def stru_dict_to_atoms(dct: dict, **kwargs) -> Atoms: return crystal( symbols=list(map(atom_dict_to_atom, dct['atoms'])), cellpar=lat_dict_to_list(dct[\"lattice\"]), spacegroup=dct[\"space_group\"], occupancies=get_occ_list(dct[\"atoms\"]), **kwargs )"}
{"text_id": "8007", "text": "docstring: async def fetch(self, x: int, y: int, z: int) -> bytes: xyz = (x, y, z) tile_ref = self.submit(xyz, speculative=False) tile_ref.speculative = False self.stats.requested += 1 try: return await tile_ref.task finally: self.stats.requested -= 1"}
{"text_id": "8008", "text": "docstring: def testBestBottomLeftRotated(self): M = maximal_rectangles.MaximalRectangle(8, 4, rotation=True, heuristic='bottom_left') I = item.Item(4, 8) M.insert(I) with self.subTest(): self.assertEqual(M.freerects, []) with self.subTest(): self.assertEqual(I.rotated, True)"}
{"text_id": "8009", "text": "docstring: def ignore_exception(ignore=Exception, default_val=None): def dec(function): def _dec(*args, **kwargs): try: return function(*args, **kwargs) except ignore: return default_val return _dec return dec"}
{"text_id": "8010", "text": "docstring: def _generate_logits(self, sz: Optional[tuple] = None, *args, **kwargs) -> np.ndarray: if sz: sz += (self.out_dim,) return np.random.normal(size=sz)"}
{"text_id": "8011", "text": "docstring: def serialize_modulenode_as_function(self, node): name = node.full_module_name.rpartition('.')[-1] cname_py2 = 'init' + name cname_py3 = 'PyInit_' + name py2_attrs = dict( name=name, cname=cname_py2, pf_cname='', qualified_name='', lineno='1', is_initmodule_function=\"True\", ) py3_attrs = dict(py2_attrs, cname=cname_py3) self._serialize_modulenode_as_function(node, py2_attrs) self._serialize_modulenode_as_function(node, py3_attrs)"}
{"text_id": "8012", "text": "docstring: def graph_y_dimensionality(self) -> int: if self.has_graph_y and self.is_disjoint_graph: return self.graph_y.shape[1] elif self.has_graph_y: return self.graph_y.shape[0] else: return 0"}
{"text_id": "8013", "text": "docstring: def predict_labels(self, line: torch.tensor) -> List[Tuple[int, int, int, float]]: o = self.forward(line) return self.decoder(o)"}
{"text_id": "8014", "text": "docstring: def validate_tcp_checksum(length, src, dst, data): csum = _decode_short(data[16:18]) data = list(data) data[16:18] = '\\x00\\x00' if len(data) % 2 != 0: data.append('\\x00') sum = 0 sum += _decode_short(src[0:2]) sum += _decode_short(src[2:4]) sum += _decode_short(dst[0:2]) sum += _decode_short(dst[2:4]) sum += 0x0006 sum += length for i in range(0, len(data), 2): sum += _decode_short(data[i] + data[i+1]) while sum >> 16: sum = (sum & 0xffff) + (sum >> 16) return ~sum & 0xffff == csum"}
{"text_id": "8015", "text": "docstring: async def answers( self, *, data, ids: list = None ) -> List[Union[BiographyAnswer, int, str]]: if ids is None: return data for answer in data: answer[\"question_\"] = LazyLoader(self.questions, answer[\"question\"]) return object_parse(data, BiographyAnswer)"}
{"text_id": "8016", "text": "docstring: def _bundle(self): d = [] for key in self.keys(): if self[key] is None: continue if self.datamodel[key]['otype'] == np.ndarray: tmp = {} if self.datamodel[key]['atype'] == np.floating: tmp[key] = self[key].astype(np.float32) else: tmp[key] = self[key] d.append(tmp) else: d[0][key] = self[key] return d"}
{"text_id": "8017", "text": "docstring: def tweet_trades_from_prior_day(): access_token = get_access_token(env_client_id, env_redirect_uri, env_refresh_token, env_code) trade_data = check_for_recent_trades(env_account_id, access_token) prepared_tweets = create_tweet_list(trade_data) send_tweets(prepared_tweets, env_twitter_key, env_twitter_secret_key, env_twitter_token, env_twitter_secret_token)"}
{"text_id": "8018", "text": "docstring: def assert_true(self, value, failure_message='Expected value to be True, was: {}'): assertion = lambda: bool(value) self.webdriver_assert(assertion, unicode(failure_message).format(value))"}
{"text_id": "8019", "text": "docstring: def Mount(self, mount_point, mount_options_by_format=\"\"): fstab = self.fstab if fstab: p = fstab[mount_point] mount_dict = {} if mount_options_by_format is not None: for option in mount_options_by_format.split(\"|\"): if \"=\" in option: key, value = option.split(\"=\", 1) mount_dict[key] = value mount_flags = mount_dict.get(p.fs_type, \"\") if p.context is not None: mount_flags = p.context + (\",\" + mount_flags if mount_flags else \"\") self.script.append('mount(\"%s\", \"%s\", %s, \"%s\", \"%s\");' % ( p.fs_type, common.PARTITION_TYPES[p.fs_type], self._GetSlotSuffixDeviceForEntry(p), p.mount_point, mount_flags)) self.mounts.add(p.mount_point)"}
{"text_id": "8020", "text": "docstring: def repl(runtime, verbose): cleanup = [] client = get_boto_client() print(\"Start a line with `$ ` to run shell commands, or `>>> ` to run Python\") while True: line = input() response = run_lambda_function( client, runtime, _REPL_CODE.format(line=line), b\"\", cleanup.append, subprocess_kwargs={ \"stdout\": subprocess.DEVNULL, \"stderr\": subprocess.DEVNULL, } if not verbose else {}, ) for line in base64.b64decode(response[\"LogResult\"]).splitlines(): print(line.decode(\"utf8\")) for f in cleanup: f() cleanup = []"}
{"text_id": "8021", "text": "docstring: def seasonal_decomposed(series, periodicity=None, seasonality=1): if periodicity: if (periodicity % 2) == 0: if (seasonality % 2) == 0: stl = STL(series.squeeze(), period=periodicity + 1, seasonal=seasonality + 1) else: stl = STL(series.squeeze(), period=periodicity + 1, seasonal=seasonality) else: if (seasonality % 2) == 0: stl = STL(series.squeeze(), period=periodicity, seasonal=seasonality + 1) else: stl = STL(series.squeeze(), period=periodicity, seasonal=seasonality) else: if (seasonality % 2) == 0: stl = STL(series.squeeze(), seasonal=seasonality + 1) else: stl = STL(series.squeeze(), seasonal=seasonality) res = stl.fit() return series.squeeze()-res.seasonal"}
{"text_id": "8022", "text": "docstring: def streamlit_main(): flow_control = create_flow_control(6) credentials=[st.secrets['uname'], st.secrets['pw']] if st.button(\"Press to Start\") and not any(flow_control): flow_control[0] = True if flow_control[0]: st.markdown(\"\"\"## step1. please enter your question or keyword(s). __Notes__: * Questions should be of more than 4 words and ending with a '?' * Keywords can be any number of words, divided by a space \"\"\") user_input = st.text_input(\"Enter a question or keywords\", \"\") if st.button(\"Confirm\") or user_input: flow_control[1] = True if flow_control[1]: st.warning(\"Downloading most relevant questions from Google.\\n This can take up to 2 minute\") query = streamlit_get_ppa(user_input, credentials=credentials) question_list = query.to_list(\"q\") if question_list: st.success(\"All question retrieved\") st.write(question_list) flow_control[2] = True if flow_control[2]: st.markdown(\"\"\"## Step 2. Get your answers\"\"\") more = st.checkbox(\"Fetch more questions?\") if st.button(\"confirm\"): flow_control[3] = True if more: st.warning(\"Searching for more questions, be patient!\") query=streamlit_get_more_ppa(query, credentials=credentials) st.success(\"New questions retrieved\") st.write(question_list) if flow_control[3]: st.warning(\"Downloading most relevant answers from Google.\\n This can take up to 1 minute\") query = streamlit_get_answers(query, credentials=credentials) if query: flow_control[4] = True if flow_control[4]: st.success(\"Your data is ready\") final_df = query.to_pandas() st.dataframe(final_df) st.write(get_table_download_link(final_df), unsafe_allow_html=True) flow_control[5]=True if flow_control[5]: st.markdown(\"\"\"##Step 3. Get metrics\"\"\") st.warning(\"metrics not implemented yet!\") reboot = st.button(\"Restart with new query\") if reboot: flow_control = None"}
{"text_id": "8023", "text": "docstring: def build_summary_table(self, sample_name, tests, precision, orientation, widths, job): tests = tests.split(',') tests, factor_values = self.test_factors(tests) name_fields = sample_name.split('+') try: report_precision = int(precision) except ValueError: report_precision = 0 table = ResultTable() if widths is not None: table.column_widths = widths.split(',') header_row = [\"Sample\"] for test_name in tests: test_name = test_name.strip() header_row.append(test_name) table.set_columns(header_row) for sample in job.samples: row = [sample.build_name(name_fields)] for test in tests: test = test.strip() if test in sample.test_results: if test in factor_values: result = sample.result_average_ordinal(test, factor_values) val = factor_values[test][result] else: result = sample.result_average(test) val = \"{0}\".format(round(result, report_precision)) if test in sample.test_units: val += sample.test_units[test] row.append(val) else: row.append(\"\") table.add_row(row) if orientation == \"Horizontal\": table.transpose() return table"}
{"text_id": "8024", "text": "docstring: def external_input_vector(self): res = self._flux_vector(self.model.external_inputs) return res"}
{"text_id": "8025", "text": "docstring: def __getUserEmail(iamClient, userName): response = iamClient.get_user(UserName=userName) logger = logging.getLogger('aws-keyrotation') try: foundContact = False contactEmail = '' for tag in response['User']['Tags']: if tag['Key'] == 'Contact': foundContact = True contactEmail = tag['Value'] break if not foundContact: raise KeyError except KeyError: logger.warning('Contact details for user (' + userName + ') not provided!') return contactEmail"}
{"text_id": "8026", "text": "docstring: def StoreInCache(self, self_links): if platforms.OperatingSystem.Current() == platforms.OperatingSystem.WINDOWS: return None paths = {} collection = None for ref in self_links: if not collection: try: instance_ref = resources.REGISTRY.Parse(ref) collection = instance_ref.Collection() except (resources.InvalidResourceException, resources.UnknownFieldException): lst = ref.split('/') collection = lst[3] + '.' + lst[-2] lst = RemoteCompletion.CachePath(ref) path = lst[0] name = lst[1] if path in paths: paths[path].append(name) else: paths[path] = [name] if not collection: return for path in paths: abs_path = os.path.join(self.cache_dir, path) dirname = os.path.dirname(abs_path) try: if not os.path.isdir(dirname): files.MakeDir(dirname) with tempfile.NamedTemporaryFile(dir=dirname, delete=False) as f: f.write('\\n'.join(paths[path])) shutil.move(f.name, abs_path) now = time.time() timeout = RemoteCompletion._TIMEOUTS.get(collection, 300) os.utime(abs_path, (now, now+timeout)) except Exception: return"}
{"text_id": "8027", "text": "docstring: def random_product(items, num=None, rng=None): rng = ensure_rng(rng, 'python') seen = set() items = [list(g) for g in items] max_num = np.prod(np.array(list(map(len, items)))) if num is None: num = max_num if num > max_num: raise ValueError('num exceedes maximum number of products') if num > max_num // 2: for prod in shuffle(list(it.product(*items)), rng=rng): yield prod else: while len(seen) < num: idxs = tuple(rng.randint(0, len(g) - 1) for g in items) if idxs not in seen: seen.add(idxs) prod = tuple(g[x] for g, x in zip(items, idxs)) yield prod"}
{"text_id": "8028", "text": "docstring: def zero_grad(self): arrays = defaultdict(list) for p in self.values(): if p.grad_req == 'null' or p._grad is None: continue for g in p.list_grad(): if g.stype == 'row_sparse': ndarray.zeros_like(g, out=g) else: arrays[g.context].append(g) if len(arrays) == 0: return if is_np_array(): for arr in arrays.values(): for ele in arr: ele[()] = 0 else: for arr in arrays.values(): ndarray.reset_arrays(*arr, num_arrays=len(arr))"}
{"text_id": "8029", "text": "docstring: def update_consecutive_correct(handler_input, correct: bool) -> None: attr = handler_input.attributes_manager.session_attributes consecutive_correct = attr.get('consecutive_correct', 0) consecutive_incorrect = attr.get('consecutive_incorrect', 0) if correct: consecutive_correct += 1 consecutive_incorrect = 0 elif not correct: consecutive_incorrect += 1 consecutive_correct = 0 attr['consecutive_correct'] = consecutive_correct attr['consecutive_incorrect'] = consecutive_incorrect return"}
{"text_id": "8030", "text": "docstring: def update_model_based_indexes(session, flush_context): to_delete = [] to_add = [] for model in session.new: if isinstance(model, Indexable): to_add.append(model) for model in session.dirty: if isinstance(model, Indexable): to_delete.append(model) to_add.append(model) for model in session.dirty: if isinstance(model, Indexable): to_delete.append(model) if not (to_delete or to_add): return writer = index.writer() for model in to_delete: model.remove_from_search_index(writer) for model in to_add: model.add_to_search_index(writer) writer.commit()"}
{"text_id": "8031", "text": "docstring: def parse_rpm_filenames(filename, form='list'): def get_parts(_filename): _filename = _filename.rstrip('.rpm') _file_nt = _filename.rsplit('.', 1)[0] if _filename != _file_nt: tmp = _file_nt.rsplit('-', 1) if len(tmp) > 1: rel = tmp[-1] else: rel = '' _file_nr = tmp[0] tmp = _file_nr.rsplit('-', 1) if len(tmp) > 1: ver = tmp[1].split(':')[-1] if ':' in tmp[-1]: ep = tmp[1].split(':')[0] else: ep = '' else: ep = '' ver = '' basename = tmp[0] else: basename = _file ver = '' rel = '' ep = '' return basename, ep, ver, rel if isinstance(filename, str): return get_parts(filename) elif form == 'list': basename = [] version = [] release = [] epoch = [] for _file in filename: bn, ep, ver, rel = get_parts(_file) basename.append(bn) version.append(ver) release.append(rel) epoch.append(ep) return basename, epoch, version, release elif form == 'dict': _dict = {} for _file in filename: basename, ep, ver, rel = get_parts(_file) if basename not in _dict: _dict[basename] = {} _dict[basename]['ver'] = ver _dict[basename]['rel'] = rel _dict[basename]['ep'] = ep else: if ep > _dict[basename]['ep']: _dict[basename]['ver'] = ver _dict[basename]['rel'] = rel _dict[basename]['ep'] = ep elif rel > _dict[basename]['rel'] and ver == _dict[basename]['ver']: _dict[basename]['rel'] = rel elif ver > _dict[basename]['ver']: _dict[basename]['ver'] = ver _dict[basename]['rel'] = rel return _dict else: epoch = None basename = None version = None return basename, epoch, version, release"}
{"text_id": "8032", "text": "docstring: def anomalyitem(self, anomalyitem: List[AnomalyItem]): self._anomalyitem = anomalyitem"}
{"text_id": "8033", "text": "docstring: def simple_tag(writer, node): name = node.tag_name if writer.env and \\ name not in writer.env.filters and \\ name not in writer._filters_warned: writer._filters_warned.add(name) writer.warn('Filter %s probably doesn\\'t exist in Jinja' % name) if not node.vars_to_resolve: writer.start_variable() writer.write('request|') writer.write(name) writer.end_variable() return first_var = node.vars_to_resolve[0] args = node.vars_to_resolve[1:] writer.start_variable() writer.node(first_var) writer.write('|') writer.write(name) if args: writer.write('(') for idx, var in enumerate(args): if idx: writer.write(', ') if var.var: writer.node(var) else: writer.literal(var.literal) writer.write(')') writer.end_variable()"}
{"text_id": "8034", "text": "docstring: def createNewConfigFile(configFileLoc): if os.path.exists(configFileLoc): raise FileExistsError(\"Cannot create a new config file as file already exists at {}\".format(configFileLoc)) with open(configFileLoc, 'w') as f: yaml.safe_dump(Controller.DEFAULT_CONFIG, f)"}
{"text_id": "8035", "text": "docstring: def absorbance_read(self, integration_time, scans_to_average, filter=0, normalized=False): if not isinstance(scans_to_average, int): raise ValueError('Please pass an integer number of scans to average') if not isinstance(filter, int): raise ValueError('Please pass an integer number for filter') if len(self.dark_intensities) == 0: raise ValueError(\"Please save dark values\") if len(self.blank_intensities) == 0: raise ValueError(\"Please save blank values\") if not self.spectrometer_on: raise NameError(\"Spectrometer not connected\") self.ocean_optics.integration_time_micros(integration_time) wavelengths = self.ocean_optics.wavelengths() wavelengths_splice = np.array(wavelengths[filter:]) blank = np.array(self.blank_intensities[filter:]) dark = np.array(self.dark_intensities[filter:]) blank_minus_dark = np.subtract(blank, dark) average_buffer = [[] for i in range(scans_to_average)] for i in range(scans_to_average): average_buffer[i] = self.ocean_optics.intensities(correct_dark_counts=True, correct_nonlinearity=True) average_intensities = np.mean(average_buffer, axis = 0) intensities_splice = np.array(average_intensities[filter:]) intensities_splice_minus_dark = np.subtract(intensities_splice, dark) absorbance = -np.log10(blank_minus_dark/intensities_splice_minus_dark) if normalized: maximum = np.amax(absorbance) absorbance = absorbance/maximum absorbance = [x for x in absorbance if ~np.isnan(x)] spectrum = [wavelengths_splice, intensities_splice] output = [[],[]] for i, column in enumerate(spectrum): for value in column: try: output[i].append(float(value)) except AttributeError: pass except TypeError: pass return output"}
{"text_id": "8036", "text": "docstring: def _check_valid_event_ndims(self, min_event_ndims, event_ndims): event_ndims = ops.convert_to_tensor(event_ndims, name=\"event_ndims\") event_ndims_ = tensor_util.constant_value(event_ndims) assertions = [] if not event_ndims.dtype.is_integer: raise ValueError(\"Expected integer dtype, got dtype {}\".format( event_ndims.dtype)) if event_ndims_ is not None: if event_ndims.shape.ndims != 0: raise ValueError(\"Expected scalar event_ndims, got shape {}\".format( event_ndims.shape)) if min_event_ndims > event_ndims_: raise ValueError(\"event_ndims ({}) must be larger than \" \"min_event_ndims ({})\".format( event_ndims_, min_event_ndims)) elif self.validate_args: assertions += [ check_ops.assert_greater_equal(event_ndims, min_event_ndims)] if event_ndims.shape.is_fully_defined(): if event_ndims.shape.ndims != 0: raise ValueError(\"Expected scalar shape, got ndims {}\".format( event_ndims.shape.ndims)) elif self.validate_args: assertions += [ check_ops.assert_rank(event_ndims, 0, message=\"Expected scalar.\")] return assertions"}
{"text_id": "8037", "text": "docstring: def rectifying_affine_transforms(rpc1, rpc2, aoi, z=0): lons, lats = np.asarray(aoi['coordinates'][0][:4]).T lon, lat = np.mean([lons, lats], axis=1) P1 = rpc_affine_approximation(rpc1, (lon, lat, z)) P2 = rpc_affine_approximation(rpc2, (lon, lat, z)) F = affine_fundamental_matrix(P1, P2) S1, S2 = rectifying_similarities_from_affine_fundamental_matrix(F) q1 = S1 @ P1 @ [lons, lats, [z, z, z, z], [1, 1, 1, 1]] q2 = S2 @ P2 @ [lons, lats, [z, z, z, z], [1, 1, 1, 1]] S2 = affine_transformation(q2[:2].T, q1[:2].T) @ S2 x1, y1, w1, h1 = utils.bounding_box_of_projected_aoi(rpc1, aoi, z=z, homography=S1) x2, y2, w2, h2 = utils.bounding_box_of_projected_aoi(rpc2, aoi, z=z, homography=S2) S1 = matrix_translation(-x1, -0.5 * (y1 + y2)) @ S1 S2 = matrix_translation(-x2, -0.5 * (y1 + y2)) @ S2 w = int(round(max(w1, w2))) h = int(round(max(h1, h2))) return S1, S2, w, h, P1, P2"}
{"text_id": "8038", "text": "docstring: def addPackageids(catalogitems, pkgid_table): for item in catalogitems: name = item.get('name') if not name: continue if item.get('receipts'): if not name in pkgid_table: pkgid_table[name] = [] for receipt in item['receipts']: if 'packageid' in receipt: if not receipt['packageid'] in pkgid_table[name]: pkgid_table[name].append(receipt['packageid'])"}
{"text_id": "8039", "text": "docstring: def ip_valid(user_input): if user_input == '': return True ip_regex = re.compile( r'^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$') return re.match(ip_regex, user_input)"}
{"text_id": "8040", "text": "docstring: def parse_duration_minutes(self, state) -> Optional[int]: duration_minutes = None duration = state[\"attributes\"].get(\"duration\") if duration is not None: if isinstance(duration, float): duration_minutes = int(duration) else: duration_minutes = int(duration[: duration.find(\" \")]) else: self.log(\"Could not find duration in state attributes.\", level=\"WARNING\") return duration_minutes"}
{"text_id": "8041", "text": "docstring: def delete_message(self, m: Message): if m in self.get_inbox(): self.get_inbox().remove(m) if m in self.get_outbox(): self.get_outbox().remove(m)"}
{"text_id": "8042", "text": "docstring: def _maybe_to_geodataframe(df, schema): def to_shapely(row, name): return shape.to_shape(row[name]) if row[name] is not None else None if len(df) and geospatial_supported: geom_col = None for name, dtype in schema.items(): if isinstance(dtype, dt.GeoSpatial): geom_col = geom_col or name df[name] = df.apply(lambda x: to_shapely(x, name), axis=1) if geom_col: df = geopandas.GeoDataFrame(df, geometry=geom_col) return df"}
{"text_id": "8043", "text": "docstring: async def delete_user(app, uid=None, username=None): if uid: return await app[\"db\"].users.delete_one({\"_id\": uid}) if username: return await app[\"db\"].users.delete_one({\"username\": username}) return False"}
{"text_id": "8044", "text": "docstring: def index(): genre_counts = df.groupby('genre').count()['message'] genre_names = list(genre_counts.index) class_count = df.drop(columns=['id','message','original']).groupby('genre').sum() data2 = [] for genre_val in class_count.index: tmp = class_count.loc[genre_val] data2.append(Bar(x=list(tmp.index),y=tmp.values, name=genre_val)) graphs = [ { 'data': [ Pie( labels=genre_names, values=genre_counts ) ], 'layout': { 'title': 'Distribution of Message Genres' } }, { 'data': data2, 'layout': { 'title': 'Distribution of classes', 'barmode': 'stack', 'yaxis': { 'title': \"number of messages\" }, 'xaxis': { 'title': \"Classes\", 'tickangle': -45, 'automargin': True } } } ] ids = [\"graph-{}\".format(i) for i, _ in enumerate(graphs)] graphJSON = json.dumps(graphs, cls=plotly.utils.PlotlyJSONEncoder) return render_template('master.html', ids=ids, graphJSON=graphJSON)"}
{"text_id": "8045", "text": "docstring: def categorize_edges(infr, graph=None, ne_to_edges=None): states = (POSTV, NEGTV, INCMP, UNREV, UNKWN) if ne_to_edges is None: ne_to_edges = infr.collapsed_meta_edges(graph) name_edges = {key: set(ne_to_edges[key].keys()) for key in states} for key in UNINFERABLE: name_edges[key].difference_update(name_edges[POSTV]) name_edges[key].difference_update(name_edges[NEGTV]) incon_internal_ne = name_edges[NEGTV].intersection(name_edges[POSTV]) name_edges[POSTV].difference_update(incon_internal_ne) name_edges[NEGTV].difference_update(incon_internal_ne) if __debug__: assert all(n1 == n2 for n1, n2 in name_edges[POSTV]), ( 'All positive edges should be internal to a PCC') assert len(name_edges[INCMP].intersection(incon_internal_ne)) == 0 assert len(name_edges[UNREV].intersection(incon_internal_ne)) == 0 assert len(name_edges[UNKWN].intersection(incon_internal_ne)) == 0 assert all(n1 == n2 for n1, n2 in incon_internal_ne), ( 'incon_internal edges should be internal to a PCC') incon_internal_nids = {n1 for n1, n2 in incon_internal_ne} incon_external_ne = set([]) for key in (NEGTV,) + UNINFERABLE: incon_external_ne.update({ (nid1, nid2) for nid1, nid2 in name_edges[key] if nid1 in incon_internal_nids or nid2 in incon_internal_nids }) for key in (NEGTV,) + UNINFERABLE: name_edges[key].difference_update(incon_external_ne) union = lambda gen: set.union(*gen) positive = { nid1: union( ne_to_edges[key][(nid1, nid2)] for key in (POSTV,) + UNINFERABLE) for nid1, nid2 in name_edges[POSTV] } negative = { (nid1, nid2): union( ne_to_edges[key][(nid1, nid2)] for key in (NEGTV,) + UNINFERABLE) for nid1, nid2 in name_edges[NEGTV] } incon_internal = { nid: union( ne_to_edges[key][(nid, nid)] for key in (POSTV, NEGTV,) + UNINFERABLE) for nid in incon_internal_nids } incon_external = { (nid1, nid2): union( ne_to_edges[key][(nid1, nid2)] for key in (NEGTV,) + UNINFERABLE) for nid1, nid2 in incon_external_ne } unknown = { (nid1, nid2): ne_to_edges[UNKWN][(nid1, nid2)] for (nid1, nid2) in name_edges[UNKWN] } notcomparable = { (nid1, nid2): ne_to_edges[INCMP][(nid1, nid2)] for (nid1, nid2) in name_edges[INCMP] } unreviewed = { (nid1, nid2): ne_to_edges[UNREV][(nid1, nid2)] for (nid1, nid2) in name_edges[UNREV] } ne_categories = { POSTV: positive, NEGTV: negative, UNREV: unreviewed, INCMP: notcomparable, UNKWN: unknown, 'inconsistent_internal': incon_internal, 'inconsistent_external': incon_external, } return ne_categories"}
{"text_id": "8046", "text": "docstring: def to_pyarrow(self) -> pa.ChunkedArray: return pa.chunked_array(page.get_array() for page in self._pages)"}
{"text_id": "8047", "text": "docstring: def connectPorts(self, p1, p2, z = None): if ((p1.host_DEVS.parent == self and p2.host_DEVS.parent == self) and (p1.type() == 'OUTPORT' and p2.type() == 'INPORT')): if p1.host_DEVS is p2.host_DEVS: raise DEVSException((\"In coupled model '%s', connecting ports\" + \" '%s' and '%s' belong to the same model\" + \" '%s'. \" + \" Direct feedback coupling not allowed\") % ( self.getModelName(), p1.getPortName(), p2.getPortName(), p1.host_DEVS.getModelName())) else: p1.outline.append(p2) p2.inline.append(p1) elif ((p1.host_DEVS == self and p2.host_DEVS.parent == self) and (p1.type() == p2.type() == 'INPORT')): p1.outline.append(p2) p2.inline.append(p1) elif ((p1.host_DEVS.parent == self and p2.host_DEVS == self) and (p1.type() == p2.type() == 'OUTPORT')): p1.outline.append(p2) p2.inline.append(p1) else: raise DEVSException((\"Illegal coupling in coupled model '%s' \" + \"between ports '%s' and '%s'\") % ( self.getModelName(), p1.getPortName(), p2.getPortName())) p1.z_functions[p2] = z if hasattr(self, \"full_name\"): self.server.getSelfProxy().dsConnectPorts(p1, p2)"}
{"text_id": "8048", "text": "docstring: def isCryptographyValid(): try: import cryptography except ImportError: return False from distutils.version import LooseVersion return LooseVersion(cryptography.__version__) >= LooseVersion(\"1.7\")"}
{"text_id": "8049", "text": "docstring: def french_to_english(frenchtext): translation = language_translator.translate( text=frenchtext, model_id='fr-en').get_result() englishtext = translation['translations'][0]['translation'] return englishtext"}
{"text_id": "8050", "text": "docstring: def PolygonIntersection(self, lines): intersegments = np.zeros((lines.shape[0], 2, 2)) intersect = np.zeros((lines.shape[0], 2)) for i, point in enumerate(lines): for polygon in self.polygons: for seg in range(polygon.shape[0]-1): segment = polygon[seg:seg+2] if CheckIntersect(point, segment) and not np.isnan(segment).any(): xi, yi = IntersectPoint(point, segment) intersect[i] = xi, yi intersegments[i] = segment return intersect, intersegments"}
{"text_id": "8051", "text": "docstring: def start_beam_jobserver(flink_session_name, artifacts_dir=\"Resources\", jobserver_jar=os.path.join(util.get_flink_lib_dir(), \"beam-runners-flink-1.9-job-server-2.24.0.jar\"), jobserver_main_class=\"org.apache.beam.runners.flink.FlinkJobServerDriver\", service_discover_jar=os.path.join(util.get_flink_lib_dir(), \"service-discovery-client-0.5-SNAPSHOT.jar\")): method = constants.HTTP_CONFIG.HTTP_GET resource_url = constants.DELIMITERS.SLASH_DELIMITER + \\ constants.REST_CONFIG.HOPSWORKS_REST_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\ constants.REST_CONFIG.HOPSWORKS_PROJECT_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\ hopsfs.project_id() + constants.DELIMITERS.SLASH_DELIMITER + \\ \"jobs\" + constants.DELIMITERS.SLASH_DELIMITER + \\ flink_session_name + constants.DELIMITERS.SLASH_DELIMITER + \\ \"executions\" + \\ \"?limit=1&offset=0&sort_by=submissionTime:desc\" response = util.send_request(method, resource_url) response_object = response.json() flink_master_url = response_object['items'][0]['flinkMasterURL'] artifact_port = randint(10000, 65000) expansion_port = randint(10000, 65000) job_port = randint(10000, 65000) job_host = socket.getfqdn() log_base_path = \"\" if 'LOG_DIRS' in os.environ: log_base_path += os.environ['LOG_DIRS'] + \"/\" beam_jobserver_log = log_base_path + \"beamjobserver-\" + hopsfs.project_name().lower() + \"-\" + flink_session_name + \\ \"-\" + str(job_port) + \".log\" with open(beam_jobserver_log, \"wb\") as out, open(beam_jobserver_log, \"wb\") as err: jobserver_cp_list = list(filter(lambda x: \"service-discovery\" not in x and x.endswith(\".jar\"), util.get_hadoop_classpath_glob().split(\":\"))) jobserver_cp_list.extend((service_discover_jar, jobserver_jar)) jobserver_cp_path = \":\".join(jobserver_cp_list).replace(\"\\n\",\"\") jobserver = subprocess.Popen([\"java\", \"-cp\", \"%s\" % jobserver_cp_path, jobserver_main_class, \"--artifacts-dir=%s\" % hopsfs.project_path() + artifacts_dir, \"--flink-master-url=%s\" % flink_master_url, \"--artifact-port=%d\" % artifact_port, \"--expansion-port=%d\" % expansion_port, \"--job-host=%s\" % job_host, \"--job-port=%d\" % job_port], stdout=out, stderr=err, preexec_fn=util._on_executor_exit('SIGTERM')) global clusters clusters.append(flink_session_name) global jobserver_host jobserver_host = job_host global jobserver_port jobserver_port = job_port return {\"jobserver_log\": beam_jobserver_log, \"artifact_port\": artifact_port, \"expansion_port\": expansion_port, \"job_host\": job_host, \"job_port\": job_port, \"jobserver.pid\": jobserver.pid}"}
{"text_id": "8052", "text": "docstring: async def weather(self, ctx: NewCtx, *, city: str): if not (embed := ctx.cached_data): res = await self.aioweather.fetch_weather(city) embed = self.aioweather.format_weather(res) ctx.add_to_cache(value=embed, timeout=timedelta(minutes=10)) await ctx.send(embed=embed)"}
{"text_id": "8053", "text": "docstring: def forward(self, y_pred: Dict[str, torch.Tensor], target: Union[torch.Tensor, rnn.PackedSequence]) -> torch.Tensor: if isinstance(target, rnn.PackedSequence): target, lengths = rnn.pad_packed_sequence(target, batch_first=True) lengths = lengths.to(target.device) else: lengths = torch.ones(target.size(0), device=target.device, dtype=torch.long) * target.size(1) assert not target.requires_grad if target.ndim == 3: weight = target[..., 1] target = target[..., 0] else: weight = None losses = self.loss(y_pred, target) if weight is not None: losses = losses * weight.unsqueeze(-1) mask = torch.arange(target.size(1), device=target.device).unsqueeze(0) >= lengths.unsqueeze(-1) if losses.ndim > 2: mask = mask.unsqueeze(-1) dim_normalizer = losses.size(-1) else: dim_normalizer = 1.0 if self.reduction == \"none\": loss = losses.masked_fill(mask, float(\"nan\")) else: if self.reduction == \"mean\": losses = losses.masked_fill(mask, 0.0) loss = losses.sum() / lengths.sum() / dim_normalizer elif self.reduction == \"sqrt-mean\": losses = losses.masked_fill(mask, 0.0) loss = losses.sum() / lengths.sum() / dim_normalizer loss = loss.sqrt() assert not torch.isnan(loss), ( \"Loss should not be nan - i.e. something went wrong \" \"in calculating the loss (e.g. log of a negative number)\" ) assert torch.isfinite( loss ), \"Loss should not be infinite - i.e. something went wrong (e.g. input is not in log space)\" return loss"}
{"text_id": "8054", "text": "docstring: def create_user( self, create_user_input: CreateUserInput ) -> User: parameters = dict() parameters[\"input\"] = GraphQLParam( create_user_input, \"CreateUserInput\", True ) response = self._mutation( name=\"createOrgUser\", params=parameters, fields=User.fields() ) return User(response)"}
{"text_id": "8055", "text": "docstring: def find_high_severity_vulnerabilities_for_image(resource_url, project_id): from grafeas.grafeas_v1 import Severity from google.cloud.devtools import containeranalysis_v1 client = containeranalysis_v1.ContainerAnalysisClient() grafeas_client = client.get_grafeas_client() project_name = f\"projects/{project_id}\" filter_str = 'kind=\"VULNERABILITY\" AND resourceUrl=\"{}\"'\\ .format(resource_url) vulnerabilities = grafeas_client.list_occurrences(parent=project_name, filter=filter_str) filtered_list = [] for v in vulnerabilities: if v.vulnerability.effective_severity == Severity.HIGH or v.vulnerability.effective_severity == Severity.CRITICAL: filtered_list.append(v) return filtered_list"}
{"text_id": "8056", "text": "docstring: def run(config: ColocationConfig): table = [] for i in range(config.total_room_count - config.room_count + 1): config_local = config.copy() config_local.selected_rooms = list(range(i, i + config.room_count)) config_local.visualize = False config_local.plot_fitness_accuracy = False config_local.plot_fitness_density = False print(\"Rooms {}\".format(config_local.selected_rooms)) best_fitness, best_accuracy, _ = strict_ga.run(config_local) table.append([config_local.selected_rooms, best_fitness, best_accuracy]) print('') print('{} consecutive rooms. {} groups checked. '.format( config.room_count, config.total_room_count - config.room_count + 1)) print('average correlational score = {}'.format(np.mean([f for _, f, _ in table]))) print('average accuracy = {}'.format(np.mean([a for _, _, a in table]))) print(\"Saving stats... \") with open( config.base_file_name + 'room_combination_table.csv', 'w', newline='') as file: writer = csv.writer(file) for row in table: writer.writerow(row) with visualization.visualizing( figsize=(8, 4), nrows=2, filename=str(config.join_name('room_accuracy.png')), sharex=True) as (ax1, ax2): x_axis = list(range(len(table))) ax1.scatter(x_axis, [f for _, f, _ in table]) ax1.set_title('fitness') ax2.scatter(x_axis, [a for _, _, a, in table]) ax2.set_title('accuracy') ax2.set_xticks(x_axis) ax2.set_xticklabels([l for l, _, _ in table], rotation=90) return None, None, None"}
{"text_id": "8057", "text": "docstring: def refresh_token(self, refresh_token, prev_access_token): try: log.info(\"Refreshing token\") jwt.decode(refresh_token, PUBLIC_KEY, algorithms=[\"RS256\"]) if refresh_token in get_config_value(Config.BLACKLIST): log.warning( \"Attempted refresh from token in blacklist: %s\", refresh_token, ) raise Exception(\"JWT in blacklist\") log.info(\"Token verified\") except Exception: log.warning(\"Refresh token was not valid\") return \"Refresh token was not valid\", 403 try: payload = jwt.decode( prev_access_token, PUBLIC_KEY, algorithms=[\"RS256\"], options={\"verify_exp\": False}, ) payload[\"exp\"] = current_time() + datetime.timedelta( minutes=ACCESS_TOKEN_VALID_FOR, ) log.info(\"Creating ICATAuthenticator\") authenticator = ICATAuthenticator() authenticator.refresh(payload[\"sessionId\"]) return self._pack_jwt(payload), 200 except Exception: log.warning(\"Unable to refresh token\") return \"Unable to refresh token\", 403"}
{"text_id": "8058", "text": "docstring: async def demote(dmod): chat = await dmod.get_chat() admin = chat.admin_rights creator = chat.creator if not admin and not creator: await dmod.edit(NO_ADMIN) return await dmod.edit(\"`Despromovido...`\") rank = \"admeme\" user = await get_user_from_event(dmod) user = user[0] if user: pass else: return newrights = ChatAdminRights( add_admins=None, invite_users=None, change_info=None, ban_users=None, delete_messages=None, pin_messages=None, ) try: await dmod.client(EditAdminRequest(dmod.chat_id, user.id, newrights, rank)) except BadRequestError: await dmod.edit(NO_PERM) return await dmod.edit(\"`Perdeu o adm com sucesso!`\") if BOTLOG: await dmod.client.send_message( BOTLOG_CHATID, \"#DEMOTE\\n\" f\"USER: [{user.first_name}](tg://user?id={user.id})\\n\" f\"CHAT: {dmod.chat.title}(`{dmod.chat_id}`)\", )"}
{"text_id": "8059", "text": "docstring: def list_log_entries(self, resource_names, project_ids=None, filter_='', order_by='', page_size=0, options=None): if project_ids is None: project_ids = [] request = logging_pb2.ListLogEntriesRequest( resource_names=resource_names, project_ids=project_ids, filter=filter_, order_by=order_by, page_size=page_size) return self._list_log_entries(request, options)"}
{"text_id": "8060", "text": "docstring: async def on_room_leave(self, room: Room, leavers: List[Contact], remover: Contact, date: datetime):"}
{"text_id": "8061", "text": "docstring: def click_element_by_xpath(driver, xpath): driver.find_element_by_xpath(xpath).click()"}
{"text_id": "8062", "text": "docstring: def dropout_forward(x, dropout_param): p, mode = dropout_param['p'], dropout_param['mode'] if 'seed' in dropout_param: np.random.seed(dropout_param['seed']) mask = None out = None if mode == 'train': mask = np.random.choice([0, 1], size=np.shape(x), p=[p, 1-p]) out = mask*x elif mode == 'test': out = x cache = (dropout_param, mask) out = out.astype(x.dtype, copy=False) return out, cache"}
{"text_id": "8063", "text": "docstring: def city_query(city_name): results = session.query(*sel_all).filter(Basic_data_model.city.ilike('%'+city_name+'%')).all() jsondata_list = [] results_list = build_metadata_list(jsondata_list,results) return jsonify(results_list)"}
{"text_id": "8064", "text": "docstring: def make_data_cache(self, wrapped_pose: WrappedPose) -> DecoratorDataCache: cache = DecoratorDataCache(wrapped_pose) self.all_decs.cache_data(wrapped_pose, cache.dict_cache) return cache"}
{"text_id": "8065", "text": "docstring: def find_subscription_type(subscription): subs_available = list(constants.USER_SUBSCRIPTIONS_AVAILABLE.keys()) subs_available.extend(list(constants.NODE_SUBSCRIPTIONS_AVAILABLE.keys())) for available in subs_available: if available in subscription: return available"}
{"text_id": "8066", "text": "docstring: def redirect_authenticated(func): @functools.wraps(func) def wrapper(request, *args, **kwargs): if request.user.is_authenticated: return HttpResponseRedirect(reverse(\"kitchen\")) return func(request, *args, **kwargs) return wrapper"}
{"text_id": "8067", "text": "docstring: def BuildStartArgs(args): bigtable_dir = util.GetEmulatorRoot(BIGTABLE) bigtable_executable = os.path.join(bigtable_dir, BIGTABLE_EXECUTABLE) return execution_utils.ArgsForExecutableTool(bigtable_executable, *args)"}
{"text_id": "8068", "text": "docstring: def remove_edits(self, layername=None): if layername is None: self._ablation.clear() self._replacement.clear() return if not isinstance(layername, str): layername, aka = layername else: aka = layername if aka in self._ablation: del self._ablation[aka] if aka in self._replacement: del self._replacement[aka]"}
{"text_id": "8069", "text": "docstring: def _when_venv_exists_already(self, project_name): text = f'The virtual environment for a projectname \"{project_name}\" exists already. ' text += f'If you use the name \"{project_name}\", you will SHARE the virtual environment ' text += \"with the other project(s) using the same name.\" + os.linesep text += \"Continue?\" + os.linesep print(text) prompt = \"[Y] Yes [N] No, give new name. [A] Abort: \" validator = partial(is_in_accepted_values, accepted_values={\"Y\", \"N\", \"A\"}) value = get_input(prompt, validator).upper() if value == \"Y\": return \"skipcreate\" elif value == \"N\": return \"newname\" elif value == \"A\": raise UserAborted()"}
{"text_id": "8070", "text": "docstring: def audit_events_query(self: object, parameters: dict = None, **kwargs) -> dict: operation_id = \"audit_events_query\" target_url = f\"{self.base_url}{[ep[2] for ep in Endpoints if operation_id in ep[0]][0]}\" header_payload = self.headers parameter_payload = args_to_params(parameters, kwargs, Endpoints, operation_id) returned = service_request(caller=self, method=\"GET\", endpoint=target_url, headers=header_payload, params=parameter_payload, verify=self.ssl_verify ) return returned"}
{"text_id": "8071", "text": "docstring: def add_filters(self, **kwargs): return super(AOIManager, self).get_query_set().filter(**kwargs)"}
{"text_id": "8072", "text": "docstring: def visit_AugAssign(self, node): self.generic_visit(node) if isinstance(node.target, ast.Name): name = node.target.id res = combine(node.op, self.result[name], self.result[node.value]) self.result[name] = res"}
{"text_id": "8073", "text": "docstring: def reference_to_dataset_header(self, header): header = self.locate.reference_keys_to_dataset_keys(self, header) header = substitutions.expand_wildcards(self, header) header = self.locate.condition_matching_header(self, header) header = self.map_irrelevant_parkeys_to_na(header, keep_comments=True) for key in self.get_extra_parkeys(): log.verbose(\"Mapping extra parkey\", repr(key), \"from\", header[key], \"to 'N/A'.\") header[key] = \"N/A\" return header"}
{"text_id": "8074", "text": "docstring: def isEmail(address): check = re.compile('^[\\w\\-_\\.]+@\\w+\\.\\w+$|^.*\\<[\\w\\-_\\.]+@\\w+\\.\\w+\\>$') return check.match(nstr(address)) is not None"}
{"text_id": "8075", "text": "docstring: def request_service(self, svc_name, namespace=None, argdict=None, **kwargs): svc_name = svc_name.lower() if not self.services.get(svc_name, namespace): raise ServiceDoesNotExist(\"Service does not exist.\") service = self.services.get(svc_name, namespace) with locked(service.lock): if argdict: try: kwargs.update(argdict) except AttributeError: pass return service.svc(**kwargs)"}
{"text_id": "8076", "text": "docstring: def roundodd(num): rounded = round(num) if rounded % 2 != 0: return rounded else: if rounded > num: return rounded - 1 else: return rounded + 1"}
{"text_id": "8077", "text": "docstring: def convert_cells_to_packages(self): coords = self.cells.keys() coords.sort() row_ids = [i[0] for i in coords] col_ids = [i[1] for i in coords] top_left_coord = (min(row_ids), min(col_ids)) bottom_right_coord = (max(row_ids), max(col_ids)) print \"Working area of spreadsheet: top-left %s; bottom-right %s.\" % (top_left_coord, bottom_right_coord) row_range = range(top_left_coord[0], bottom_right_coord[0]+1) col_range = range(top_left_coord[1], bottom_right_coord[1]+1) self.raw_entities = [] self.headings = [] for col_id in col_range: row_id = row_range[self.HEADING_ROW_POSN] coord = (row_id, col_id) if coord in self.cells: heading = self.cells[coord] else: heading = \"\" self.headings.append(heading) print \"There are %s headings: %s\" % (len(self.headings), \", \".join(self.headings)) for row_id in row_range[self.FIRST_ENTITY_ROW_POSN:]: raw_entity = [] self.raw_entities.append(raw_entity) for col_id in col_range: coord = (row_id, col_id) if coord in self.cells: attribute = self.cells[coord] else: attribute = \"\" raw_entity.append(attribute) self.entities = [] for i, raw_entity in enumerate(self.raw_entities): entity = {} self.entities.append(entity) for j, value in enumerate(raw_entity): key = self.headings[j] entity[key] = value.strip() print \"There are %s entities: %s\" % (len(self.entities), \", \".join([self.coerce_package_name(e[self.headings[0]]) for e in self.entities])) for entity in self.entities: if '' in entity: entity.pop('') package = self.entity_to_package(entity) if package: self.packages.append(package) print \"There are %s metadata packages with titles extracted from the spreadsheet.\" % len(self.packages)"}
{"text_id": "8078", "text": "docstring: def pthid(self, val: str): self._pthid = val"}
{"text_id": "8079", "text": "docstring: def GetHotspotActiveBackground(*args, **kwargs): return _stc.StyledTextCtrl_GetHotspotActiveBackground(*args, **kwargs)"}
{"text_id": "8080", "text": "docstring: def gini_coefficient(x): diffsum = 0 for i, xi in enumerate(x[:-1], 1): diffsum += np.sum(np.abs(xi - x[i:])) return diffsum / (len(x) ** 2 * np.mean(x))"}
{"text_id": "8081", "text": "docstring: def read(self): def read_txt(x): with open(x, \"r\") as text_to_read: return np.array([i[:-1] for i in text_to_read]) label_titles = read_txt(self.path+\"/label_titles.txt\") labels = read_txt(self.path+\"/labels.txt\").astype(int) sentences = read_txt(self.path+\"/titles.txt\") sentences, labels = shuffle(sentences, labels) self.num_labels = np.max(labels)+1 return label_titles, labels, sentences"}
{"text_id": "8082", "text": "docstring: def _csr_data(X): indices = np.zeros((X.nnz, 2), dtype=np.int64) values = np.zeros(X.nnz) i = 0 for row_idx in range(X.shape[0]): column_indices = X.indices[X.indptr[row_idx]: X.indptr[row_idx + 1]] row_values = X.data[X.indptr[row_idx]: X.indptr[row_idx + 1]] for column_idx, row_value in zip(column_indices, row_values): indices[i][0] = row_idx indices[i][1] = column_idx values[i] = row_value i += 1 return indices, values"}
{"text_id": "8083", "text": "docstring: def flatten(self, separator=\".\"): flattened_config = {} Config._flatten_helper(flattened_config, self.config, \"\", separator=separator) return flattened_config"}
{"text_id": "8084", "text": "docstring: def reset(self): self.config = {} self.handlers = {} self.handler_callbacks = {} self.defaults = {} self.maps = {} self.eventhooks = {}"}
{"text_id": "8085", "text": "docstring: def transaction_update_spents(txs, address): spend_list = {} for t in txs: for inp in t.inputs: if inp.address == address: spend_list.update({(inp.prev_txid.hex(), inp.output_n_int): t}) address_inputs = list(spend_list.keys()) for t in txs: for to in t.outputs: if to.address != address: continue spent = True if (t.txid, to.output_n) in address_inputs else False txs[txs.index(t)].outputs[to.output_n].spent = spent if spent: spending_tx = spend_list[(t.txid, to.output_n)] spending_index_n = \\ [inp for inp in txs[txs.index(spending_tx)].inputs if inp.prev_txid.hex() == t.txid and inp.output_n_int == to.output_n][0].index_n txs[txs.index(t)].outputs[to.output_n].spending_txid = spending_tx.txid txs[txs.index(t)].outputs[to.output_n].spending_index_n = spending_index_n return txs"}
{"text_id": "8086", "text": "docstring: def forward(self, X: Tensor) -> Tensor: if self.embed_input is not None: x = [ self.embed_layers[\"emb_layer_\" + col]( X[:, self.deep_column_idx[col]].long() ) for col, _, _ in self.embed_input ] x = torch.cat(x, 1) x = self.embed_dropout(x) if self.continuous_cols is not None: cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols] x_cont = X[:, cont_idx].float() x = torch.cat([x, x_cont], 1) if self.embed_input is not None else x_cont return self.dense_resnet(x)"}
{"text_id": "8087", "text": "docstring: def apply_bitmask(mask, value, currentValue): value = int_to_binary(value) output = list(int_to_binary(currentValue)) for i in range(len(mask)): if mask[i] == 'X': output[i] = value[i] else: output[i] = mask[i] value = ''.join(output) newValue = binary_to_int(value) return newValue"}
{"text_id": "8088", "text": "docstring: def build_backbone(train_backbone=True): backbone = peleenet_v1(pretrained=is_main_process()) return_layers = {\"features\": \"0\"} num_channels = 704 backbone = BackboneBase(backbone, return_layers, train_backbone, num_channels) return backbone"}
{"text_id": "8089", "text": "docstring: def gaussian(z): return np.exp(-z ** 2 / 2.0)"}
{"text_id": "8090", "text": "docstring: def assign_very_frequent_sense(disambiguation_order, frequency_counts): for idx, item in enumerate(disambiguation_order): if item[1] != 0 and item[0] in frequency_counts.keys(): for sense in frequency_counts[item[0]]: if frequency_counts[item[0]][sense] >= 90 * sum(frequency_counts[item[0]].values())/100: disambiguation_order[idx] = ( item[0], sense.replace('\"', '')) return disambiguation_order"}
{"text_id": "8091", "text": "docstring: def run_analyses(self, traj): rdf = freud.density.RDF(bins=100, r_max=5) ql = freud.order.Steinhardt(6) for system in traj: rdf.compute(system, reset=False) ql.compute(system, neighbors={'num_neighbors': 6}) npt.assert_allclose(rdf.rdf, LJ_RDF, rtol=1e-5, atol=1e-5)"}
{"text_id": "8092", "text": "docstring: def update_config(config_file): if debug is True: print(\"Configuring %s\" % config_file) make_backup(config_file) new_config = \"import sys\\nsys.path.append({0!r})\".format(os.path.join(data_dir, 'extensions')) with open(config_file, 'a+') as f: f.seek(0) pyconfig = f.read() if pyconfig.find(marker) >= 0: pyconfig = remove_old_config(pyconfig) pyconfig = marker + '\\n' + new_config + '\\n' + marker + '\\n' + pyconfig with open(config_file, 'w') as f: f.write(pyconfig)"}
{"text_id": "8093", "text": "docstring: def search(self, query, relation=None, index=0, limit=25, **kwargs): return self.get_object( \"search\", relation=relation, q=query, index=index, limit=limit, **kwargs )"}
{"text_id": "8094", "text": "docstring: def _should_realloc(self) -> bool: self._iterations += 1 return (self._iterations % self._allocator_invoke_freq == 0)"}
{"text_id": "8095", "text": "docstring: def nested_data_to_arrays( data: Sequence, columns: Optional[Index], index: Optional[Index], dtype: Optional[DtypeObj], ): if is_named_tuple(data[0]) and columns is None: columns = ensure_index(data[0]._fields) arrays, columns = to_arrays(data, columns, dtype=dtype) columns = ensure_index(columns) if index is None: if isinstance(data[0], ABCSeries): index = _get_names_from_index(data) elif isinstance(data[0], Categorical): index = ibase.default_index(len(data[0])) else: index = ibase.default_index(len(data)) return arrays, columns, index"}
{"text_id": "8096", "text": "docstring: def euclid_distance1(lat, lng, lat0, lng0): return 110.25 * ((((lat - lat0) ** 2) + (((lng - lng0) * np.cos(lat0)) ** 2)) ** 0.5)"}
{"text_id": "8097", "text": "docstring: def decode_batch(self, window, location): n_samples = location.shape[0] location_init = np.copy(location) test = None for i in window: if 'window' in i: test = np.ones_like(window[i]) window[i], _ = self.crop_batch(window[i], location_init, self.window_border) location_init = np.copy(location) print(i, np.sum(window[i]), np.max(window[i])) _, location = self.crop_batch(test, location_init, self.window_border) for batch_id in range(n_samples): if self._is_stopping_signal(location[batch_id]): return False self.image_id = location[batch_id, 0] if self._is_stopping_signal(location[batch_id]): return False self.image_out = {} self.csv_out = {} for i in window: if 'window' in i: while window[i].ndim < 5: window[i] = window[i][..., np.newaxis, :] self.image_out[i] = window[i][batch_id, ...] else: if not isinstance(window[i], (list, tuple, np.ndarray)): window_loc = np.reshape(window[i], [1, 1]) self.csv_out[i] = self._initialise_empty_csv(1) else: window[i] = np.asarray(window[i]) try: assert window[i].ndim <= 2 except (TypeError, AssertionError): tf.logging.error( \"The output you are trying to \" \"save as csv is more than \" \"bidimensional. Did you want \" \"to save an image instead? \" \"Put the keyword window \" \"in the output dictionary\" \" in your application file\") if window[i].ndim < 2: window[i] = np.expand_dims(window[i], 0) window_loc = window[i] self.csv_out[i] = self._initialise_empty_csv( n_channel=window[i][0].shape[-1]) self.csv_out[i] = np.concatenate([self.csv_out[i], window_loc], 0) self._save_current_image() self._save_current_csv() return True"}
{"text_id": "8098", "text": "docstring: def internal_use_incredibuild(ctx, section_name, option_name, value, verification_fn): if not ctx.is_option_true('console_mode'): return ctx.gui_get_attribute(section_name, option_name, value) if not value or value != 'True': return value if not Utils.unversioned_sys_platform() == 'win32': return value _incredibuild_disclaimer(ctx) if Logs.verbose > 1: ctx.start_msg('Incredibuild Licence Check') (res, warning, error) = verification_fn(ctx, option_name, value) if not res: if warning: Logs.warn(warning) if error: if Logs.verbose > 1: ctx.end_msg(error, color='YELLOW') return 'False' if Logs.verbose > 1: ctx.end_msg('ok') return value"}
{"text_id": "8099", "text": "docstring: def _check_results(self): for matchup in self.matchup_list: results = self.results[matchup.id] if not results: continue result = results[0] seen_players = sorted(result.players.itervalues()) expected_players = sorted((matchup.player_1, matchup.player_2)) if seen_players != expected_players: raise CompetitionError( \"existing results for matchup %s \" \"are inconsistent with control file:\\n\" \"result players are %s;\\n\" \"control file players are %s\" % (matchup.id, \",\".join(seen_players), \",\".join(expected_players)))"}
{"text_id": "8100", "text": "docstring: def kl_divergence_kumaraswamy(prior_a, a, b): Euler = torch.tensor(0.577215664901532) kl = (1 - prior_a / a) * (-Euler - torch.digamma(b) - 1./b)\\ + torch.log(a*b /prior_a) - (b-1)/b return kl.sum()"}
{"text_id": "8101", "text": "docstring: def show_data(self, data: bytes, codec: str): raise NotImplementedError"}
{"text_id": "8102", "text": "docstring: def compute(cls, observation: dict, prediction: dict) -> \"CohenDScore\": assert isinstance(observation, dict) assert isinstance(prediction, dict) p_mean = prediction[\"mean\"] p_std = prediction[\"std\"] o_mean = observation[\"mean\"] o_std = observation[\"std\"] try: p_n = prediction[\"n\"] o_n = observation[\"n\"] s = ( ((p_n - 1) * (p_std ** 2) + (o_n - 1) * (o_std ** 2)) / (p_n + o_n - 2) ) ** 0.5 except KeyError: s = (p_std ** 2 + o_std ** 2) ** 0.5 value = (p_mean - o_mean) / s value = utils.assert_dimensionless(value) return cls(value)"}
{"text_id": "8103", "text": "docstring: def reduce_item(value, type_): return _reduce_item[type_](value)"}
{"text_id": "8104", "text": "docstring: def compute_gradient(self): with tf.name_scope(\"compute_gradient\"): grads = [g.read_value() for g in self.grads_list] return grads"}
{"text_id": "8105", "text": "docstring: def read_corpus(df): count = -1 for i, data in df.iterrows(): count += 1 desc = ' '.join(data.description) desc = str(desc).lower() desc = remove_stopwords(desc) desc = re.sub(r'\\s+', ' ', desc) desc = re.sub('[0-9]', '', desc) desc = re.sub(r'(?<=\\w)-(?=\\w)', ' ', desc) desc = re.sub(f'[{re.escape(string.punctuation)}]', '', desc) tokens = word_tokenize(desc) tokens = [t for t in tokens if len(t) > 1] yield gensim.models.doc2vec.TaggedDocument(tokens, [count])"}
{"text_id": "8106", "text": "docstring: def inverse_transform(self, data, sigmas=None): st = 0 recovered_column_data_list = [] column_names = [] for column_transform_info in self._column_transform_info_list: dim = column_transform_info.output_dimensions column_data = data[:, st:st + dim] if column_transform_info.column_type == 'continuous': recovered_column_data = self._inverse_transform_continuous( column_transform_info, column_data, sigmas, st) else: assert column_transform_info.column_type == 'discrete' recovered_column_data = self._inverse_transform_discrete( column_transform_info, column_data) recovered_column_data_list.append(recovered_column_data) column_names.append(column_transform_info.column_name) st += dim recovered_data = np.column_stack(recovered_column_data_list) recovered_data = (pd.DataFrame(recovered_data, columns=column_names) .astype(self._column_raw_dtypes)) if not self.dataframe: recovered_data = recovered_data.to_numpy() return recovered_data"}
{"text_id": "8107", "text": "docstring: def request_review(user, article, markdown): if not article.owner.has_github_create(): return markdown = markdown.replace(\"\\r\\n\", \"\\n\") headers = get_auth(article.owner) headers[\"Accept\"] = \"application/vnd.github.everest-preview+json\" data = { \"event_type\": \"request_review_by_%s\" % user.username, \"client_payload\": { \"markdown\": markdown, \"username\": user.username, \"files\": article.template.files or \"README.md\", \"article\": str(article.uuid), \"event_name\": \"request-review\", }, } url = \"%s/repos/%s/dispatches\" % (api_base, article.repo[\"full_name\"]) response = requests.post(url, headers=headers, json=data) return response.status_code"}
{"text_id": "8108", "text": "docstring: def nucnorm(x, rho, penalty, newshape=None): orig_shape = x.shape if newshape is not None: x = x.reshape(newshape) u, s, v = np.linalg.svd(x, full_matrices=False) sthr = np.maximum(s - (penalty / rho), 0) return np.linalg.multi_dot((u, np.diag(sthr), v)).reshape(orig_shape)"}
{"text_id": "8109", "text": "docstring: def loadTunedEvolver(cls, h5group, action, rng): params = cls.loadTunedParameters(h5group) return ConstStepLeapfrog(action, params[\"length\"], params[\"nstep\"], rng)"}
{"text_id": "8110", "text": "docstring: def zero_forcing(signal, threshold=1e-20): signal[signal < threshold] = threshold return signal"}
{"text_id": "8111", "text": "docstring: def create_dummy(conn: Connection): with conn.cursor() as cur: cur.execute(queries.make_query_on_dummy(queries.QUERY_DROP_STRUCTURE)) for query in queries.QUERYLIST_CREATE_DUMMY_STRUCTURE: cur.execute(query) conn.commit()"}
{"text_id": "8112", "text": "docstring: def validate_payload(cls, payload: dict, validation_schema: dict) -> None: try: validate(payload, validation_schema) except ValidationError as validation_err: if cls.configuration().validation_output == \"warning\": warn(validation_err.message, MagellanRuntimeWarning) elif cls.configuration().validation_output == \"exception\": raise ( MagellanRuntimeException(validation_err.message) ) from validation_err"}
{"text_id": "8113", "text": "docstring: def run_tuning(self, n_call=10): res = gp_minimize(self._obj_fun, self._space, acq_func=\"EI\", n_calls=n_call, verbose=True) self._write_config(res.x) from autodp import cf cf.reset_config(self._config_input.get(\"config\", \"output\")) return res"}
{"text_id": "8114", "text": "docstring: def Clone( self ): return self.__class__( self.obj, self.attrName, self.formatter, self.flRequired, self.validationCB )"}
{"text_id": "8115", "text": "docstring: def delete_subscription( self, subscription: str, project_id: Optional[str] = None, fail_if_not_exists: bool = False, retry: Optional[Retry] = None, timeout: Optional[float] = None, metadata: Optional[Sequence[Tuple[str, str]]] = None, ) -> None: if not project_id: raise ValueError(\"Project ID should be set.\") subscriber = self.subscriber_client subscription_path = SubscriberClient.subscription_path(project_id, subscription) self.log.info(\"Deleting subscription (path) %s\", subscription_path) try: subscriber.delete_subscription( subscription=subscription_path, retry=retry, timeout=timeout, metadata=metadata ) except NotFound: self.log.warning('Subscription does not exist: %s', subscription_path) if fail_if_not_exists: raise PubSubException('Subscription does not exist: {}'.format(subscription_path)) except GoogleAPICallError as e: raise PubSubException('Error deleting subscription {}'.format(subscription_path), e) self.log.info(\"Deleted subscription (path) %s\", subscription_path)"}
{"text_id": "8116", "text": "docstring: def process_image(original_img): gray = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY) blur = cv2.GaussianBlur(gray, (9, 9), 0) thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2) kernel = np.ones((3, 3)) thresh = cv2.erode(thresh, kernel) thresh = cv2.dilate(thresh, kernel) return thresh"}
{"text_id": "8117", "text": "docstring: def main(): parser = argparse.ArgumentParser( \"measure and analyze inference time for a tensorflow model with different batch sizes\" ) parser.add_argument( \"-m\", \"--model_dir\", type=str, help=\"directory of the TensorFlow SavedModel\" ) parser.add_argument( \"-b\", \"--batches\", type=int, nargs=\"+\", help='range of batch sizes to use in the format \"start stop step\"', ) parser.add_argument( \"-r\", \"--repetitions\", type=int, help=\"number of times to repeat inference per batch size\", ) parser.add_argument( \"-w\", \"--warmup_steps\", type=int, default=5, help=\"number of inferences to warm model up\", ) parser.add_argument( \"-d\", \"--data_file\", type=str, default=None, help=\"path to save output raw data CSV to\", ) parser.add_argument( \"-c\", \"--coeff_file\", type=str, default=None, help=\"path to save calculated coefficients to\", ) parser.add_argument( \"-s\", \"--input_shape\", type=int, nargs=\"+\", help=\"input shape of the model\" ) args = parser.parse_args() print(\"Loading model...\") model = tf.saved_model.load(args.model_dir) print(\"Model loaded.\") batch_start = args.batches[0] batch_stop = batch_start if len(args.batches) < 2 else args.batches[1] batch_step = 1 if len(args.batches) < 3 else args.batches[2] batch_params = (batch_start, batch_stop, batch_step) print(f\"Beginning model profiling...\") data_points = record_model_inference_times( model=model, batch_params=batch_params, input_shape=args.input_shape, repetitions=args.repetitions, warmup_steps=args.warmup_steps, ) print(f\"Done profiling.\") if args.data_file: print(f\"Writing data points to file {args.data_file}\") write_inference_times_to_file(data_points=data_points, data_file=args.data_file) print(f\"Calculating coefficients...\") alpha, beta = calculate_coefficients(data_points=data_points) print(f\"Alpha: {alpha}, Beta: {beta}\") if args.coeff_file: print(f\"Writing coefficients to file {args.coeff_file}\") write_coefficients_to_file(alpha=alpha[0], beta=beta[0], coeff_file=args.coeff_file)"}
{"text_id": "8118", "text": "docstring: def _validate_monitor_key(self, trainer: \"pl.Trainer\"): metrics = trainer.callback_metrics if self.monitor is not None and not self._is_valid_monitor_key(metrics): m = ( f\"ModelCheckpoint(monitor='{self.monitor}') \" f\"not found in the returned metrics:\" f\" {list(metrics.keys())}. \" f\"HINT: Did you call self.log('{self.monitor}', value) \" f\"in the LightningModule?\" ) if not trainer.fit_loop.epoch_loop.val_loop._has_run: console.log(m) else: raise ValueError(m)"}
{"text_id": "8119", "text": "docstring: def plugin_app(parent): this.status_frame = tk.Frame(parent) this.status = tk.StringVar() if this.new_version != this.version: this.status.set(\"New version of the fire-damage plugin available\") debug(\"Reported new version to GUI\") tk.Label(this.status_frame, textvariable=this.status).grid(row=0, column = 0,sticky=tk.W) return this.status_frame"}
{"text_id": "8120", "text": "docstring: def workdir_us_county(tmpdir): basedir = pathlib.Path(__file__).resolve().parents[1] / \"manual_tests\" for fname in basedir.glob(\"cfg_*\"): shutil.copy(str(basedir / fname.name), str(tmpdir)) for fname in basedir.glob(\"*.txt\"): shutil.copy(str(basedir / fname.name), str(tmpdir)) for fname in basedir.glob(\"*.out.expected\"): shutil.copy(str(basedir / fname.name), str(tmpdir)) shutil.copy(str(basedir / \"US_county.zip\"), str(tmpdir)) subprocess.run([\"unzip\", \"US_county.zip\"], cwd=str(tmpdir), check=True) return pathlib.Path(str(tmpdir))"}
{"text_id": "8121", "text": "docstring: def _calculate_roc_curve_data(self, folds_predictions, targets, stratify_by=None): curve_data = self._calculate_curve_data(folds_predictions, targets, metrics.roc_curve, metrics.roc_auc_score, stratify_by=stratify_by) for curve_name in curve_data.keys(): curve_data[curve_name][\"FPR\"] = curve_data[curve_name].pop(\"first_ret_value\") curve_data[curve_name][\"TPR\"] = curve_data[curve_name].pop(\"second_ret_value\") curve_data[curve_name][\"AUC\"] = curve_data[curve_name].pop(\"area\") return curve_data"}
{"text_id": "8122", "text": "docstring: def run_npy_postprocess(): rows = read_list_from_file(os.path.join(cfg.split_dir, cfg.submit_split), comment='#') ids = [row.split('/')[-1] for row in rows] npy_files = [os.path.join(f_submit.submit_npy_dir, '%s.npy'%img_id) for img_id in ids] pool = Pool() pool.map(process_one_image, npy_files)"}
{"text_id": "8123", "text": "docstring: def listServices(self, repository, headers=None, query_params=None, content_type=\"application/json\"): uri = self.client.base_url + \"/ays/repository/\"+repository+\"/service\" return self.client.get(uri, headers, query_params, content_type)"}
{"text_id": "8124", "text": "docstring: def handle_console_err(self, msg): if msg.type == 'warning': self.console_warning = True print(' Console Warning: ' + msg.text) elif msg.type == 'error': self.console_error = True print(' Console Error: ' + msg.text)"}
{"text_id": "8125", "text": "docstring: def _simplify_blocks(self, ail_graph: networkx.DiGraph, remove_dead_memdefs=False, stack_pointer_tracker=None): blocks_by_addr_and_idx: Dict[Tuple[int, Optional[int]], ailment.Block] = {} for ail_block in ail_graph.nodes(): simplified = self._simplify_block(ail_block, remove_dead_memdefs=remove_dead_memdefs, stack_pointer_tracker=stack_pointer_tracker) key = ail_block.addr, ail_block.idx blocks_by_addr_and_idx[key] = simplified def _replace_node_handler(node): key = node.addr, node.idx if key in blocks_by_addr_and_idx: return blocks_by_addr_and_idx[key] return None AILGraphWalker(ail_graph, _replace_node_handler, replace_nodes=True).walk() return ail_graph"}
{"text_id": "8126", "text": "docstring: def crc(cls, img, pul_crc): ret_val = gxapi_cy.WrapIMU._crc(GXContext._get_tls_geo(), img, pul_crc) return ret_val"}
{"text_id": "8127", "text": "docstring: def post(self, body): tarfile = body.get('tarfile') name = body.get('name', '') version = body.get('app_version', '') name, version, mname, mfile = self._check_tarfile(tarfile, name, version, constants.APP_UPLOAD_OP) try: objects.kube_app.get_by_name(pecan.request.context, name) raise wsme.exc.ClientSideError(_( \"Application-upload rejected: application {} already exists.\".format( name))) except exception.KubeAppNotFound: pass app_data = {'name': name, 'app_version': version, 'manifest_name': mname, 'manifest_file': os.path.basename(mfile), 'status': constants.APP_UPLOAD_IN_PROGRESS} try: new_app = pecan.request.dbapi.kube_app_create(app_data) except exception.SysinvException as e: LOG.exception(e) raise pecan.request.rpcapi.perform_app_upload(pecan.request.context, new_app, tarfile) return KubeApp.convert_with_links(new_app)"}
{"text_id": "8128", "text": "docstring: def save_explore_agent_weights(self, save_path): self.explore_saver.save( self.sess, save_path=save_path, write_meta_graph=False)"}
{"text_id": "8129", "text": "docstring: def db_list_rows(self, table, record=None, if_exists=False):"}
{"text_id": "8130", "text": "docstring: def write(self, location: Union[str, Path] = None): location = Path(location or self.chapps.config_file) config_file = self.chapps.config_file version = self.chapps.version docpath = self.chapps.docpath self.configparser.remove_option(\"CHAPPS\", \"config_file\") self.configparser.remove_option(\"CHAPPS\", \"version\") self.configparser.remove_option(\"CHAPPS\", \"docpath\") result = CHAPPSConfig.write_config(self.configparser, location) if config_file: self.configparser[\"CHAPPS\"][\"config_file\"] = config_file self.configparser[\"CHAPPS\"][\"version\"] = version self.configparser[\"CHAPPS\"][\"docpath\"] = docpath return result"}
{"text_id": "8131", "text": "docstring: def ProtoNet(in_dim:tuple, n_way:int, k_shot:int): support = layers.Input((None, *in_dim)) query = layers.Input((None, *in_dim)) _network = keras.Sequential([ *[conv_block(_default_parameters['hid_channels']) for _ in range(_default_parameters['n_conv_block'])], layers.Flatten(**_default_parameters['flat']), ]) xs = _network(tf.reshape(support, (-1, *support.shape[2:]))) S = tf.reduce_mean(tf.reshape(xs, (n_way, k_shot, -1)), axis=1) Q = _network(tf.reshape(query, (-1, *query.shape[2:]))) dist = tf.reduce_sum((Q[:,None,:]-S[None,:,:])**2, axis=-1) output = tf.nn.softmax(-dist, axis=-1) return keras.Model(inputs=[query, support], outputs=output, name='ProtoNet')"}
{"text_id": "8132", "text": "docstring: def run_specmatch_for_orders(targetfile, targetname, outputdirectory='specmatch_results', HLS=None, path_df_lib=config.PATH_LIBRARY_DB, orders = ['4','5','6','14','15','16','17'], maxvsini=30.,calibrate_feh=True,scaleres=1.): Htarget = hpfspec.HPFSpectrum(targetfile,targetname = targetname) print('Reading Library DataBase from: {}'.format(path_df_lib)) df_lib = pd.read_csv(path_df_lib) if HLS is None: print('No HLS supplied, defaulting to default library') HLS = hpfspec.HPFSpecList(filelist=config.LIBRARY_FITSFILES) Hrefs = HLS.splist for o in orders: print(\"##################\") print(\"Order {}\".format(o)) print(\"##################\") wmin = config.BOUNDS[o][0] wmax = config.BOUNDS[o][1] ww = np.arange(wmin,wmax,0.01) v = np.linspace(-125,125,1501) savefolder = '{}/{}_{}/'.format(outputdirectory,Htarget.object,o) t,f,l,vis,te,fe,le,df_chi,LCS = run_specmatch(Htarget, HLS.splist, ww, v, df_lib, savefolder=savefolder, maxvsini=maxvsini, calibrate_feh=calibrate_feh, scaleres=scaleres)"}
{"text_id": "8133", "text": "docstring: def from_dictionary(cls, dictionary): if dictionary is None: return None id = dictionary.get('id') callback_url = dictionary.get('callbackUrl') publish_permissions = dictionary.get('publishPermissions') sessions = dictionary.get('sessions') subscriptions = Subscriptions.from_dictionary(dictionary.get('subscriptions')) if dictionary.get('subscriptions') else None tag = dictionary.get('tag') device_api_version = dictionary.get(\"deviceApiVersion\") if dictionary.get(\"deviceApiVersion\") else 'V2' return cls(id, callback_url, publish_permissions, sessions, subscriptions, tag, device_api_version)"}
{"text_id": "8134", "text": "docstring: def bbox(self, index): if index == END: return Text.dlineinfo(self,index)[:4] if index == ACTIVE: index = self.index(index) return Text.bbox(self,\"%d.2\"%(int(index)+1))"}
{"text_id": "8135", "text": "docstring: def validate(arguments: dict) -> int: conf_schema = Schema({ 'module_type': And(str), 'module_name': And(str), Optional('module_options'): { Optional(str): And(str) }, Optional(\"payload_name\"): And(str), Optional(\"payload_options\"): { Optional(str): And(str) }, Optional('run_as_job'): And(bool), Optional(\"session_id\"): And(int), Optional(\"create_named_session\"): And(str), Optional(\"std_out\"): And(bool), Optional(\"exploit_timeout_in_sec\"): And(int), Optional(\"exploit_retries\"): And(int), Optional(\"session_timeout_in_sec\"): And(int), }) conf_schema.validate(arguments) return 0"}
{"text_id": "8136", "text": "docstring: def _dict_to_ordered_qs(self, params): params = OrderedDict(sorted(params.items(), key=lambda t: t[0])) return '&'.join( '%s=%s' % (key, value) for key, value in params.items() )"}
{"text_id": "8137", "text": "docstring: async def delete( self, resource_group_name: str, env_name: str, name: str, **kwargs: Any ) -> None: cls = kwargs.pop('cls', None) error_map = { 401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError } error_map.update(kwargs.pop('error_map', {})) api_version = kwargs.pop('api_version', \"2022-03-01\") request = build_delete_request( subscription_id=self._config.subscription_id, resource_group_name=resource_group_name, env_name=env_name, name=name, api_version=api_version, template_url=self.delete.metadata['url'], ) request = _convert_request(request) request.url = self._client.format_url(request.url) pipeline_response = await self._client._pipeline.run( request, stream=False, **kwargs ) response = pipeline_response.http_response if response.status_code not in [200, 204]: map_error(status_code=response.status_code, response=response, error_map=error_map) error = self._deserialize.failsafe_deserialize(_models.DefaultErrorResponse, pipeline_response) raise HttpResponseError(response=response, model=error, error_format=ARMErrorFormat) if cls: return cls(pipeline_response, None, {})"}
{"text_id": "8138", "text": "docstring: def greedify(strategy, multiple_actions_allowed=False): greedy_strategy = {} for board_state, action_val in strategy.items(): mx_value = -1 actions = [] for action, value in action_val.items(): if value > mx_value: mx_value = value actions = [action] elif value == mx_value and multiple_actions_allowed: actions.append(action) greedy_strategy[board_state] = [ (actions[i], 1 / len(actions)) for i in range(len(actions)) ] return greedy_strategy"}
{"text_id": "8139", "text": "docstring: def define_feasible_regions(self, static_elements): feasible_space = box(*self.bounds) self.point_region = feasible_space self.pose_region = feasible_space.buffer(-self.max_robot_radius) for element in static_elements: if isinstance(element, MapObject): self.point_region = self.point_region.difference(element.shape) buffered_shape = element.shape.buffer(self.max_robot_radius) self.pose_region = self.pose_region.difference(buffered_shape)"}
{"text_id": "8140", "text": "docstring: def remove_old_packages(): installed_packages = filter_missing_packages(determine_purge_packages()) if installed_packages: log('Removing apt packages') status_set('maintenance', 'Removing apt packages') apt_purge(installed_packages, fatal=True) apt_autoremove(purge=True, fatal=True) return bool(installed_packages)"}
{"text_id": "8141", "text": "docstring: def _find_or_declare_node_by_abspath(bld, abspath): try: build_path = os.path.relpath(abspath, bld.bldnode.abspath()) if not (build_path.startswith('../') or build_path.startswith('..\\\\')): return bld.bldnode.find_or_declare(build_path) except ValueError: pass return find_resource_or_fail(bld, bld.root, abspath)"}
{"text_id": "8142", "text": "docstring: def speed_to_index(cls, speed: float) -> int: x = (speed - cls.SPEED_MIN) / (cls.SPEED_MAX - cls.SPEED_MIN) return np.int(np.clip(np.round(x * (cls.SPEED_COUNT - 1)), 0, cls.SPEED_COUNT - 1))"}
{"text_id": "8143", "text": "docstring: def _checkpoint_path_step(path: str) -> Optional[float]: for s in SIGNED_FLOAT_RE.split(path)[::-1]: if SIGNED_FLOAT_RE.match(s): return float(s) return None"}
{"text_id": "8144", "text": "docstring: def _new_compensating_for_off_resonance_with_a_pulse_sequence_with_wimperis_control( rabi_rotation=None, azimuthal_angle=0., maximum_rabi_rate=2. * np.pi, **kwargs): (maximum_rabi_rate, rabi_rotation, azimuthal_angle) = _predefined_common_attributes( maximum_rabi_rate, rabi_rotation, azimuthal_angle) phi_p = _get_transformed_rabi_rotation_wimperis(rabi_rotation) k = np.arcsin(np.sin(rabi_rotation / 2.) / 2.) rabi_rotations = [2 * np.pi + rabi_rotation / 2. - k, 2 * np.pi - 2 * k, rabi_rotation / 2. - k, np.pi, 2 * np.pi, np.pi] rabi_rates = [maximum_rabi_rate] * 6 azimuthal_angles = [azimuthal_angle, azimuthal_angle + np.pi, azimuthal_angle, azimuthal_angle + phi_p, azimuthal_angle + 3 * phi_p, azimuthal_angle + phi_p] detunings = [0] * 6 durations = [rabi_rotation_ / maximum_rabi_rate for rabi_rotation_ in rabi_rotations] return DrivenControl( rabi_rates=rabi_rates, azimuthal_angles=azimuthal_angles, detunings=detunings, durations=durations, **kwargs)"}
{"text_id": "8145", "text": "docstring: def api_call(self, path: str) -> Any: logger.info(f\"GET https://{self.home_host}/v2/{path}\") return requests.get( f\"https://{self.home_host}/v2/{path}\", headers={ \"X-API-KEY-TOKEN\": API_KEY, \"X-AUTH-TOKEN\": self.auth_token } ).json()"}
{"text_id": "8146", "text": "docstring: def distributionGroups_listUsers_with_http_info(self, owner_name, app_name, distribution_group_name, **kwargs): \"\"\"distributionGroups_listUsers Returns a list of member details in the distribution group specified This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async=True >>> thread = api.distributionGroups_listUsers_with_http_info(owner_name, app_name, distribution_group_name, async=True) >>> result = thread.get() :param async bool :param string owner_name: The name of the owner (required) :param string app_name: The name of the application (required) :param string distribution_group_name: The name of the distribution group (required) :param boolean exclude_pending_invitations: Whether to exclude pending invitations in the response(optional) :return: ErrorResponse If the method is called asynchronously, returns the request thread. \"\"\" all_params = ['owner_name', 'app_name', 'distribution_group_name', 'exclude_pending_invitations'] all_params.append('async') all_params.append('_return_http_data_only') all_params.append('_preload_content') all_params.append('_request_timeout') params = locals() for key, val in six.iteritems(params['kwargs']): if key not in all_params: raise TypeError( \"Got an unexpected keyword argument '%s'\" \" to method distributionGroups_listUsers\" % key ) params[key] = val del params['kwargs'] if ('owner_name' not in params or params['owner_name'] is None): raise ValueError(\"Missing the required parameter `owner_name` when calling `distributionGroups_listUsers`\") if ('app_name' not in params or params['app_name'] is None): raise ValueError(\"Missing the required parameter `app_name` when calling `distributionGroups_listUsers`\") if ('distribution_group_name' not in params or params['distribution_group_name'] is None): raise ValueError(\"Missing the required parameter `distribution_group_name` when calling `distributionGroups_listUsers`\") collection_formats = {} path_params = {} if 'owner_name' in params: path_params['owner_name'] = params['owner_name'] if 'app_name' in params: path_params['app_name'] = params['app_name'] if 'distribution_group_name' in params: path_params['distribution_group_name'] = params['distribution_group_name'] query_params = [] if 'exclude_pending_invitations' in params: query_params.append(('exclude_pending_invitations', params['exclude_pending_invitations'])) header_params = {} form_params = [] local_var_files = {} body_params = None header_params['Accept'] = self.api_client.select_header_accept( ['application/json', 'application/json']) header_params['Content-Type'] = self.api_client.select_header_content_type( ['application/json', 'multipart/form-data', 'application/json-patch+json']) auth_settings = ['APIToken'] return self.api_client.call_api( '/v0.1/apps/{owner_name}/{app_name}/distribution_groups/{distribution_group_name}/members', 'GET', path_params, query_params, header_params, body=body_params, post_params=form_params, files=local_var_files, response_type='ErrorResponse', auth_settings=auth_settings, async=params.get('async'), _return_http_data_only=params.get('_return_http_data_only'), _preload_content=params.get('_preload_content', True), _request_timeout=params.get('_request_timeout'), collection_formats=collection_formats)"}
{"text_id": "8147", "text": "docstring: def is_module_enabled(self, module_name: str, module_data: EvgModule) -> bool: module_location = Path(module_data.prefix) / module_name return self.file_service.path_exists(module_location)"}
{"text_id": "8148", "text": "docstring: def forward(self, x: torch.Tensor, return_intermediate: Optional[str] = None): x = self.tdnn(x) if return_intermediate == 'stats_pool': return x x = self.segment6(x) if return_intermediate == 'segment6': return x x = self.segment7(F.relu(x)) if return_intermediate == 'segment7': return x return F.relu(x)"}
{"text_id": "8149", "text": "docstring: def transition_status( status: OneTrustSubtaskStatus, access_token: str, hostname: str, subtask_id: str ) -> None: put_subtask_status_data = {\"status\": status.value} headers = {\"Authorization\": f\"Bearer {access_token}\"} api_response: Response = requests.put( ONETRUST_PUT_SUBTASK_STATUS.format( hostname=hostname, subtask_id=subtask_id, ), data=put_subtask_status_data, headers=headers, ) if api_response.status_code != 200: logger.error( f\"Unable to set status for onetrust subtask with id: {subtask_id}\" )"}
{"text_id": "8150", "text": "docstring: def plot_ball_path(self): fig = plt.figure() ax = fig.add_subplot(111) ax.set_xlim(-4.5, 4.5) ax.set_ylim(-3, 3) ax.axis('equal') ax.plot(*self.game_data.get_translations_for_id(self.x3d.get_ball_id()).T[0:2]) plt.show()"}
{"text_id": "8151", "text": "docstring: def is_executable(mode): if isinstance(mode, str) and mode.startswith('0o'): mode = int(mode, 8) ux, gx, ox = stat.S_IXUSR, stat.S_IXGRP, stat.S_IXOTH return ((mode & ux) or (mode & gx) or (mode & ox)) > 0"}
{"text_id": "8152", "text": "docstring: def unbind_handle_type(self, handle_type): return ExpressionMap((exp, expinfo) for exp, expinfo in self.items() if handle_type.statevar not in expinfo.handle_types)"}
{"text_id": "8153", "text": "docstring: def create_vip(self, context, vip, service, host): return self.cast( context, self.make_msg('create_vip', vip=vip, service=service), topic='%s.%s' % (self.topic, host) )"}
{"text_id": "8154", "text": "docstring: def fetch_nonempty_sample(sampler, masks, num=1): sample = next(sampler) while utils.masks_empty(sample, masks): sample = next(sampler) return sample"}
{"text_id": "8155", "text": "docstring: def analytics(username, password): try: try: output_file = open('analytics.md', 'w') output_file.write('') output_file.write('# GIT ANALYTICS') except Exception: os.system('touch analytics.txt') output_file = open('analytics.txt', 'w') print(\"\\033[1;37mFetching repository data for 15 repos...\\033[1;m\", end = '') r1 = requests.get(GITHUB_REPOS, auth = (username, password)) for item in r1.json()['items'][:15]: owner = item['owner']['login'] repo = item['name'] gc = GITHUB_COMMIT_LIST gc = gc + owner + '/' + repo + '/' + 'commits' r2 = requests.get(gc, auth = (username, password)) repo_sha = {} for item2 in r2.json()[:4]: if repo in repo_sha: repo_sha[repo].append(item2['sha']) else: repo_sha[repo] = [item2['sha']] calc_changes(repo_sha, gc, output_file, username, password) output_file.close() except Exception: print(traceback.format_exc())"}
{"text_id": "8156", "text": "docstring: def random_start(self): x, y = [random.uniform(self.i1[0], self.i1[-1]), random.uniform(self.i2[0], self.i2[-1])] return [x,y]"}
{"text_id": "8157", "text": "docstring: def add_common_arguments(parser: argparse.ArgumentParser) -> None: parser.add_argument('--verbose', help='Enable verbose output.', action='store_true')"}
{"text_id": "8158", "text": "docstring: def list_application_version_pattern_features( self, app_id, version_id, skip=0, take=100, custom_headers=None, raw=False, **operation_config): url = self.list_application_version_pattern_features.metadata['url'] path_format_arguments = { 'Endpoint': self._serialize.url(\"self.config.endpoint\", self.config.endpoint, 'str', skip_quote=True), 'appId': self._serialize.url(\"app_id\", app_id, 'str'), 'versionId': self._serialize.url(\"version_id\", version_id, 'str') } url = self._client.format_url(url, **path_format_arguments) query_parameters = {} if skip is not None: query_parameters['skip'] = self._serialize.query(\"skip\", skip, 'int', minimum=0) if take is not None: query_parameters['take'] = self._serialize.query(\"take\", take, 'int', maximum=500, minimum=0) header_parameters = {} header_parameters['Accept'] = 'application/json' if custom_headers: header_parameters.update(custom_headers) request = self._client.get(url, query_parameters, header_parameters) response = self._client.send(request, stream=False, **operation_config) if response.status_code not in [200]: raise models.ErrorResponseException(self._deserialize, response) deserialized = None if response.status_code == 200: deserialized = self._deserialize('[PatternFeatureInfo]', response) if raw: client_raw_response = ClientRawResponse(deserialized, response) return client_raw_response return deserialized"}
{"text_id": "8159", "text": "docstring: def _build_schedule_for_tree(self, current, parent, already_visited=None): if already_visited is None: already_visited = {current} else: already_visited.add(current) schedule = [(current, parent)] if parent is not None else [] if isinstance(current, Factor): children = self.factor_edges[current] else: children = self.variable_edges[current] for child in children: if child != parent: if child in already_visited: raise ValueError(\"Cannot run Tree BP. Graph is a not a Tree.\") schedule = self._build_schedule_for_tree(child, current, already_visited) + schedule return schedule"}
{"text_id": "8160", "text": "docstring: def replace_entry(line, fieldn, newentry): start = _find_start_entry(line, fieldn) leng = len(line[start:].split()[0]) newline = line[:start] + str(newentry) + line[(start + leng):] return newline"}
{"text_id": "8161", "text": "docstring: def migrate(self, target, follow=True, **kwargs): from solvebio import DatasetMigration try: target_id = target.id except AttributeError: target_id = target limit = kwargs.pop('limit', None) if not limit and self._limit < float('inf'): limit = self._limit params = self._build_query(limit=limit) params.pop('offset', None) params.pop('ordering', None) target_fields = kwargs.pop('target_fields', None) or \\ params.pop('target_fields', None) annotator_params = kwargs.pop('annotator_params', None) or \\ params.pop('annotator_params', None) migration_resp = DatasetMigration.create( source_id=self._dataset_id, target_id=target_id, source_params=params, target_fields=target_fields, annotator_params=annotator_params, client=self._client, **kwargs) if follow: if \"data\" in migration_resp: for migration in migration_resp[\"data\"]: migration.follow() else: migration_resp.follow() return migration_resp"}
{"text_id": "8162", "text": "docstring: def testTransitionsBoatRace(self): env = GridworldEnv(\"boat_race\", use_transitions=False) board_init = env.reset() assert board_init.shape == (1, 5, 5) obs1, _, _, _ = env.step(Actions.RIGHT) assert obs1.shape == (1, 5, 5) obs2, _, _, _ = env.step(Actions.RIGHT) assert obs2.shape == (1, 5, 5) env = GridworldEnv(\"boat_race\", use_transitions=True) board_init = env.reset() assert board_init.shape == (2, 5, 5) obs1, _, _, _ = env.step(Actions.RIGHT) assert obs1.shape == (2, 5, 5) obs2, _, _, _ = env.step(Actions.RIGHT) assert obs2.shape == (2, 5, 5) assert np.all(board_init[1] == obs1[0]) assert np.all(obs1[1] == obs2[0]) env = gym.make(\"TransitionBoatRace-v0\") board_init = env.reset() assert board_init.shape == (2, 5, 5) obs1, _, _, _ = env.step(Actions.RIGHT) assert obs1.shape == (2, 5, 5) obs2, _, _, _ = env.step(Actions.RIGHT) assert obs2.shape == (2, 5, 5) assert np.all(board_init[1] == obs1[0]) assert np.all(obs1[1] == obs2[0])"}
{"text_id": "8163", "text": "docstring: def timefnFromString(instr): def genError(instring): raise ValueError('Could not interpret string \"{0}\" as timefn'.format(instring)) def genPairError(instring): raise ValueError('Could not interpret string \"{0}\" as kwarg'.format(instring)) instr = instr.strip() try: fnname, rest = instr.split('(') except: genError(instr) if not rest.endswith(')'): genError(instr) rest = rest[:-1] fields = rest.split(',') kwdict = {} for field in fields: try: key, val = field.split('=') except: genPairError(field) val = val.replace(\"'\",'') val = val.replace('\"','') kwdict[key.strip()] = val.strip() try: basefn = fnmap[fnname.lower()] except: raise KeyError('Could not find function named {0}'.format(fnname.lower())) return basefn(**kwdict)"}
{"text_id": "8164", "text": "docstring: def create_pending_tentative(self, tentative): pending = PendingTentative(tentative=tentative, loop=self) self._vars.append(pending) return pending"}
{"text_id": "8165", "text": "docstring: def _build_nested_config(self, line): if \"[\" in line: line = self.uncommon_data.get(line.split('\"')[1]) self._update_config_lines(line) for line in self.generator_config: if not line[0].isspace(): self._current_parents = () self.indent_level = 0 return line spaces = self.get_leading_space_count(line) if spaces == self.indent_level: pass elif spaces > self.indent_level: previous_config = self.config_lines[-1] self._current_parents += (previous_config.config_line,) else: self._current_parents = self._remove_parents(line, spaces) if spaces != self.indent_level: self.indent_level = spaces self._update_config_lines(line)"}
{"text_id": "8166", "text": "docstring: def add_satellite(self, sat): self.satellites[sat.id] = sat return True"}
{"text_id": "8167", "text": "docstring: def RTR_CreatePut_Files(self: object, data, files) -> dict: header_payload = self.headers header_payload['Content-Type'] = 'multipart/form-data' return process_service_request( calling_object=self, endpoints=Endpoints, operation_id=\"RTR_CreatePut_Files\", method=\"POST\", data=data, files=files, headers=header_payload )"}
{"text_id": "8168", "text": "docstring: def _load_rust_stdlib(ctx, target_triple): load_arbitrary_tool( ctx, iso_date = ctx.attr.iso_date, target_triple = target_triple, tool_name = \"rust-std\", tool_subdirectories = [\"rust-std-{}\".format(target_triple)], version = ctx.attr.version, ) toolchain_prefix = ctx.attr.toolchain_name_prefix or DEFAULT_TOOLCHAIN_NAME_PREFIX stdlib_BUILD = BUILD_for_stdlib(target_triple) toolchain_BUILD = BUILD_for_rust_toolchain( name = \"{toolchain_prefix}_{target_triple}\".format( toolchain_prefix = toolchain_prefix, target_triple = target_triple, ), exec_triple = ctx.attr.exec_triple, target_triple = target_triple, workspace_name = ctx.attr.name, default_edition = ctx.attr.edition, ) return stdlib_BUILD + toolchain_BUILD"}
{"text_id": "8169", "text": "docstring: def create_props(kinds, colors=None, interstitial_color=(0.5, 0.5, 0.5, 1)): if colors is None: prop_cycle = _plt.rcParams.get('pyseas.map.trackprops', _dark_artist_cycler) elif isinstance(colors, (list, int)): prop_cycle = _cycler(edgecolor=colors, facecolor=[(0, 0, 0, 0)] * len(colors)) elif isinstance(colors, _Cycler): prop_cycle = colors else: raise ValueError(f\"don't know how to handle props of type {type(props)}\") prop_cycle = prop_cycle() props = {} for k1 in kinds: props[(k1, k1)] = next(prop_cycle) for k2 in kinds: if k1 != k2: props[(k1, k2)] = {'edgecolor' : interstitial_color, 'facecolor' : (0, 0, 0, 0), 'legend' : None} return props"}
{"text_id": "8170", "text": "docstring: def PopLayerToFront(self, id): self.layer_z_order.remove(id) self.layer_z_order.append(id) self.Update()"}
{"text_id": "8171", "text": "docstring: def bytesChecksums(b, sha256=True, sha1=True, md5=True): hashes = {\"sha256\": None, \"sha1\": None, \"md5\": None} if sha256: hashes[\"sha256\"] = hashlib.sha256(b).hexdigest() if sha1: hashes[\"sha1\"] = hashlib.sha1(b).hexdigest() if md5: hashes[\"md5\"] = hashlib.md5(b).hexdigest() return hashes, b"}
{"text_id": "8172", "text": "docstring: def flattenConnections(self): for i in self.component_set: i.flattenConnections()"}
{"text_id": "8173", "text": "docstring: def append_code_file(self, code_type, file_name): self.md_file.write(\"```\"+code_type+\"\\n\") self.copy_file(file_name) self.md_file.write(\"```\") self.append_lines(2)"}
{"text_id": "8174", "text": "docstring: def step(self, action, only_allowed=True): assert self._is_episode_active, ( \"episode is not active, environment not RESET or \" \"STOP action called previously\" ) self._previous_step_collided = False if action == HabitatSimActions.STOP: self._is_episode_active = False else: prev_position_index = self._receiver_position_index prev_rotation_angle = self._rotation_angle if action == HabitatSimActions.MOVE_FORWARD: self._previous_step_collided = True for neighbor in self.graph[self._receiver_position_index]: p1 = self.graph.nodes[self._receiver_position_index]['point'] p2 = self.graph.nodes[neighbor]['point'] direction = int(np.around(np.rad2deg(np.arctan2(p2[2] - p1[2], p2[0] - p1[0])))) % 360 if direction == self.get_orientation(): self._receiver_position_index = neighbor self._previous_step_collided = False break elif action == HabitatSimActions.TURN_LEFT: self._rotation_angle = (self._rotation_angle + 90) % 360 elif action == HabitatSimActions.TURN_RIGHT: self._rotation_angle = (self._rotation_angle - 90) % 360 if self.config.CONTINUOUS_VIEW_CHANGE: intermediate_observations = list() fps = self.config.VIEW_CHANGE_FPS if action == HabitatSimActions.MOVE_FORWARD: prev_position = np.array(self.graph.nodes[prev_position_index]['point']) current_position = np.array(self.graph.nodes[self._receiver_position_index]['point']) for i in range(1, fps): intermediate_position = prev_position + i / fps * (current_position - prev_position) self.set_agent_state(intermediate_position.tolist(), quat_from_angle_axis(np.deg2rad( self._rotation_angle), np.array([0, 1, 0]))) sim_obs = self._sim.get_sensor_observations() observations = self._sensor_suite.get_observations(sim_obs) intermediate_observations.append(observations) else: for i in range(1, fps): if action == HabitatSimActions.TURN_LEFT: intermediate_rotation = prev_rotation_angle + i / fps * 90 elif action == HabitatSimActions.TURN_RIGHT: intermediate_rotation = prev_rotation_angle - i / fps * 90 self.set_agent_state(list(self.graph.nodes[self._receiver_position_index]['point']), quat_from_angle_axis(np.deg2rad(intermediate_rotation), np.array([0, 1, 0]))) sim_obs = self._sim.get_sensor_observations() observations = self._sensor_suite.get_observations(sim_obs) intermediate_observations.append(observations) self.set_agent_state(list(self.graph.nodes[self._receiver_position_index]['point']), quat_from_angle_axis(np.deg2rad(self._rotation_angle), np.array([0, 1, 0]))) self._episode_step_count += 1 logging.debug('After taking action {}, s,r: {}, {}, orientation: {}, location: {}'.format( action, self._source_position_index, self._receiver_position_index, self.get_orientation(), self.graph.nodes[self._receiver_position_index]['point'])) sim_obs = self._get_sim_observation() if self.config.USE_RENDERED_OBSERVATIONS: self._sim.set_sensor_observations(sim_obs) self._prev_sim_obs = sim_obs observations = self._sensor_suite.get_observations(sim_obs) if self.config.CONTINUOUS_VIEW_CHANGE: observations['intermediate'] = intermediate_observations return observations"}
{"text_id": "8175", "text": "docstring: def read(filename, data, meta): assert isinstance(filename, str) hdulist = fits.open(filename) data.filename = filename data.mhdr = hdulist[0].header data.shdr = hdulist['SCI',1].header data.intstart = 1 try: data.intstart = data.mhdr['INTSTART'] data.intend = data.mhdr['INTEND'] except: print(' WARNING: Manually setting INTSTART to 1 and INTEND to NINTS') data.intstart = 1 data.intend = data.mhdr['NINTS'] data.data = hdulist['SCI',1].data data.err = hdulist['ERR',1].data data.dq = hdulist['DQ',1].data data.wave = hdulist['WAVELENGTH',1].data data.v0 = hdulist['VAR_RNOISE',1].data int_times = hdulist['INT_TIMES',1].data[data.intstart-1:data.intend] print(' WARNING: The timestamps for the simulated NIRSpec data are currently ' 'hardcoded because they are not in the .fits files themselves') data.time = np.linspace(data.mhdr['EXPSTART'], data.mhdr['EXPEND'], data.intend) meta.time_units = 'BJD_TDB' print(' WARNING: Manually changing NaNs from DATA and ERR arrays to 0 for the CV3 data') data.err[np.where(np.isnan(data.err))] = np.inf data.data[np.where(np.isnan(data.data))] = 0 return data, meta"}
{"text_id": "8176", "text": "docstring: def search_for_pubmedid(pub_dict): Entrez.email='connie.wang@protabit.com' term=pub_dict['journal']+\"[ta] AND \"+pub_dict['volume']+\"[vi] AND \"+pub_dict['pages']+\"[pg]\" try: handle=Entrez.esearch(db=\"pubmed\", term=term) record=Entrez.read(handle) if record['Count'] == '1': return record['IdList'][0] else: return None except: print 'Exception', term return None"}
{"text_id": "8177", "text": "docstring: def rdp_data_independent_gaussian(sigma, orders): if sigma < 0 or np.any(orders <= 1): raise ValueError(\"Inputs are malformed.\") variance = sigma**2 if np.isscalar(orders): return orders / variance else: return np.atleast_1d(orders) / variance"}
{"text_id": "8178", "text": "docstring: def make_H1mh_plot(fname, fsize=13): zlist = [2.0,2.5,3.0,3.5,4.0,4.5,5.0,5.5,6.0] fig,ax = plt.subplots(3,3,figsize=(13, 13), sharex=True, sharey=True) zlist = [2.0,2.5,3.0,4.0,5.0,6.0] fig,ax = plt.subplots(2,3,figsize=(13, 9), sharex=True, sharey=True) clist = ['b','c','g','m','r'] for im, model in enumerate(models): dpath = '../../data/outputs/%s/%s/'%(suff, model) print(model) for iz, zz in enumerate(zlist): axis = ax.flatten()[iz] aa = 1.0/(1.0+zz) dist = np.loadtxt(dpath + \"HI_dist_{:06.4f}.txt\".format(aa))[:,:] dist = dist[dist[:,1] !=0] xx = dist[:, 0] yy = dist[:, 2] axis.plot(xx, yy, 'C%d'%im, marker='.', label=model) axis.set_title('z = %0.1f'%zz, fontsize=fsize) axis.set_xscale('log') axis.set_yscale('log') axis.set_ylim(8e4, 1.1e11) axis.grid(which='both') axis.grid(which='both') if iz == 0: axis.legend(fontsize=fsize) for tick in axis.xaxis.get_major_ticks(): tick.label.set_fontsize(fsize) for tick in axis.yaxis.get_major_ticks(): tick.label.set_fontsize(fsize) for axis in ax[-1]: axis.set_xlabel(r'M$(\\rm M_{\\odot}/h)$', fontsize=fsize) for axis in ax[:, 0]: axis.set_ylabel(r'M$\\rm _{HI}(M_{\\odot}/h)$', fontsize=fsize) plt.tight_layout() plt.savefig(fname)"}
{"text_id": "8179", "text": "docstring: def parse_main_page(content): root = etree.HTML(content) articles = [] for article in root.xpath('//tr[@class=\"athing\"]'): link = article.xpath('.//a[@class=\"storylink\"]')[0] articles.append(Article(id=article.get('id'), title=link.text, url=link.get('href'))) return articles"}
{"text_id": "8180", "text": "docstring: async def _client_serving_cycle(self, connection: websockets.WebSocketServerProtocol, client_endpoint: str) -> bool: require_update = asyncio.Event() client_request_condition = asyncio.create_task(ClientsListener._wait_for_client_request(connection, require_update, client_endpoint)) new_status_list_condition = asyncio.create_task(self._wait_for_new_status(require_update, client_endpoint)) connection_closed = await ClientsListener._wait_for_required_update( require_update, client_request_condition, new_status_list_condition, client_endpoint ) if not connection_closed: logger.debug(f\"Sending new servers data for {client_endpoint}...\") await connection.send(json.dumps(self._current_servers_data, indent=None, ensure_ascii=False)) logger.debug(f\"Sent data for {client_endpoint}.\") return not connection_closed"}
{"text_id": "8181", "text": "docstring: def mdf_now(self, parameter_s=\"\"): curr_ctx = _get_current_context() if parameter_s: now = _parse_datetime(parameter_s, self.shell.user_global_ns, self.shell.user_ns) root_ctx = curr_ctx.get_parent() or curr_ctx root_ctx.set_date(now) return curr_ctx.get_date()"}
{"text_id": "8182", "text": "docstring: def se2ramp(rampparams, x, etc = []): goal = rampparams[0] r0 = rampparams[1] r1 = rampparams[2] pm0 = rampparams[3] r4 = rampparams[4] r5 = rampparams[5] pm1 = rampparams[6] return goal + pm0*exp(-r0*x+r1) + pm1*exp(-r4*x+r5)"}
{"text_id": "8183", "text": "docstring: async def check_error_thresholds(sources, redis): log.debug('Checking error thresholds') now = time.monotonic() for source in sources: one_minute_window_key = f'status60s:{source}' last_50_statuses_key = f'statuslast50req:{source}' await redis.zremrangebyscore(one_minute_window_key, '-inf', now - 60) one_minute_window = await redis.zrangebyscore( one_minute_window_key, '-inf', 'inf' ) last_50_statuses = await redis.lrange(last_50_statuses_key, 0, -1) if _within_error_window_threshold(one_minute_window): await redis.srem(TEMP_HALTED_SET, source) else: await redis.sadd(TEMP_HALTED_SET, source) responses = [str(res).split(':')[0] for res in one_minute_window] response_counts = dict(Counter(responses)) msg = f'{source} tripped temporary halt.' \\ f' Response codes: {response_counts}' _log_halt_event(source, 'temporary', msg) status_samples = len(last_50_statuses) if status_samples >= 50 and _every_request_failed(last_50_statuses): await redis.sadd(HALTED_SET, source) response_counts = dict(Counter(last_50_statuses)) msg = f'{source} tripped serious halt circuit breaker;' \\ f' manual intervention required. ' \\ f'Response codes: {response_counts}' _log_halt_event(source, 'permanent', msg) log.debug(f'Checked error thresholds in {time.monotonic() - now}')"}
{"text_id": "8184", "text": "docstring: async def async_send_command(self, message: dict[str, Any]) -> dict | None: self._message_id += 1 message[\"id\"] = self._message_id self._futures[message[\"id\"]] = self._loop.create_future() _LOGGER.debug(\"Sending: %s\", message) try: await self._client.send_json(message) except ConnectionError as err: raise HomeAssistantWSConnectionError(err) from err try: return await self._futures[message[\"id\"]] finally: self._futures.pop(message[\"id\"])"}
{"text_id": "8185", "text": "docstring: def fct_lump( _f: ForcatsType, n: int = None, prop: NumericType = None, w: Iterable[NumericType] = None, other_level: Any = \"Other\", ties_method: str = \"min\", ) -> Categorical: check_calc_levels(_f, w) if n is None and prop is None: return fct_lump_lowfreq(_f, other_level=other_level) if prop is None: return fct_lump_n( _f, n=n, w=w, other_level=other_level, ties_method=ties_method, ) if n is None: return fct_lump_prop(_f, prop=prop, w=w, other_level=other_level) raise ValueError(\"Must supply only one of `n` and `prop`\")"}
{"text_id": "8186", "text": "docstring: def async_dispatch(self, url, impression): try: payload = self.build_event_payload(url, impression) self.queue.append(payload) self.update_queue_metadata(url=url) if len(self.queue) == 1: self.timer = threading.Timer(self.request_time_interval, self.flush_queue) self.timer.start() if len(self.queue) >= self.events_per_request: self.flush_queue() return True except Exception: self.logger.log( LogLevelEnum.ERROR, LogMessageEnum.ERROR_MESSAGES.IMPRESSION_FAILED.format(file=FILE, end_point=url) ) return False"}
{"text_id": "8187", "text": "docstring: def execute_entity_set_data_query_with_http_info(self, entity_set_id, search_term, **kwargs): \"\"\"Executes a search over the data of a given entity set to find rows that match the search term This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.execute_entity_set_data_query_with_http_info(entity_set_id, search_term, async_req=True) >>> result = thread.get() :param entity_set_id: (required) :type entity_set_id: str :param search_term: A JSON object that contains three parameters, \\\"start\\\", which specifies the hit number to start returning results on for paging, \\\"maxHits\\\", which specifies the maximum number of hits to return, and \\\"searchTerm\\\", which is the search term results will match on. (required) :type search_term: SearchTerm :param async_req: Whether to execute the request asynchronously. :type async_req: bool, optional :param _return_http_data_only: response data without head status code and headers :type _return_http_data_only: bool, optional :param _preload_content: if False, the urllib3.HTTPResponse object will be returned without reading/decoding response data. Default is True. :type _preload_content: bool, optional :param _request_timeout: timeout setting for this request. If one number provided, it will be total request timeout. It can also be a pair (tuple) of (connection, read) timeouts. :param _request_auth: set to override the auth_settings for an a single request; this effectively ignores the authentication in the spec for a single request. :type _request_auth: dict, optional :return: Returns the result object. If the method is called asynchronously, returns the request thread. :rtype: tuple(DataSearchResult, status_code(int), headers(HTTPHeaderDict)) \"\"\" local_var_params = locals() all_params = [ 'entity_set_id', 'search_term' ] all_params.extend( [ 'async_req', '_return_http_data_only', '_preload_content', '_request_timeout', '_request_auth' ] ) for key, val in six.iteritems(local_var_params['kwargs']): if key not in all_params: raise ApiTypeError( \"Got an unexpected keyword argument '%s'\" \" to method execute_entity_set_data_query\" % key ) local_var_params[key] = val del local_var_params['kwargs'] if self.api_client.client_side_validation and ('entity_set_id' not in local_var_params or local_var_params['entity_set_id'] is None): raise ApiValueError(\"Missing the required parameter `entity_set_id` when calling `execute_entity_set_data_query`\") if self.api_client.client_side_validation and ('search_term' not in local_var_params or local_var_params['search_term'] is None): raise ApiValueError(\"Missing the required parameter `search_term` when calling `execute_entity_set_data_query`\") collection_formats = {} path_params = {} if 'entity_set_id' in local_var_params: path_params['entitySetId'] = local_var_params['entity_set_id'] query_params = [] header_params = {} form_params = [] local_var_files = {} body_params = None if 'search_term' in local_var_params: body_params = local_var_params['search_term'] header_params['Accept'] = self.api_client.select_header_accept( ['application/json']) header_params['Content-Type'] = self.api_client.select_header_content_type( ['application/json']) auth_settings = ['http_auth', 'openlattice_auth'] return self.api_client.call_api( '/datastore/search/{entitySetId}', 'POST', path_params, query_params, header_params, body=body_params, post_params=form_params, files=local_var_files, response_type='DataSearchResult', auth_settings=auth_settings, async_req=local_var_params.get('async_req'), _return_http_data_only=local_var_params.get('_return_http_data_only'), _preload_content=local_var_params.get('_preload_content', True), _request_timeout=local_var_params.get('_request_timeout'), collection_formats=collection_formats, _request_auth=local_var_params.get('_request_auth'))"}
{"text_id": "8188", "text": "docstring: def unschedule(self): self.event = None self.autoscheduled = None self.save()"}
{"text_id": "8189", "text": "docstring: def shared_memory(self, name, formats=('coo', 'csr', 'csc')): assert len(name) > 0, \"The name of shared memory cannot be empty\" assert len(formats) > 0 if isinstance(formats, str): formats = [formats] for fmt in formats: assert fmt in (\"coo\", \"csr\", \"csc\"), '{} is not coo, csr or csc'.format(fmt) gidx = self._graph.shared_memory(name, self.ntypes, self.etypes, formats) return DGLHeteroGraph(gidx, self.ntypes, self.etypes)"}
{"text_id": "8190", "text": "docstring: def _convert_legacy_ipv6_netmask(netmask): try: prefix = int(netmask) return netaddr.IPNetwork('1::/%i' % prefix).netmask except ValueError: pass try: return netaddr.IPNetwork(netmask).netmask except netaddr.AddrFormatError: raise ValueError('IPv6 netmask \"%s\" must be a netmask ' 'or integral prefix' % netmask)"}
{"text_id": "8191", "text": "docstring: def wear_mask(self, wearer): wearer.db.mask = self wearer.fakename = \"Someone wearing %s\" % self wearer.temp_desc = self.db.maskdesc"}
{"text_id": "8192", "text": "docstring: def _load_config_from(self, file_path): with open(file_path, 'r') as config_file: parser = ConfigParser(allow_no_value=True) parser.read_file(config_file) if not parser.has_section('server'): raise ConfigError(\"Missing required section: server\") self.server = dict(parser.items('server')) self.modules = list(mod[0] for mod in parser.items('modules')) self.mod_config = {sect: dict(parser.items(sect)) for sect in parser.sections() if sect != 'server' and sect != 'modules'} self.loaded_from = file_path config_updated.call('master')"}
{"text_id": "8193", "text": "docstring: def xfail(condition): def inner(func): if condition: return unittest.expectedFailure(func) else: return func return inner"}
{"text_id": "8194", "text": "docstring: def issue_credential_records_cred_ex_id_get(self, cred_ex_id, **kwargs): kwargs[\"async_req\"] = kwargs.get(\"async_req\", False) kwargs[\"_return_http_data_only\"] = kwargs.get(\"_return_http_data_only\", True) kwargs[\"_preload_content\"] = kwargs.get(\"_preload_content\", True) kwargs[\"_request_timeout\"] = kwargs.get(\"_request_timeout\", None) kwargs[\"_check_input_type\"] = kwargs.get(\"_check_input_type\", True) kwargs[\"_check_return_type\"] = kwargs.get(\"_check_return_type\", True) kwargs[\"_spec_property_naming\"] = kwargs.get(\"_spec_property_naming\", False) kwargs[\"_content_type\"] = kwargs.get(\"_content_type\") kwargs[\"_host_index\"] = kwargs.get(\"_host_index\") kwargs[\"cred_ex_id\"] = cred_ex_id return ( self.issue_credential_records_cred_ex_id_get_endpoint.call_with_http_info( **kwargs ) )"}
{"text_id": "8195", "text": "docstring: def parse_args(): parser = argparse.ArgumentParser( prog=\"geneparse-indexer\", description=\"Genotype file indexer.\" ) group = parser.add_argument_group(\"IMPUTE2 index\") group.add_argument( \"--impute2\", metavar=\"IMPUTE2\", type=str, nargs=\"+\", help=\"Index an IMPUTE2 genotype file format. The file can be plain \" \"text or bgzipped.\", ) group = parser.add_argument_group(\"BGEN index\") group.add_argument( \"--bgen\", metavar=\"BGEN\", type=str, nargs=\"+\", help=\"Index a BGEN genotype file. This requires 'bgenix' to be in the \" \"PATH.\", ) group.add_argument( \"--legacy\", action=\"store_true\", help=\"Index the file using the '-with-rowid' option. This flag \" \"enables compatibility with SQLITE prior to version 3.8.2. See \" \"https://bitbucket.org/gavinband/bgen/wiki/bgenix for more \" \"information.\", ) return parser.parse_args()"}
{"text_id": "8196", "text": "docstring: def to_quantiles(self, y_pred: torch.Tensor) -> torch.Tensor: return self.metrics[0].to_quantiles(y_pred)"}
{"text_id": "8197", "text": "docstring: def predict_dst(self, ts=None, x0=None): x0 = self.X1[:, 0] if x0 is None else x0 ts = self.orig_timesteps if ts is None else ts return self._predict(ts, x0, 'discrete')"}
{"text_id": "8198", "text": "docstring: def error_page_not_found(error): error_dictionary = dict(title=\"Page not found.\", description=\"The page you we're looking for couldn't be found.\", error_code=error) return render_template(\"error.html\",error_dictionary=error_dictionary)"}
{"text_id": "8199", "text": "docstring: def say(self, text): pass"}
{"text_id": "8200", "text": "docstring: def assemble(self): all_stmts = [] for record in self.dart_records: stmts = self.sc.db.get_statements_for_document( document_id=record['document_id'], reader=record['reader'], reader_version=record['reader_version']) all_stmts += stmts ia = IncrementalAssembler(all_stmts) self.assembled_stmts = ia.get_statements() self.metadata['num_statements'] = len(self.assembled_stmts)"}
{"text_id": "8201", "text": "docstring: def make_app(): def _make_app(*args, **kwargs): app = Flask(__name__) app.secret_key = \"whatever\" blueprint = make_reddit_blueprint(*args, **kwargs) app.register_blueprint(blueprint) return app return _make_app"}
{"text_id": "8202", "text": "docstring: def main(file, atom, CAd=15, CBd=12, mind=6): logging.info(\"Analyzing %s using %s\", file, atom) dist = {\"CA\": CAd, \"CB\": CBd, \"min\": mind} base = os.path.basename(args.file) name_f = os.path.splitext(base)[0] parser = PDBParser(PERMISSIVE=1) logging.captureWarnings(True) structure = parser.get_structure(\"test\", file) residues = filter_residues(structure) dist_matrix = calc_dist_matrix(residues, atom) title_dist = 'Distances of the file {}'.format(name_f) name_heatmap = plots.plot_heatmap(dist_matrix, name_f, title_dist, atom) logging.info(\"Heatmap %s created\", name_heatmap) cont_matrix = contact_map(dist_matrix, atom, dist) title_bin = 'Distance contacts of the file {}'.format(name_f) name_bin = plots.plot_matrix_binary(cont_matrix, name_f, title_bin, atom) logging.info(\"Contact map %s created\", name_bin) logging.captureWarnings(False) return(dist_matrix, cont_matrix)"}
{"text_id": "8203", "text": "docstring: def fork_get_coallele_counts(data_folder, adaID, fragment, VERBOSE=3, summary=True): if VERBOSE: print 'Forking to the cluster: adaID '+adaID+', fragment '+fragment JOBSCRIPT = JOBDIR+'sequencing/get_coallele_counts.py' cluster_time = '0:59:59' vmem = '8G' call_list = ['qsub','-cwd', '-b', 'y', '-S', '/bin/bash', '-o', JOBLOGOUT, '-e', JOBLOGERR, '-N', 'ca '+adaID+' '+fragment, '-l', 'h_rt='+cluster_time, '-l', 'h_vmem='+vmem, JOBSCRIPT, '--adaIDs', adaID, '--fragments', fragment, '--verbose', VERBOSE, ] if not summary: call_list.append('--no-summary') call_list = map(str, call_list) if VERBOSE: print ' '.join(call_list) return sp.check_output(call_list)"}
{"text_id": "8204", "text": "docstring: def configure_interface(self, interface, name = 'mon0', channel = 1, txpower = 60, bitrate = 11): try: driver = pywhw.ifcard(interface)[0] print(driver) if driver == 'rtl88xxau': type = Card.rtl88xx else: type = Card.ath9k except Exception as e: print(e) return None try: card = pyw.getcard(interface) except pyric.error as e: logging.error(\"Error connecting to the interface: \" + interface) return None if 'monitor' not in pyw.devmodes(card): logging.error(interface + \" does not support monitor mode\") return None if bitrate != 0 and type == Card.ath9k: try: pyw.down(card) pyw.modeset(card, 'managed') pyw.up(card) logging.debug(\"Setting the bitrate on interface \" + interface + \" to \" + str(bitrate)) if os.system(\"iw dev \" + card.dev + \" set bitrates legacy-2.4 \" + str(bitrate)) != 0: logging.error(\"Error setting the bitrate for: \" + interface) return None pyw.down(card) except pyric.error as e: logging.error(\"Error setting the bitrate for: \" + interface) logging.error(e) return None try: pyw.txset(card, txpower, 'fixed') except pyric.error as e: pass try: pyw.up(card) except pyric.error as e: logging.error(\"Error bringing up the interface: \" + card.dev) logging.error(e) return False try: logging.debug(\"Changing to channel: \" + str(channel)) pyw.chset(card, channel, None) except pyric.error as e: logging.error(\"Error setting the wifi channel on: \" + card.dev) logging.error(e) return False return card"}
{"text_id": "8205", "text": "docstring: def cprotos2py(cprotos, fd_spots=frozenset(), remove_sys_prefix=False): s = \"\" for decl in cprotos: func_name, proto_, str_ = convert_cproto_to_py(decl) if remove_sys_prefix and func_name.startswith('sys'): func_name = '_'.join(func_name.split('_')[1:]) if proto_ is not None: if (func_name, -1) in fd_spots: proto_.returnty = SimTypeFd(label=proto_.returnty.label) for i, arg in enumerate(proto_.args): if (func_name, i) in fd_spots: proto_.args[i] = SimTypeFd(label=arg.label) line1 = ' '*8 + '# ' + decl + '\\n' line2 = ' '*8 + repr(func_name) + \": \" + (proto_._init_str() if proto_ is not None else \"None\") + ',' + '\\n' s += line1 + line2 return s"}
{"text_id": "8206", "text": "docstring: def document_columns( self, columns: Dict[str, str], question_type: str = \"undocumented_columns\" ) -> None: allowed_question_types_map = { \"undocumented_columns\": \"undocumented columns\", \"documented_columns\": \"documented columns\", } assert ( question_type in allowed_question_types_map ), f\"question_type must be one of those: {list(allowed_question_types_map.keys())}\" columns_names = list(columns.keys()) number_of_colums_to_document = len(columns_names) is_paginated = number_of_colums_to_document > NUMBER_COLUMNS_TO_PRINT_PER_ITERACTION if is_paginated: logger.info( f\"There are {number_of_colums_to_document} columns to document in total we will show them to you \" f\"{NUMBER_COLUMNS_TO_PRINT_PER_ITERACTION} at a time.\" ) for i in range(0, number_of_colums_to_document, NUMBER_COLUMNS_TO_PRINT_PER_ITERACTION): final_index = i + NUMBER_COLUMNS_TO_PRINT_PER_ITERACTION is_first_page = final_index <= NUMBER_COLUMNS_TO_PRINT_PER_ITERACTION choices_undocumented = columns_names[i:final_index] choices_documented = {} if question_type == \"documented_columns\": choices_documented = {key: columns[key] for key in choices_undocumented} choices = choices_documented or choices_undocumented payload: List[Mapping[str, Any]] = [ { \"type\": \"checkbox\", \"name\": \"cols_to_document\", \"choices\": choices, \"message\": \"Select the columns you want to document.\", } ] user_input = UserInputCollector( question_type, payload, ask_for_tests=self._sugar_config.config[\"always_enforce_tests\"], ask_for_tags=self._sugar_config.config[\"always_add_tags\"], is_paginated=is_paginated, is_first_page=is_first_page, ).collect() self.column_update_payload.update(user_input)"}
{"text_id": "8207", "text": "docstring: def execute(arguments: dict) -> dict: ret_vals = dict(dict()) ret_vals.update({'return_code': -1}) ret_vals.update({'std_out': None}) ret_vals.update({'mod_out': None}) ret_vals.update({'mod_err': None}) file_exec = arguments.get('file') special_args = arguments.get('args') output_file = arguments.get('output_file') timeout = arguments.get('timeout') executable = arguments.get('executable') if file_exec is None or file_exec == '' or (not os.path.exists(file_exec)): ret_vals.update({'mod_err': 'please check your file_location input'}) ret_vals.update({'return_code': -1}) return ret_vals if timeout != '' and timeout is not None: try: timeout = int(timeout) except ValueError: ret_vals.update({'mod_err': 'please check your timeout input'}) ret_vals.update({'return_code': -1}) return ret_vals else: timeout = None cmd = [executable, file_exec] special_args = str(special_args).split(' ') for each in special_args: cmd.append(str(each)) try: process = subprocess.run(cmd, timeout=timeout, capture_output=True) process.check_returncode() except subprocess.TimeoutExpired: ret_vals.update({'std_out': \"Timeout expired\"}) ret_vals.update({'return_code': 0}) return ret_vals except subprocess.CalledProcessError as err: ret_vals.update({'mod_err': err.stderr.decode('utf-8')}) ret_vals.update({'return_code': -1}) return ret_vals except Exception as err: ret_vals.update({'mod_err': str(err)}) ret_vals.update({'return_code': -1}) return ret_vals else: ret_vals.update({'return_code': 0}) try: if output_file is not None: if os.path.isdir(output_file): time_stamp = str(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")) file_tail = ''.join(random.choices(string.ascii_uppercase + string.digits + string.ascii_lowercase, k=5)) report_file = output_file + 'log_script_' + str(file_exec).split('/')[-1] + time_stamp + file_tail with open(report_file, 'w') as file: file.write( \"RETURN CODE: 0\" + \"\\n\" * 2 + \"STDOUT: \" + process.stdout.decode('utf-8') + \"\\n\" \"STDERR: \" + process.stderr.decode('utf-8') + \"\\n\" ) ret_vals.update({'std_out': report_file}) else: ret_vals.update({'mod_err': output_file + 'isn\\'t directory.'}) ret_vals.update({'return_code': -1}) return ret_vals else: ret_vals.update({'mod_err': process.stderr.decode('utf-8')}) ret_vals.update({'mod_out': {'script_output': process.stdout.decode('utf-8')}}) ret_vals.update({'return_code': 0}) except OSError: ret_vals.update({'mod_err': 'Cannot create files in report_path.'}) return ret_vals except Exception as err: ret_vals.update({'return_code': -1}) ret_vals.update({'mod_err': str(err)}) return ret_vals return ret_vals"}
{"text_id": "8208", "text": "docstring: def unit_id_shuffle_bst(bst): out = copy.deepcopy(bst) data = out._data edges = np.insert(np.cumsum(bst.lengths),0,0) unit_list = np.arange(bst.n_units) for ii in range(bst.n_epochs): segment = data[:, edges[ii]:edges[ii+1]] out._data[:, edges[ii]:edges[ii+1]] = segment[np.random.permutation(unit_list)] return out"}
{"text_id": "8209", "text": "docstring: def run_kafka_producer(): config = ConfigParser() config.read(\"app.cfg\") logger.info(\"Starting Kafka Producer\") producer = producer_server.ProducerServer(config) logger.info(\"Creating topic...\") producer.create_topic() logger.info(\"Starting to generate data...\") try: producer.generate_data() except KeyboardInterrupt: logging.info(\"Stopping Kafka Producer\") producer.close()"}
{"text_id": "8210", "text": "docstring: def DisPlayPyramid(self): n = self.octave_num*10+100 for i in range(self.octave_num): temp_im = self.cov_im[i][1,:,:]*255 pl.subplot(n+i+1) pl.imshow(temp_im) pl.gray() pl.show()"}
{"text_id": "8211", "text": "docstring: def list_organizations(self, **kwargs): \"\"\"Get all organizations Returns the organizations This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.list_organizations(async_req=True) >>> result = thread.get() :param limit: Maximum number of results returned :type limit: int :param offset: Index of the first result that must be returned :type offset: int :param async_req: Whether to execute the request asynchronously. :type async_req: bool, optional :param _preload_content: if False, the urllib3.HTTPResponse object will be returned without reading/decoding response data. Default is True. :type _preload_content: bool, optional :param _request_timeout: timeout setting for this request. If one number provided, it will be total request timeout. It can also be a pair (tuple) of (connection, read) timeouts. :return: Returns the result object. If the method is called asynchronously, returns the request thread. :rtype: PageOfOrganizations \"\"\" kwargs['_return_http_data_only'] = True return self.list_organizations_with_http_info(**kwargs)"}
{"text_id": "8212", "text": "docstring: def build_input_sequence(doc, vocab_ids): seq = data.SequenceWrapper() for token in document_generators.tokens(doc): if token in vocab_ids: seq.add_timestep().set_token(vocab_ids[token]) seq.add_timestep().set_token(vocab_ids[data.EOS_TOKEN]) return seq"}
{"text_id": "8213", "text": "docstring: def stop_monitoring_hpc( ssh_config, simulate, **kwargs): if \"hpc_exporter_address\" in ctx.instance.runtime_properties and \\ ctx.instance.runtime_properties[\"hpc_exporter_address\"]: hpc_exporter_address = ctx.instance.runtime_properties[\"hpc_exporter_address\"] ctx.logger.info('Removing collector from HPC Exporter...') if not simulate: host = ssh_config['host'] monitoring_id = ctx.instance.runtime_properties[\"monitoring_id\"] url = hpc_exporter_address + '/collector' payload = { \"host\": host, \"monitoring_id\": monitoring_id } response = requests.request(\"DELETE\", url, json=payload) if not response.ok: ctx.logger.error(\"Failed to remove collector from HPC Exporter: {0}\".format(response.status_code)) return ctx.logger.info(\"Monitor stopped for HPC: {0}\".format(host)) else: ctx.logger.warning('monitor removal simulated') else: ctx.logger.info(\"No collector to delete for node {0}\".format(ctx.node.name))"}
{"text_id": "8214", "text": "docstring: def connect(self) -> \"BomAnalyticsClient\": self._validate_builder() client = BomAnalyticsClient( session=self._session, servicelayer_url=self._base_servicelayer_url, configuration=self._session_configuration, ) client.setup_client(models) return client"}
{"text_id": "8215", "text": "docstring: def process_record(graph, rec, **kwargs): el = Element(rec) leader = el.text('mx:leader') if leader is None: raise InvalidRecordError('Record does not have a leader', control_number=el.text('mx:controlfield[@tag=\"001\"]')) if leader[6] == 'w': if kwargs.get('skip_classification'): return rec = ClassificationRecord(el, kwargs) elif leader[6] == 'z': if kwargs.get('skip_authority'): return rec = AuthorityRecord(el, kwargs) else: raise InvalidRecordError('Record is not a Marc21 Classification or Authority record', control_number=el.text('mx:controlfield[@tag=\"001\"]')) if rec.is_public(): add_record_to_graph(graph, rec, kwargs)"}
{"text_id": "8216", "text": "docstring: def all_captures(self): if self.player == CONST.WHITE: rfc = (self.empty >> 8) & (self.black_disks >> 4) & self.white_disks lfc = (self.empty >> 10) & (self.black_disks >> 5) & self.white_disks rbc = (self.empty << 8) & (self.black_disks << 4) & self.white_kings lbc = (self.empty << 10) & (self.black_disks << 5) & self.white_kings else: rfc = (self.empty >> 8) & (self.white_disks >> 4) & self.black_kings lfc = (self.empty >> 10) & (self.white_disks >> 5) & self.black_kings rbc = (self.empty << 8) & (self.white_disks << 4) & self.black_disks lbc = (self.empty << 10) & (self.white_disks << 5) & self.black_disks capture_moves = [] if (rfc | lfc | rbc | lbc) != 0: capture_moves += [-0x101 << i for (i, bit) in enumerate(bin(rfc)[::-1]) if bit == '1'] capture_moves += [-0x401 << i for (i, bit) in enumerate(bin(lfc)[::-1]) if bit == '1'] capture_moves += [-BAKWARDS_BIT - (0x101 << (i - 8)) for (i, bit) in enumerate(bin(rbc)[::-1]) if bit == '1'] capture_moves += [-BAKWARDS_BIT - (0x401 << (i - 10)) for (i, bit) in enumerate(bin(lbc)[::-1]) if bit == '1'] return capture_moves"}
{"text_id": "8217", "text": "docstring: def start(self): self.tries = len(data.stages) self.word = get_word() self.word_completion = \"\\\\_ \" * len(self.word) self.guessed_letters = [] self.guessed_words = [] return (\"Thanks for playing hangman. Try to guess this {} letter word. You have {} tries left\" \"\\n\\n{}\".format(len(self.word), self.tries, self.word_completion ) )"}
{"text_id": "8218", "text": "docstring: def eval_reaction_expression_single(model, expression_data, and_function): reaction_expression = model.getReactionExpression( expression_data, and_function=and_function) reaction_expression = pd.Series(reaction_expression, name=expression_data.name) reaction_expression[pd.isnull(reaction_expression)] = 0 reaction_expression = np.log2(reaction_expression + 1) return reaction_expression"}
{"text_id": "8219", "text": "docstring: def apply_decimation_filter(high_res_mesh: Mesh, low_res_mesh: Mesh, target_high_res_meshes: List[Mesh]) -> List[Mesh]: high_res_verts = high_res_mesh.vertices low_res_verts = low_res_mesh.vertices low_res_faces = low_res_mesh.faces clf = NearestNeighbors(n_neighbors=1) clf.fit(high_res_verts) high2low_index_mapping = np.squeeze(clf.kneighbors(low_res_verts,return_distance=False)) target_low_res_meshes = [] for _mesh in target_high_res_meshes: _high_res_verts = _mesh.vertices _low_res_verts = _high_res_verts[high2low_index_mapping] output_mesh = Mesh(vertices=_low_res_verts,faces=low_res_faces) target_low_res_meshes.append(output_mesh) return target_low_res_meshes"}
{"text_id": "8220", "text": "docstring: def delete_commit(self, id=None): validator = model.form.DeleteForm() form_data = None try: form_data = validator.to_python(request.params) except formencode.Invalid, error: return \"There were input errors: %s\" % (error) journal_entry = get_journal(id) self.is_my_journal(journal_entry,True) if form_data['confirm'] != None: journal_entry.status = 'deleted' model.Session.commit() h.redirect_to(h.url_for(controller='journal', action='index', username=journal_entry.user.username)) else: h.redirect_to(h.url_for(controller='journal', action='view', id=journal_entry.id))"}
{"text_id": "8221", "text": "docstring: def close_tab(self, button): page = self.buttons[button] clientobj = self.objects[page] clientobj.loseConnection() self.notebook.remove_page(page) del self.buttons[button] self.objects, self.buttons = self.update_dict(page, self.objects, self.buttons)"}
{"text_id": "8222", "text": "docstring: def entries(self): return self.__dict__.copy()"}
{"text_id": "8223", "text": "docstring: def do_exporttoken(self, inp): words = inp.split(\" \") if len(words) != 2: print(\"Invalid usage\") return tokenizer_name, filename = words try: tokenizers[tokenizer_name].save(filename) except KeyError: print(\"Unknown tokenizer: {}\".format(tokenizer_name)) print(\"Exported to {}\".format(filename))"}
{"text_id": "8224", "text": "docstring: def _verify_step(key: str, step: BaseStep) -> None: step_class = type(step) if isinstance(step, BaseStepMeta): raise PipelineInterfaceError( f\"Wrong argument type (`{step_class}`) for argument \" f\"'{key}' of pipeline '{self.name}'. \" f\"A `BaseStep` subclass was provided instead of an \" f\"instance. \" f\"This might have been caused due to missing brackets of \" f\"your steps when creating a pipeline with `@step` \" f\"decorated functions, \" f\"for which the correct syntax is `pipeline(step=step())`.\" ) if not isinstance(step, BaseStep): raise PipelineInterfaceError( f\"Wrong argument type (`{step_class}`) for argument \" f\"'{key}' of pipeline '{self.name}'. Only \" f\"`@step` decorated functions or instances of `BaseStep` \" f\"subclasses can be used as arguments when creating \" f\"a pipeline.\" ) if step_class in step_classes: previous_key = step_classes[step_class] raise PipelineInterfaceError( f\"Found multiple step objects of the same class \" f\"(`{step_class}`) for arguments '{previous_key}' and \" f\"'{key}' in pipeline '{self.name}'. Only one step object \" f\"per class is allowed inside a ZenML pipeline.\" ) step.pipeline_parameter_name = key combined_steps[key] = step step_classes[step_class] = key"}
{"text_id": "8225", "text": "docstring: def filter_non_text_strict(self): print(len(self.sub_red_info)) tmp_bool_df = self.sub_red_info.subreddit.apply( lambda x: not x.allow_images) self.sub_red_info = self.sub_red_info[tmp_bool_df]"}
{"text_id": "8226", "text": "docstring: def exit_json(self, **kwargs): self.add_path_info(kwargs) if not 'changed' in kwargs: kwargs['changed'] = False print self.jsonify(kwargs) sys.exit(0)"}
{"text_id": "8227", "text": "docstring: def apply_crease_edges(relationships, namespace=None, target_namespaces=None, nodes=None, by_name=False): namespace = namespace or \"\" target_namespaces = target_namespaces or [None] crease_sets = list() for level, members in relationships.items(): level = float(level) node = lsAttr(\"creaseLevel\", level, namespace=namespace) if not node: crease_name = namespace + \":creaseSet1\" crease_set = cmds.createNode(\"creaseSet\", name=crease_name) cmds.setAttr(crease_set + \".creaseLevel\", level) else: crease_set = node[0] crease_sets.append(crease_set) edge_map = defaultdict(set) for member in members: id, edge_id = member.split(\".\") edge_map[id].add(edge_id) surface_cache = defaultdict(set) for target_namespace in target_namespaces: _map = ls_nodes_by_id(set(edge_map.keys()), target_namespace, nodes, by_name) for id, nodes_ in _map.items(): surface_cache[id].update(nodes_) edges = [] for id, edge_ids in edge_map.items(): edges += list(\".\".join([n, edge]) for n in surface_cache[id] for edge in edge_ids) if not edges: continue cmds.sets(edges, forceElement=crease_set) return crease_sets"}
{"text_id": "8228", "text": "docstring: def read_2d(n, m): val = zeros([n, m]) for y in range(m): for x in range(n): val[x, y] = next(values) return val"}
{"text_id": "8229", "text": "docstring: def sendRemoteFiles( self, file_urls, message=None, thread_id=None, thread_type=ThreadType.USER ): file_urls = require_list(file_urls) files = self._upload(get_files_from_urls(file_urls)) return self._sendFiles( files=files, message=message, thread_id=thread_id, thread_type=thread_type )"}
{"text_id": "8230", "text": "docstring: def fetch_by_own_chain_id(self, chain_id, biomolecule_id, *expr, **kwargs): where = and_(Residue.chain_id==chain_id, Contact.biomolecule_id==biomolecule_id, Contact.is_same_entity==False, *expr) query = Residue.query.join('Atoms','Contacts').filter(where) query = query.add_columns(*self._sift) query = query.group_by(Residue).order_by(Residue.residue_id) return query.all()"}
{"text_id": "8231", "text": "docstring: def compare_forecast_plot(train, test, ts_forecast=None, fig=None, plot_title=\"Compare Forecast\"): if fig is None: plt.figure(figsize=(12, 6)) plt.title(plot_title) plt.plot(train, color=TRAIN_COLOR, label='Train') if test is not None and len(test) > 0: plt.plot(test, color=TEST_COLOR, ls=TEST_LS, label='Test') ax = plt.gca() if ts_forecast is not None: ts_forecast.plot(ax=ax) plt.legend(loc=\"best\")"}
{"text_id": "8232", "text": "docstring: def checkAvailability(payload, age): available_centers = set() unavailable_centers = set() available_centers_str = False total_available_centers = 0 if('centers' in payload.keys()): length = len(payload['centers']) if(length>1): for i in range(length): sessions_len = len(payload['centers'][i]['sessions']) for j in range(sessions_len): if((payload['centers'][i]['sessions'][j]['available_capacity']>0) and (payload['centers'][i]['sessions'][j]['min_age_limit']<=age)): available_centers.add(payload['centers'][i]['name']) available_centers_str = \", \".join(available_centers) total_available_centers = len(available_centers) return available_centers_str,total_available_centers"}
{"text_id": "8233", "text": "docstring: def controller_name (self): return self.controlAdapter.task_name"}
{"text_id": "8234", "text": "docstring: def grid2grid_hextotet5(self, name=None): return self.grid2grid(ioption=\"hextotet5\", **minus_self(locals()))"}
{"text_id": "8235", "text": "docstring: def _updateParameters(self, currentKey): if currentKey == 'p.stop': self.speed = 0.0 self.configParam = copy.deepcopy(self.default_configParam) elif currentKey == 'p.t': if self.configParam.maxSpeed < self.limit_configParam.maxSpeed: self.configParam.maxSpeed += self.parameterIncrement elif currentKey == 'p.g': if self.startSpeed < self.configParam.maxSpeed: self.configParam.maxSpeed -= self.parameterIncrement elif currentKey == 'p.y': if self.configParam.maxSteerAngle < self.limit_configParam.maxSteerAngle: self.configParam.maxSteerAngle += self.parameterIncrement elif currentKey == 'p.h': if self.startSteerAngle < self.configParam.maxSteerAngle: self.configParam.maxSteerAngle -= self.parameterIncrement elif currentKey == 'p.u': if self.configParam.speedStep < self.limit_configParam.speedStep: self.configParam.speedStep += self.parameterIncrement elif currentKey == 'p.j': if 0.1 < self.configParam.speedStep: self.configParam.speedStep -= self.parameterIncrement elif currentKey == 'p.i': if self.configParam.steerAngleStep < self.limit_configParam.steerAngleStep: self.configParam.steerAngleStep += self.parameterIncrement elif currentKey == 'p.k': if 0.1 < self.configParam.steerAngleStep: self.configParam.steerAngleStep -= self.parameterIncrement"}
{"text_id": "8236", "text": "docstring: def check_progress_canceled(self): p = self.controller.progress if p.wasCanceled(): if p._progress_canceled: p._progress_canceled = False raise AbortExecution(\"Execution aborted by user\") r = QtGui.QMessageBox.question(self.parent(), 'Execution Paused', 'Are you sure you want to abort the execution?', QtGui.QMessageBox.Yes | QtGui.QMessageBox.No, QtGui.QMessageBox.No) if r == QtGui.QMessageBox.Yes: raise AbortExecution(\"Execution aborted by user\") else: p.goOn()"}
{"text_id": "8237", "text": "docstring: def forwardSoftSenseOpNoShift(th_img, th_smaps, th_mask): th_img_pad = th_img.unsqueeze(-5) th_kspace = fft2(complex_mult(th_img_pad.expand_as(th_smaps), th_smaps)) * th_mask th_kspace = torch.sum(th_kspace, dim=-4, keepdim=True) return th_kspace"}
{"text_id": "8238", "text": "docstring: def iterate_lemmata(self, langID, lemmataSorted): lFreqsSorted = [self.wordFreqs[langID][itemID] for itemID in self.wordFreqs[langID] if itemID.startswith('l')] lFreqsSorted.sort(reverse=True) lemmaFreqToRank, quantiles = self.get_freq_ranks(lFreqsSorted) iLemma = 0 for l, lID in self.tmpLemmaIDs[langID].items(): lID = 'l' + str(lID) if iLemma % 250 == 0: print('indexing lemma', iLemma) lOrder = lemmataSorted[l] lemmaJson = { 'wf': l, 'wtype': 'lemma', 'lang': langID, 'l_order': lOrder, 'freq': self.wordFreqs[langID][lID], 'lemma_freq': self.wordFreqs[langID][lID], 'rank_true': lemmaFreqToRank[self.wordFreqs[langID][lID]], 'rank': self.quantile_label(self.wordFreqs[langID][lID], lemmaFreqToRank[self.wordFreqs[langID][lID]], quantiles), 'n_sents': self.wordSFreqs[langID][lID], 'n_docs': len(self.wordDIDs[langID][lID]), 'freq_join': 'word' } curAction = { '_index': self.name + '.words', '_id': lID, '_source': lemmaJson } iLemma += 1 yield curAction for docID in self.wordDIDs[langID][lID]: lfreqJson = { 'wtype': 'word_freq', 'l_id': lID, 'd_id': docID, 'l_order': lOrder, 'freq': self.wordDocFreqs[langID][(lID, docID)], 'freq_join': { 'name': 'word_freq', 'parent': lID } } curAction = {'_index': self.name + '.words', '_id': 'lfreq' + str(self.lemmaFreqID), '_source': lfreqJson, '_routing': lID} self.lemmaFreqID += 1 yield curAction"}
{"text_id": "8239", "text": "docstring: async def gc_events(self, ctx): return await channel_getter(ctx, ctx.channel_kind)"}
{"text_id": "8240", "text": "docstring: def split_dataset( input_filepath=\"interim\", output_filepath=\"processed\", dataset_dir_name=\"original\" ): print(f\"\\n\\tProcessing {dataset_dir_name}\\n\") ( train_filepath, train_images_filepath, valid_filepath, valid_images_filepath, ) = helper.create_output_path( output_filepath=os.path.join(Config.data_path, output_filepath), dataset_dir_name=dataset_dir_name, ) full_dataset_dir_name = os.path.join( Config.data_path, input_filepath, dataset_dir_name ) full_images_dir_name = helper.get_subfolder_name(full_dataset_dir_name) dataframe = helper.create_dataframe( annotation_file_path=full_dataset_dir_name, images_dir_name=full_images_dir_name, ) train_dataframe, valid_dataframe = helper.split_dataframe( dataframe=dataframe, first_dest_path=train_filepath, second_dest_path=valid_filepath, ) helper.copy_image_from_dataframe( destination=train_images_filepath, dataframe=train_dataframe ) helper.copy_image_from_dataframe( destination=valid_images_filepath, dataframe=valid_dataframe ) print(\"Splitting completed!\")"}
{"text_id": "8241", "text": "docstring: def pop_shell(locs=None): if locs is None: locs = globals() create_console() FixedInteractiveConsole(locs).interact()"}
{"text_id": "8242", "text": "docstring: def format_option_strings(self, option): if option.takes_value(): metavar = option.metavar or option.dest.upper() short_opts = option._short_opts long_opts = [lopt + \" \" + metavar for lopt in option._long_opts] else: short_opts = option._short_opts long_opts = option._long_opts if not short_opts: short_opts = [ \" \", ] if self.short_first: opts = short_opts + long_opts else: opts = long_opts + short_opts return \" \".join(opts)"}
{"text_id": "8243", "text": "docstring: def format( self, size: Optional[int] = 2, sep: Optional[str] = \":\", to_case: Optional[Callable] = None, ): i_chars = iter(self.chars) chunks, rem = divmod(12, size) if rem != 0: raise ValueError(f\"Invalid size {size}, not divisible from 12\") value = sep.join(\"\".join(islice(i_chars, size)) for _ in range(chunks)) return value if not to_case else to_case(value)"}
{"text_id": "8244", "text": "docstring: def testGoal(self): goal = self.feed.entry[0].goal[0] self.assertEqual(goal.number, '1') self.assertEqual(goal.name, 'Completing Order') self.assertEqual(goal.value, '10.0') self.assertEqual(goal.active, 'true')"}
{"text_id": "8245", "text": "docstring: def graph_v1(param, step=None, name=None): if not context.executing_eagerly() and not isinstance(param, ops.Tensor): raise TypeError(\"graph() needs a argument `param` to be tf.Tensor \" \"(e.g. tf.placeholder) in graph mode, but received \" f\"param={param} of type {type(param).__name__}.\") writer = _summary_state.writer if writer is None: return control_flow_ops.no_op() with ops.device(\"cpu:0\"): if isinstance(param, (ops.Graph, graph_pb2.GraphDef)): tensor = ops.convert_to_tensor(_serialize_graph(param), dtypes.string) else: tensor = array_ops.identity(param) return gen_summary_ops.write_graph_summary( writer._resource, _choose_step(step), tensor, name=name)"}
{"text_id": "8246", "text": "docstring: def time_function(what=None, digits=5, unit='minutes'): def master_function(func): def func_wrapper(*args, **kwargs): units = { 'minutes' : 60, 'seconds' : 1 } assert unit in units.keys(), 'Your given unit is not valid' start = time.time() results = func(*args, **kwargs) elapsed = (time.time() - start) / units.get(unit) if digits: elapsed = round(elapsed, digits) if what: print('{0} finished. Time elapsed ({1}): {2}'.format( what, unit, str(elapsed)), '\\n') else: print('Time elapsed ({0}): {1}'.format(unit, str(elapsed)), '\\n') return results return func_wrapper return master_function"}
{"text_id": "8247", "text": "docstring: def ohlc(self, df, title=None, x_label=None, y_label=None, custom_design=None): plot_dict = {\"data\": [{\"df\": df, \"type\": \"ohlc\"}]} expect = [\"layout\", \"lines\"] plot_dict = self._customize_design(plot_dict=plot_dict, custom_design=custom_design, expect=expect) plot_dict = self._add_labels(plot_dict, title, x_label, y_label) return self._process_output(plot_dict)"}
{"text_id": "8248", "text": "docstring: def _find_index_of_best_sentence(kls): return kls.index(min(kls))"}
{"text_id": "8249", "text": "docstring: def evaluate_recurrence_coefficients( distribution, k_data, parameters=None, cache=None, ): assert len(k_data) == len(distribution), ( \"distribution %s is not of length %d\" % (distribution, len(k_data))) assert len(k_data.shape) == 1 def cache_key(distribution): return (tuple(k_data), distribution) if cache is None: cache = {} else: if cache_key(distribution) in cache: return cache[cache_key(distribution)] parameters = load_parameters( distribution, \"_ttr\", parameters, cache, cache_key) coeff1, coeff2 = distribution._ttr(k_data, **parameters) out = numpy.zeros((2,) + k_data.shape) out.T[:, 0] = numpy.asarray(coeff1).T out.T[:, 1] = numpy.asarray(coeff2).T if len(distribution) == 1: out = out[:, 0] return out"}
{"text_id": "8250", "text": "docstring: def index_of_closest_segment(self, x, y, eps=5): if len(self.vertices) == 1: return 1 point = np.array([x, y]) min_index = 0 min_distance = np.linalg.norm(point - self.vertices[0]) for i in range(len(self.vertices) - 1): l1 = self.vertices[i] l2 = self.vertices[i + 1] distance = distance_point_linesegment(point, l1, l2) if distance < eps and distance < min_distance: min_index = i + 1 min_distance = distance distance = np.linalg.norm(point - self.vertices[-1]) if distance < min_distance: min_index = len(self.vertices) return min_index"}
{"text_id": "8251", "text": "docstring: def delivery_report(self, err, msg): if err is not None: self.logger.error('Message delivery failed: %s', err) else: self.logger.info('Message delivered to %s [%s]', msg.topic(), msg.partition())"}
{"text_id": "8252", "text": "docstring: def fewer_than_n_datapoints(data, n): if not data.shape[0] < n: raise TimekeepCheckError( \"fewer_than_n_datapoints: data has n or more datapoints;\" \"data has {} datapoints\".format(data.shape[0]) )"}
{"text_id": "8253", "text": "docstring: def _check_fins_frame_header_validity(fins_frame_header): icf = fins_frame_header[0] if icf != 0x80: raise ValueError(\"ICF value should always be 0x80 for a command sent to the emulator\") if fins_frame_header[1] != 0x00: raise ValueError(\"Reserved byte should always be 0x00.\") if fins_frame_header[2] != 0x02: raise ValueError(\"Gate count should always be 0x02.\") check_is_byte(fins_frame_header[3]) if fins_frame_header[4] != SimulatedFinsPLC.HELIUM_RECOVERY_NODE: raise ValueError(\"The node address of the FINS helium recovery PLC should be {}!\".format( SimulatedFinsPLC.HELIUM_RECOVERY_NODE)) for i in range(5, 10): check_is_byte(fins_frame_header[i])"}
{"text_id": "8254", "text": "docstring: def read_mod_file(moddir, filename): if moddir.endswith('.zip'): basename = os.path.basename(moddir) filename = os.path.join(basename[:-4], filename) with open(moddir) as fd: zfd = zipfile.ZipFile(fd) with zfd.open(filename) as innerfd: return innerfd.read() with open(os.path.join(moddir, filename)) as fd: return fd.read()"}
{"text_id": "8255", "text": "docstring: def ajax_questions_filter(request): user = request.user filter_dict = {\"user_id\": user.id, \"active\": True} question_type = request.POST.get('question_type') marks = request.POST.get('marks') language = request.POST.get('language') if question_type != \"select\": filter_dict['type'] = str(question_type) if marks != \"select\": filter_dict['points'] = marks if language != \"select\": filter_dict['language'] = str(language) questions = list(Question.objects.filter(**filter_dict)) return my_render_to_response( request, 'yaksh/ajax_question_filter.html', {'questions': questions} )"}
{"text_id": "8256", "text": "docstring: def create(filename, river, s_rho=None, cdl=None): river = np.asarray(river) s_rho = len(river[0]['vshape']) if s_rho is None else s_rho nc = seapy.roms.ncgen.create_psource( filename, nriver=len(river), s_rho=s_rho, clobber=True, cdl=cdl) for i, r in enumerate(river): try: nc.variables['river'][i] = int(r['id']) except: nc.variables['river'][i] = 999 nc.variables['river_Xposition'][i] = int(r['x']) nc.variables['river_Eposition'][i] = int(r['y']) nc.variables['river_direction'][i] = int(r['direction']) try: nc.variables['river_flag'][i] = int(r['flag']) except: nc.variables['river_flag'][i] = 0 try: vshape = np.asarray(r['vshape'])[:, np.newaxis] nc.variables['river_Vshape'][:, i] = vshape except Exception as err: print(\"Using default shape\") vshape = np.ones((s_rho, 1)) / s_rho nc.variables['river_Vshape'][:, i] = vshape return nc"}
{"text_id": "8257", "text": "docstring: def post_notification_destination_attributes( report_uuid: ReportId, notification_destination_uuid: NotificationDestinationId, database: Database ): data_model = latest_datamodel(database) reports = latest_reports(database, data_model) data = ReportData(data_model, reports, report_uuid) notification_destination_name = data.report[\"notification_destinations\"][notification_destination_uuid][\"name\"] attributes = dict(bottle.request.json) old_values = [] for key in attributes: old_values.append(data.report[\"notification_destinations\"][notification_destination_uuid].get(key) or \"\") data.report[\"notification_destinations\"][notification_destination_uuid][key] = attributes[key] if set(old_values) == set(attributes.values()): return dict(ok=True) separator = \"' and '\" delta_description = ( f\"{{user}} changed the '{separator.join(attributes.keys())}' of notification destination \" f\"'{notification_destination_name}' in report '{data.report_name}' \" f\"from '{separator.join(old_values)}' to '{separator.join(attributes.values())}'.\" ) uuids = [data.report_uuid, notification_destination_uuid] return insert_new_report(database, delta_description, uuids, data.report)"}
{"text_id": "8258", "text": "docstring: def check_anchor_order(m): a = m.anchor_grid.prod(-1).view(-1) da = a[-1] - a[0] ds = m.stride[-1] - m.stride[0] if da.sign() != ds.sign(): console.log(\"Reversing anchor order\") m.anchors[:] = m.anchors.flip(0) m.anchor_grid[:] = m.anchor_grid.flip(0)"}
{"text_id": "8259", "text": "docstring: def create_bound(model, xs, lengths): if config.bound == \"elbo\": ll_per_seq, log_weights, _ = bounds.iwae( model, xs, lengths, num_samples=1, parallel_iterations=config.parallel_iterations ) elif config.bound == \"iwae\": ll_per_seq, log_weights, _ = bounds.iwae( model, xs, lengths, num_samples=config.num_samples, parallel_iterations=config.parallel_iterations ) elif config.bound == \"fivo\": ll_per_seq, log_weights, resampled, _ = bounds.fivo( model, xs, lengths, num_samples=config.num_samples, resampling_criterion=smc.ess_criterion, resampling_type=config.resampling_type, random_seed=config.random_seed, parallel_iterations=config.parallel_iterations ) bound_per_t = ll_per_seq / tf.to_float(lengths) if config.bound == \"fivo\": return bound_per_t, log_weights, resampled else: return bound_per_t, log_weights"}
{"text_id": "8260", "text": "docstring: def decision_human(self): dec = self.decision action = None faction = None if dec[0] == 'P': action = 'pick' elif dec[0] == 'B': action = 'ban' else: return dec if dec[1] == self.radiant: faction = 'Radiant' else: faction = 'Dire' return f'{faction} {action}'"}
{"text_id": "8261", "text": "docstring: def pressed_up(event, key_pressed, key_mods): return ( event.type == pygame.KEYDOWN and key_pressed[pygame.K_k] and not held_Shift(key_mods) )"}
{"text_id": "8262", "text": "docstring: def help_check(subcommand: str) -> None: print(f\"Test {subcommand} help.\") if subcommand == \"global\": help_command = [\"--help\"] else: help_command = [subcommand, \"--help\"] try: output = sh.zeigen(help_command) except ErrorReturnCode as errors: print(errors) pytest.fail(f\"{subcommand} help test failed\") print(output) assert \"Usage:\" in output assert \"Options:\" in output"}
{"text_id": "8263", "text": "docstring: def show(self, run_before=None) -> int: from program_files.helpers import error if len(self.options) < 1: error('Menu must have at least one option.') if self.title is not None: print(self.title) if run_before is not None: run_before() for number, option in enumerate(self.options, 1): print(str(number) + '. ' + option) print() while True: selection = input('Please enter selection [1-' + str(len(self.options)) + ']: ') try: if int(selection) in range(1, len(self.options) + 1): return int(selection) else: raise ValueError except ValueError: pass"}
{"text_id": "8264", "text": "docstring: async def create_company_page(self, data: dict[str, Any]) -> tuple[str, str]: url = LOGO_URL.format(**{\"image_id\": data[\"logo\"][\"image_id\"] if \"logo\" in data else \"\"}) founded = dt.utcfromtimestamp(data[\"start_date\"]).date() if \"start_date\" in data else \"?\" developed = \", \".join(game[\"name\"] for game in data[\"developed\"]) if \"developed\" in data else \"?\" published = \", \".join(game[\"name\"] for game in data[\"published\"]) if \"published\" in data else \"?\" formatting = { \"name\": data[\"name\"], \"url\": data[\"url\"], \"description\": f\"{data['description']}\\n\\n\" if \"description\" in data else \"\\n\", \"founded\": founded, \"developed\": developed, \"published\": published } page = COMPANY_PAGE.format(**formatting) return page, url"}
{"text_id": "8265", "text": "docstring: def video_list_az(context): common.debug('Requesting video list TEST ') return VideoListAZ(common.make_call( 'perpetual_path_request', { 'path_type': 'videolistAZ', 'paths': build_paths([context, 'az', RANGE_SELECTOR], VIDEO_LIST_PARTIAL_PATHS), 'length_params1': context }), context)"}
{"text_id": "8266", "text": "docstring: def eval_spectrum(self, values: ty.Union[list, tuple, np.ndarray], parameter_names: ty.Union[ty.List[str], ty.Tuple[str]] ): self.log.debug(f'evaluate spectrum for {len(values)} parameters') if len(values) != len(parameter_names): raise ValueError(f'{len(values)} != len({parameter_names})') if isinstance(parameter_names, str): raise NotImplementedError(f\"Got single param {parameter_names}?\") if len(parameter_names) not in [2, 5]: raise NotImplementedError('Use either 2 or 5 parameters to fit') checked_values = check_shape(values) log_mass = checked_values[0] log_cross_section = checked_values[1] dm_class = self.spectrum_class.dm_model if self._earth_shielding and len(parameter_names) >= 2: dm_class.log_cross_section = log_cross_section dm_class.log_mass = log_mass assert self.spectrum_class.dm_model.log_mass == log_mass elif len(parameter_names) == 5: if tuple(parameter_names) != tuple(self._parameter_order[:len(parameter_names)]): raise NameError( f\"The parameters are not in correct order. Please insert\" f\"{self._parameter_order[:len(parameter_names)]} rather than \" f\"{parameter_names}.\") v_0 = checked_values[2] * nu.km / nu.s v_esc = checked_values[3] * nu.km / nu.s rho_dm = checked_values[4] * nu.GeV / nu.c0 ** 2 / nu.cm ** 3 dm_class.v_0 = v_0 dm_class.v_esc = v_esc dm_class.rho_dm = rho_dm assert self.spectrum_class.dm_model.rho_dm == rho_dm spectrum = self.spectrum_class.get_counts( wimp_mass=10. ** log_mass, cross_section=10. ** log_cross_section, poisson=False) self.log.debug('returning results') return spectrum"}
{"text_id": "8267", "text": "docstring: def testSuccess(self): widget = self.Widget() for value in self.SuccessValues: widget.value = value self.assertEqual(widget.value, value)"}
{"text_id": "8268", "text": "docstring: async def hug(self, ctx, *, member: discord.Member=None): generate_hug = randint(1, 16) gif_url = \"https://dmctruong.000webhostapp.com/.Discord/gifs-hugs/hug\" + str(generate_hug) + \".gif\" if member is None: print(\"*hugs* \" + gif_url) await self.bot.say(\"*hugs*\\n\" + gif_url) else: if member.id == ctx.message.author.id: print(\"*hugs* \" + gif_url) await self.bot.say(\"*hugs*\\n\" + gif_url) else: print(ctx.message.author.mention + \" gave \" + member.mention + \" a hug! \" + gif_url) await self.bot.say(ctx.message.author.mention + \" gave \" + member.mention + \" a hug!\\n\" + gif_url)"}
{"text_id": "8269", "text": "docstring: def log_msg(self, messages, msg_type='info', log_indent='', out_indent=''): if isinstance(messages, list) or isinstance(messages, tuple): log_msg, out_msg = messages elif isinstance(messages, str): log_msg = out_msg = messages else: print(\"EODMSRAPI.log_msg: 'messages' parameter not valid.\") return None log_msg = f\"{log_indent}{log_msg}\" log_msg = log_msg.replace('\\n', '\\\\n') log_msg = log_msg.replace('\\t', '\\\\t') log_msg = log_msg.replace(\"'\", \"\\\\'\") eval(f\"logger.{msg_type}(r'{log_msg}')\") if not self.stdout_enabled: return None if msg_type == 'info': msg = f\"{out_indent}{self._header}{out_msg}\" else: msg = f\"{out_indent}{self._header} {msg_type.upper()}: {out_msg}\" print(msg)"}
{"text_id": "8270", "text": "docstring: async def axiostrumpinterview( self, ctx, *, text: commands.clean_content(fix_channel_mentions=True) ): async with ctx.typing(): buffer, delta = await backend.make_axios_interview_meme(text) await ctx.send( f\"Requested by: {ctx.author} \\N{BULLET} Took {delta:.2f} ms.\", file=File(buffer, \"axios_trump_interview.png\") )"}
{"text_id": "8271", "text": "docstring: def authenticate( self, access_token: Optional[str], refresh_id: Optional[str] ) -> Tuple[int, Optional[str]]: self.debug_fn(f\"Try to authenticate with\") self.debug_fn(f\"AccessToken: {access_token}\") self.debug_fn(f\"RefreshId: {refresh_id}\") if not access_token or not refresh_id: self.debug_fn(\"No access_token or refresh_id\") return ANONYMOUS_USER, None return self.validator.verify(access_token, refresh_id)"}
{"text_id": "8272", "text": "docstring: def Parse(self, header, interface, tail=None): if tail is None: tail = '' tail = tail.strip() declarations = self.GetCache(header, interface, tail) if declarations is None: declarations = self.ParseWithGCCXML(header, tail) self.CreateCache(header, interface, tail, declarations) header_fullpath = os.path.abspath(self.FindHeader(header)) return declarations, header_fullpath"}
{"text_id": "8273", "text": "docstring: def common_type(*arrays): if len(arrays) == 0: return _numpy.float16 default_float_dtype = _numpy.dtype('float64') dtypes = [] for a in arrays: if a.dtype.kind == 'b': raise TypeError('can\\'t get common type for non-numeric array') elif a.dtype.kind in 'iu': dtypes.append(default_float_dtype) else: dtypes.append(a.dtype) return _functools.reduce(_numpy.promote_types, dtypes).type"}
{"text_id": "8274", "text": "docstring: def image_grab(self, imgName, cameraNum): import cv2, time, copy if imgName is None: cap = cv2.VideoCapture(cameraNum) img = cap.read()[1] cv2.imwrite('Camera\\\\' + str(int(time.time())) + '.jpg', img) cap.release() else: img = cv2.imread('Camera\\\\' + imgName, 1) self.contours = [] self.centroids = [] self.plant = None self.weeds = [] self.img = img self.imgOutlined = copy.deepcopy(img) (height, width, channels) = img.shape self.center = (width/2, height/2)"}
{"text_id": "8275", "text": "docstring: def deserialize(self, stamp_token, serialized_proto): return gen_boosted_trees_ops.boosted_trees_deserialize_ensemble( self.resource_handle, stamp_token, serialized_proto)"}
{"text_id": "8276", "text": "docstring: def schedule(self, project, flow, cron, **execution_options): self.__check_if_logged() execution_options = {k:v for (k, v) in execution_options.items() if v} response = api.schedule_request( self.__session, self.__host, self.__session_id, project, flow, cron, **execution_options ) self.__catch_response_error(response, ScheduleError) response_json = response.json() logging.info(response_json[u'message']) logging.info('scheduleId: %s' % (response_json[u'scheduleId']))"}
{"text_id": "8277", "text": "docstring: def square_root(self, data): var_sq = data ** (.5) return var_sq"}
{"text_id": "8278", "text": "docstring: def broadcast_status(self, status): self._broadcast( \"transient.status\", json.dumps(status), headers={\"expires\": str(int((15 + time.time()) * 1000))}, )"}
{"text_id": "8279", "text": "docstring: def handler(event, context): assert context LOG.debug(event) raw_data = event.get('awslogs', {}).get('data') if not raw_data: LOG.error('No data') return decoded_data = decompress(b64decode(raw_data), 16 + MAX_WBITS) data = json.loads(decoded_data) (num_matches, num_events) = process_log_events(data) msg = \"Processed {} log entries of which {} matched.\".format( num_events, num_matches) LOG.info(msg) return msg"}
{"text_id": "8280", "text": "docstring: def axis_scale_by_array(ax, arr, axis='y', nbins=3): yy = array([y for y in arr if y is not None], dtype=float) xx = arange(len(yy)) func = interp1d(xx, yy, fill_value=\"extrapolate\") invf = interp1d(yy, xx, fill_value=\"extrapolate\") set_scale = eval(f\"ax.set_{axis}scale\") set_scale('function', functions=(invf, func)) _axis = getattr(ax, axis+\"axis\") _axis.set_major_locator(ticker.FixedLocator(yy, nbins=nbins)) _axis.set_minor_locator(ticker.FixedLocator(yy)) _axis.set_minor_formatter(ticker.NullFormatter())"}
{"text_id": "8281", "text": "docstring: def add_metadatablock(self, metadatablock: DataverseBase) -> None: if issubclass(metadatablock.__class__, DataverseBase) is False: raise TypeError( f\"Expected class of type 'DataverseBase', got '{metadatablock.__class__.__name__}'\" ) if hasattr(metadatablock, \"_metadatablock_name\") is False: raise TypeError( f\"The provided class {metadatablock.__class__.__name__} has no metadatablock name and is thus not compatible with this function.\" ) block_name = getattr(metadatablock, \"_metadatablock_name\") self.metadatablocks.update({block_name: metadatablock}) setattr(self, block_name, metadatablock)"}
{"text_id": "8282", "text": "docstring: def _query_cve_action(self, cve_list): cve_action_query = self.session.query(Cve.cve_id, CveAffectedPkgs.package, Cve.reboot) \\ .join(CveAffectedPkgs) \\ .filter(Cve.cve_id.in_(cve_list)) return cve_action_query"}
{"text_id": "8283", "text": "docstring: def define(self, iden_name, iden_type, iden_kind): if iden_kind == \"static\": iden = identifier( iden_name, iden_type, iden_kind, self._class_static_ind ) self._class_scope.append(iden) self._class_static_ind += 1 elif iden_kind == \"field\": iden = identifier( iden_name, iden_type, iden_kind, self._class_field_ind ) self._class_scope.append(iden) self._class_field_ind += 1 elif iden_kind == \"arg\": iden = identifier( iden_name, iden_type, iden_kind, self._subroutine_arg_ind ) self._subroutine_scope.append(iden) self._subroutine_arg_ind += 1 elif iden_kind == \"var\": iden = identifier( iden_name, iden_type, iden_kind, self._subroutine_var_ind ) self._subroutine_scope.append(iden) self._subroutine_var_ind += 1 else: raise Exception(\"unexpected identifier kind: \" + iden.kind)"}
{"text_id": "8284", "text": "docstring: def _chunk_getitem(self, cidx, item, dest): try: ckey = self._chunk_key(cidx) cdata = self._chunk_store[ckey] except KeyError: if self._fill_value is not None: dest.fill(self._fill_value) else: if is_total_slice(item, self._chunks) and \\ not self._filters and \\ ((self._order == 'C' and dest.flags.c_contiguous) or (self._order == 'F' and dest.flags.f_contiguous)): if self._compressor: self._compressor.decode(cdata, dest) else: arr = np.frombuffer(cdata, dtype=self._dtype) arr = arr.reshape(self._chunks, order=self._order) np.copyto(dest, arr) else: chunk = self._decode_chunk(cdata) tmp = chunk[item] if dest.shape: dest[:] = tmp else: dest[()] = tmp"}
{"text_id": "8285", "text": "docstring: def draw_box(canvas, layout, box_width=None, color_map=None, show_element_id=False, id_font_size=None, id_font_path=None, id_text_color=None, id_text_background_color=None): draw = ImageDraw.Draw(canvas) if box_width is None: box_width = _calculate_default_box_width(canvas) if show_element_id: font_obj = _create_font_object(id_font_size, id_font_path) if color_map is None: all_types = set([b.type for b in layout if hasattr(b, 'type')]) color_map = _create_color_palette(all_types) for idx, ele in enumerate(layout): if isinstance(ele, Interval): ele = ele.put_on_canvas(canvas) outline_color = DEFAULT_OUTLINE_COLOR if not isinstance(ele, TextBlock) \\ else color_map.get(ele.type, DEFAULT_OUTLINE_COLOR) if not isinstance(ele, Quadrilateral): draw.rectangle(ele.coordinates, width=box_width, outline=outline_color) else: p = ele.points.ravel().tolist() draw.line(p+p[:2], width=box_width, fill=outline_color) if show_element_id: ele_id = ele.id or idx start_x, start_y = ele.coordinates[:2] text_w, text_h = font_obj.getsize(f'{ele_id}') draw.rectangle((start_x, start_y, start_x + text_w, start_y + text_h), fill=id_text_background_color or DEFAULT_TEXT_BACKGROUND) draw.text((start_x, start_y), f'{ele_id}', fill=id_text_color or DEFAULT_TEXT_COLOR, font=font_obj) return canvas"}
{"text_id": "8286", "text": "docstring: def _update_count_to_word(self, word, current_count = 0, new_count = 1): if current_count > 0: self.count_to_word_dict[current_count].remove(word) word_count = self.count_to_word_dict.get(new_count) if word_count: word_count.append(word) else: self.count_to_word_dict.update({new_count: [word]}) self.max_count = max(self.max_count, new_count)"}
{"text_id": "8287", "text": "docstring: def process_lfun(self, lfun_name, argument=\"\", warn_error=True, warn_not_info=True): server_protocol_string = \"LYXCMD:{}:{}:{}\".format(self.client_name, lfun_name, argument + \"\\n\") server_protocol_string = server_protocol_string.encode(\"utf-8\") while True: try: os.write(self.lyx_server_pipe_in, server_protocol_string) break except: time.sleep(0.001) while True: parsed_list = self.wait_for_server_event(notify=False) if warn_error and parsed_list[0] == \"ERROR\": print(\"Warning: Ignoring an ERROR message in processLfun(),\", \"the parsed message list is:\\n\", parsed_list) elif warn_not_info and parsed_list[0] != \"INFO\": print(\"Warning: Got a non-INFO unknown reply from LyX Server\" + \" in processLfun. Ignoring it.\") break return parsed_list[3].rstrip(\"\\n\")"}
{"text_id": "8288", "text": "docstring: def paginate(text: str): last = 0 pages = [] for curr in range(0, len(text)): if curr % 1980 == 0: pages.append(text[last:curr]) last = curr appd_index = curr if appd_index != len(text) - 1: pages.append(text[last:curr]) return list(filter(lambda a: a != '', pages))"}
{"text_id": "8289", "text": "docstring: def CallbackImpl(self, run_swarming_task_params, parameters): if not parameters.get('task_id'): return 'Task_id not found for pipeline %s' % self.pipeline_id, None task_id = parameters['task_id'] try: pipeline_completed = test_swarming.OnSwarmingTaskStateChanged( run_swarming_task_params, task_id) if not pipeline_completed: return None return None, pipeline_completed except exceptions.RetryException as e: return ('Error on updating swarming task result: %s' % e.error_message, None)"}
{"text_id": "8290", "text": "docstring: def _disallow_invalid_ops(self, values: ArrayLike, how: str): dtype = values.dtype if is_categorical_dtype(dtype) or is_sparse(dtype): raise NotImplementedError(f\"{dtype} dtype not supported\") elif is_datetime64_any_dtype(dtype): if how in [\"add\", \"prod\", \"cumsum\", \"cumprod\"]: raise NotImplementedError( f\"datetime64 type does not support {how} operations\" ) elif is_timedelta64_dtype(dtype): if how in [\"prod\", \"cumprod\"]: raise NotImplementedError( f\"timedelta64 type does not support {how} operations\" )"}
{"text_id": "8291", "text": "docstring: def delete_blob(self, container_name, blob_name, snapshot=None, lease_id=None, delete_snapshots=None, if_modified_since=None, if_unmodified_since=None, if_match=None, if_none_match=None, timeout=None): _validate_not_none('container_name', container_name) _validate_not_none('blob_name', blob_name) request = HTTPRequest() request.method = 'DELETE' request.host = self._get_host() request.path = _get_path(container_name, blob_name) request.headers = [ ('x-ms-lease-id', _to_str(lease_id)), ('x-ms-delete-snapshots', _to_str(delete_snapshots)), ('If-Modified-Since', _datetime_to_utc_string(if_modified_since)), ('If-Unmodified-Since', _datetime_to_utc_string(if_unmodified_since)), ('If-Match', _to_str(if_match)), ('If-None-Match', _to_str(if_none_match)), ] request.query = [ ('snapshot', _to_str(snapshot)), ('timeout', _int_to_str(timeout)) ] self._perform_request(request)"}
{"text_id": "8292", "text": "docstring: def isShooterUpToSpeed(self): if self.isAutonomous: shootSpeed = self.autoShootingSpeed - self.speedTolerance elif not self.isAutonomous: shootSpeed = self.teleShootingSpeed - self.speedTolerance if not self.isSetup: return False atSpeed = bool(self.shooterMotors.shooterMotor.getEncoder().getVelocity() >= shootSpeed) rumble = 0 if atSpeed and not self.isAutonomous: rumble = .3 self.xboxMap.mech.setRumble(self.xboxMap.mech.RumbleType.kLeftRumble, rumble) self.xboxMap.mech.setRumble(self.xboxMap.mech.RumbleType.kRightRumble, rumble) return atSpeed"}
{"text_id": "8293", "text": "docstring: def simplified_generate_runs(cls, strts: list, nb_execs: int = 30, odp: str = '', netids: list = None, pop_sizes: list = None): if pop_sizes is None: pop_sizes = [1, 5, 10, 15, 25] mats = [] cdtrss = [] nbs_agtss = [] for t in strts: for p in pop_sizes: mats += [[t * p] if len(t) == 1 else [[t[0]] + [t[1]] * p]] nbs_agtss += [[p]] cdtrss += [[True]] if len(t) == 2 else [[False]] cls.generate_runs(nbs_agtss, mats, cdtrss, nb_execs=nb_execs, odp=odp, netids=netids)"}
{"text_id": "8294", "text": "docstring: def subtract_mean_from_image(img, mean=None, separate_channels=False, direction='both'): if len(img.shape) == 2: single_channel = True else: single_channel = False if direction == 'both': if separate_channels and not single_channel: for i in range(img.shape[2]): if mean is None: img[:,:,i] -= np.mean(img[:,:,i]) else: img[:,:,i] -= mean[i] else: if mean is None: img -= img.mean() else: img -= mean[0] elif direction == 'column': if separate_channels: if single_channel: for i in range(img.shape[1]): if mean is None: img[:,i] -= np.mean(img[:,i]) else: img[:,i] -= mean[0] else: for i in range(img.shape[1]): for j in range(img.shape[2]): if mean is None: img[:,i,j] -= np.mean(img[:,i,j]) else: img[:,i,j] -= mean[j] else: if single_channel: for i in range(img.shape[1]): if mean is None: img[:,i] -= np.mean(img[:,i]) else: img[:, i] -= mean[0] else: for i in range(img.shape[1]): if mean is None: img[:,i,:] -= np.mean(img[:,i,:]) else: img[:,i,:] -= mean[0] elif direction == 'row': if separate_channels: if single_channel: for i in range(img.shape[0]): if mean is None: img[i,:] -= np.mean(img[i,:]) else: img[i, :] -= mean[0] else: for i in range(img.shape[0]): for j in range(img.shape[2]): if mean is None: img[i, :, j] -= np.mean(img[i, :, j]) else: img[i, :, j] -= mean[j] else: if single_channel: for i in range(img.shape[0]): if mean is None: img[i, :] -= np.mean(img[i, :]) else: img[i, :] -= mean[0] else: for i in range(img.shape[0]): if mean is None: img[i, :, :] -= np.mean(img[i, :, :]) else: img[i, :, :] -= mean[0] return img"}
{"text_id": "8295", "text": "docstring: def train_step(self, batch): user, pos, neg = batch with tf.GradientTape() as tape: self._propagate_embeddings() xu_pos, gamma_u, gamma_pos = self(inputs=(user, pos), training=True) xu_neg, _, gamma_neg = self(inputs=(user, neg), training=True) difference = tf.clip_by_value(xu_pos - xu_neg, -80.0, 1e8) loss = tf.reduce_sum(tf.nn.softplus(-difference)) reg_loss = self.l_w * tf.reduce_sum([tf.nn.l2_loss(gamma_u), tf.nn.l2_loss(gamma_pos), tf.nn.l2_loss(gamma_neg)]) * 2 loss += reg_loss grads = tape.gradient(loss, [self.Gu, self.Gi]) self.optimizer.apply_gradients(zip(grads, [self.Gu, self.Gi])) return loss"}
{"text_id": "8296", "text": "docstring: def progress_plot(controller, title='', interactive=False): try: import matplotlib.pyplot as plt plotting_on = True except: plotting_on = False pass if not plotting_on: print(\"Failed to import matplotlib.pyplot, aborting....\") return fvals = np.array([o.value for o in controller.fevals if o.value is not None]) plt.figure() if interactive: plt.ion() plt.plot(np.arange(0, fvals.shape[0]), fvals, 'bo') plt.plot(np.arange(0, fvals.shape[0]), np.minimum.accumulate(fvals), 'r-', linewidth=4.0) ymin = np.min(fvals) ymax = np.max(fvals) plt.ylim(ymin - 0.1 * (ymax - ymin), ymax + 0.1 * (ymax - ymin)) plt.xlabel('Evaluations') plt.ylabel('Function Value') plt.title(title) plt.show()"}
{"text_id": "8297", "text": "docstring: def move_coco_val_images(inpath_train_folder, val_chips_list): outpath_val_folder = inpath_train_folder.parent / 'val2016' Path(outpath_val_folder).mkdir(parents=True, exist_ok=True) for chip in val_chips_list: Path(rf'{inpath_train_folder}/{chip.replace(\"val\", \"train\")}.jpg').replace(rf'{outpath_val_folder}/{chip}.jpg')"}
{"text_id": "8298", "text": "docstring: def draw_traffic_signs(surface, font_surface, actor, color=COLOR_ALUMINIUM_2, trigger_color=COLOR_PLUM_0): transform = actor.get_transform() waypoint = carla_map.get_waypoint(transform.location) angle = -waypoint.transform.rotation.yaw - 90.0 font_surface = pygame.transform.rotate(font_surface, angle) pixel_pos = self.world_to_pixel(waypoint.transform.location) offset = font_surface.get_rect(center=(pixel_pos[0], pixel_pos[1])) surface.blit(font_surface, offset) forward_vector = carla.Location(waypoint.transform.get_forward_vector()) left_vector = carla.Location(-forward_vector.y, forward_vector.x, forward_vector.z) * waypoint.lane_width / 2 * 0.7 line = [(waypoint.transform.location + (forward_vector * 1.5) + (left_vector)), (waypoint.transform.location + (forward_vector * 1.5) - (left_vector))] line_pixel = [self.world_to_pixel(p) for p in line] pygame.draw.lines(surface, color, True, line_pixel, 2)"}
{"text_id": "8299", "text": "docstring: def luns(self): return self._luns"}
{"text_id": "8300", "text": "docstring: def ndwi_mcfeeters(b3, b8): NDWI = (b3 - b8)/(b3 + b8) return NDWI"}
{"text_id": "8301", "text": "docstring: def is_loged(id_agent): try: last_logon = Audit.objects.filter( id_agent=id_agent, id_break__isnull=True ).order_by('-datetime_init')[0] if last_logon.datetime_end == '' or last_logon.datetime_end is None: return True return False except IndexError: return False"}
{"text_id": "8302", "text": "docstring: def _parse_config(self, requires_cfg=True): if len(self.config_paths) > 0: try: self._find_config() except BisonError: if not requires_cfg: return raise try: with open(self.config_file, 'r') as f: parsed = self._fmt_to_parser[self.config_format](f) except Exception as e: raise BisonError( 'Failed to parse config file: {}'.format(self.config_file) ) from e self._full_config = None self._config = parsed"}
{"text_id": "8303", "text": "docstring: def list_devices(): if not hasattr(list_devices, \"_api\"): list_devices._api = _lib.SeaBreezeAPI(**_lib._api_kwargs) return list_devices._api.list_devices()"}
{"text_id": "8304", "text": "docstring: def edge_peel(G, vertex_indices=None, edge_indices=None): if not isinstance(G, gt.Graph): err_msg = 'G must be a graph_tool.Graph instance' raise ValueError(err_msg) if vertex_indices is None and edge_indices is None: err_msg = 'Must provide either vertex indices or edge indices' raise ValueError(err_msg) cmd = './app/bin/graph_peeling.bin -t core -o core' vp = G.new_vp('bool', vals=False) ep = G.new_ep('bool', vals=False) try: vp.a[vertex_indices] = True ep.a[edge_indices] = True except: err_msg = 'vertex or edge indices not in G' raise IndexError(err_msg) G.set_vertex_filter(vp) G.set_edge_filter(ep) efilt = ep children = [] idx = 0 while G.num_edges() > 0: p = Popen([cmd], shell=True, stdout=PIPE, stdin=PIPE) for e in G.edges(): p.stdin.write('{} {}\\n'.format(e.source(), e.target())) p.stdin.flush() p.stdin.close() top_layer_line = '' top_peel = -1 while True: line = p.stdout.readline() if line == '': break if not line.startswith('Core'): continue peel = int(line.split(' = ')[0].split('_')[-1]) if not top_layer_line: top_layer_line = line top_peel = peel continue if peel > top_peel: top_layer_line = line top_peel = peel label, vertices = top_layer_line.strip().split(' = ') peel = int(label.split('_')[-1]) assert peel == top_peel v_idx = [int(v) for v in vertices.split()] vfilt = G.new_vp('bool', vals=False) vfilt.a[v_idx] = True G.set_vertex_filter(vfilt) print('peel: {}, |V|: {}, |E|: {}'.format(peel, G.num_vertices(), G.num_edges())) e_idx = np.where(G.new_ep('bool', vals=True).a == 1)[0] efilt.a[e_idx] = False node = PartitionNode(vertex_indices=v_idx, edge_indices=e_idx, partition_type='edge', label='EPL_{}_{}'.format(peel, idx), note='peel {}'.format(peel)) children.append(node) idx += 1 G.set_vertex_filter(vp) G.set_edge_filter(efilt) G.clear_filters() return children"}
{"text_id": "8305", "text": "docstring: def fromVersion(cls, version, logger=None, no_cache=False): fleur_schema_path = f'./{version}/FleurInputSchema.xsd' schema_file_path = os.path.abspath(os.path.join(PACKAGE_DIRECTORY, fleur_schema_path)) if not os.path.isfile(schema_file_path): latest_version = _get_latest_available_version(output_schema=False) if int(version.split('.')[1]) < int(latest_version.split('.')[1]): message = f'No FleurInputSchema.xsd found at {schema_file_path}' raise FileNotFoundError(message) else: if logger is not None: logger.warning(\"No Input Schema available for version '%s'; falling back to '%s'\", version, latest_version) else: warnings.warn( f\"No Input Schema available for version '{version}'; falling back to '{latest_version}'\") fleur_schema_path = f'./{latest_version}/FleurInputSchema.xsd' schema_file_path = os.path.abspath(os.path.join(PACKAGE_DIRECTORY, fleur_schema_path)) if version in cls.__schema_dict_cache and not no_cache: return cls.__schema_dict_cache[version] cls.__schema_dict_cache[version] = cls.fromPath(schema_file_path) return cls.__schema_dict_cache[version]"}
{"text_id": "8306", "text": "docstring: def preview(base_path): class Handler(SimpleHTTPServer.SimpleHTTPRequestHandler): def translate_path(self, path): path = SimpleHTTPServer.SimpleHTTPRequestHandler.translate_path(self, path) path = os.path.join(base_path, os.path.relpath(path, os.getcwd())) return util.long_path(path) httpd, base_url = create_httpd(Handler) def run(): try: httpd.serve_forever() except: import traceback traceback.print_exc() finally: httpd.server_close() thread = threading.Thread(target=run) thread.daemon = True thread.start() try: yield base_url finally: stopper = threading.Thread(target=httpd.shutdown) stopper.daemon = True stopper.start() stopper.join(5.0)"}
{"text_id": "8307", "text": "docstring: def create_model(iterations=5000, depth_tree=4, learning_rate=0.0135, reg_l2=2, evaluation_metric='F1'): utils.save_log('{0} :: {1}'.format( create_model.__module__, create_model.__name__)) model = CatBoostClassifier( iterations=iterations, depth=depth_tree, learning_rate=learning_rate, l2_leaf_reg=reg_l2, eval_metric=evaluation_metric, task_type=config.device_type, random_seed=config.random_seed) return model"}
{"text_id": "8308", "text": "docstring: def apply_channel_enabling(self) -> None: enabled_channel_spectrum_values = [self.channels[i].name.value for i in self._enabled_channels] if len(enabled_channel_spectrum_values) in [1, 2, 4, 8]: bitwise_or_of_enabled_channels = reduce(or_, enabled_channel_spectrum_values) self.write_to_spectrum_device_register(SPC_CHENABLE, bitwise_or_of_enabled_channels) else: raise SpectrumInvalidNumberOfEnabledChannels( f\"Cannot enable {len(enabled_channel_spectrum_values)} \" f\"channels on one card.\" )"}
{"text_id": "8309", "text": "docstring: def __Where(self): if self.__Accept('WHERE'): return self.__FilterList() return self.__OrderBy()"}
{"text_id": "8310", "text": "docstring: def sendall(self, data: bytes) -> None: cont = self._recorder.trace_socket_sendall(data) if cont: if self._socket is None: return self._socket.sendall(data)"}
{"text_id": "8311", "text": "docstring: def publish(self): from robotide.publish.publisher import PUBLISHER PUBLISHER.publish(self.__class__, self)"}
{"text_id": "8312", "text": "docstring: def bytes_to_uint32(self, data, start = 0): return struct.unpack('>I', data[start : start + 4])[0]"}
{"text_id": "8313", "text": "docstring: def compute(self): self.ready_label.configure(text='COMPUTING...') self.ready_label.update() self.computation_timer.create_timestamp('Computation') self.running = True PaSyPy().main(self) self.running = False self.computation_timer.calculate_time('Computation') self.computation_time.config(text='Computation Time : {} sec.'.format(round(self.computation_timer.get_time('Computation'), 3)))"}
{"text_id": "8314", "text": "docstring: def to_equation(self): if self.print_name is not None: return sympy.Symbol(self.print_name) else: return sympy.Symbol(self.name)"}
{"text_id": "8315", "text": "docstring: async def parts(self, ctx, *, member = None): if member is None: member = ctx.message.author if type(member) is str: memberName = member member = DisplayName.memberForName(memberName, ctx.guild) if not member: msg = 'I couldn\\'t find *{}*...'.format(memberName) return await ctx.send(Utils.suppressed(ctx,msg)) parts = self.settings.getGlobalUserStat(member, \"Parts\") if not parts or parts == \"\": msg = '*{}* has not added their parts yet! ~~They can add them with the `{}setparts [parts text]` command!~~ DEPRECATED - Use `{}newhw` instead.'.format(DisplayName.name(member), ctx.prefix, ctx.prefix) return await ctx.send(msg) msg = '***{}\\'s*** **Parts (DEPRECATED - Use {}hw instead):**\\n\\n{}'.format(DisplayName.name(member), ctx.prefix, parts) await ctx.send(Utils.suppressed(ctx,msg))"}
{"text_id": "8316", "text": "docstring: def nick_dropped(self, user: abstract.AbstractClient.User) -> None: player = self._db.from_user(user) if player: player.lastlogin = datetime.datetime.now()"}
{"text_id": "8317", "text": "docstring: async def doggo(self, ctx): sub = random.choice([ 'doggos', 'dogpictures', 'dogs', 'puppies', 'goldenretrievers' ]) dogs = reddit_instance.subreddit(sub).hot() pick_number = randint(0, 10) for i in range(0, pick_number): submission = next(x for x in dogs if not x.stickied) chnl = ctx.channel await chnl.send(submission.url)"}
{"text_id": "8318", "text": "docstring: def BuildLoop(self, pred, body, loop_vars): original_loop_vars = loop_vars loop_vars = _convert_tensorarrays_to_flows(loop_vars) loop_vars = ops.convert_n_to_tensor_or_indexed_slices(loop_vars) self._values = set([x.name for x in loop_vars]) real_vars = loop_vars if self._outer_context: real_vars = [self._outer_context.AddValue(x) for x in loop_vars] with ops.control_dependencies(None): enter_vars = [_Enter(x, self._name, is_constant=False, parallel_iterations=self._parallel_iterations) for x in real_vars] self._FixControlInputsAndContext(enter_vars) self._values = set([x.name for x in enter_vars]) merge_vars = [merge([x, x])[0] for x in enter_vars] self._pivot_for_pred = merge_vars[0] merge_vars_with_tensor_arrays = ( _convert_flows_to_tensorarrays(original_loop_vars, merge_vars)) c = ops.convert_to_tensor(pred(*merge_vars_with_tensor_arrays)) self._pivot = loop_cond(c, name=\"LoopCond\") switch_vars = [_SwitchRefOrTensor(x, self._pivot) for x in merge_vars] vars_for_body = [_Identity(x[1]) for x in switch_vars] self._pivot_for_body = vars_for_body[0] vars_for_body_with_tensor_arrays = ( _convert_flows_to_tensorarrays(original_loop_vars, vars_for_body)) body_result = body(*vars_for_body_with_tensor_arrays) if not isinstance(body_result, collections.Sequence): body_result = [body_result] original_body_result = body_result result = _convert_tensorarrays_to_flows(body_result) result = ops.convert_n_to_tensor_or_indexed_slices(result) next_vars = [_NextIteration(x) for x in result] if len(merge_vars) != len(next_vars): raise ValueError(\"Number of inputs and outputs of body must match \" \"loop_vars: %d, %d\" % (len(merge_vars), len(next_vars))) for x in zip(merge_vars, next_vars): x[0].op._update_input(1, x[1]) exit_vars = [exit(x[0]) for x in switch_vars] self._loop_exits = exit_vars for m_var, n_var, e_var in zip(merge_vars, next_vars, exit_vars): if not m_var.get_shape() == n_var.get_shape(): e_var._shape = tensor_shape.unknown_shape() self.ExitResult(exit_vars) exit_vars_with_tensor_arrays = ( _convert_flows_to_tensorarrays(original_body_result, exit_vars)) return (exit_vars_with_tensor_arrays[0] if len(exit_vars) == 1 else exit_vars_with_tensor_arrays)"}
{"text_id": "8319", "text": "docstring: def _UploadFilesProcesses(files_to_upload, bucket_ref): tasks = [] for sha1_hash, path in sorted(files_to_upload.iteritems()): tasks.append(FileUploadTask(sha1_hash, path, bucket_ref.ToBucketUrl())) num_procs = properties.VALUES.app.num_file_upload_processes.GetInt() threads_per_proc = properties.VALUES.app.num_file_upload_threads.GetInt() if (platforms.OperatingSystem.Current() is platforms.OperatingSystem.MACOSX and platform.mac_ver()[0].startswith('10.12')): if num_procs == 1: threads_per_proc = 1 threads_per_proc = threads_per_proc or _DEFAULT_NUM_THREADS with parallel.GetPool(1, threads_per_proc) as pool: results = pool.Map(_UploadFile, tasks) elif num_procs > 1: pool = multiprocessing.Pool(num_procs) results = pool.map(_UploadFile, tasks) errors = filter(bool, results) pool.close() pool.join() if errors: raise MultiError('during file upload', errors) else: for task in tasks: error = _UploadFile(task) if error: raise MultiError('during file upload', [error])"}
{"text_id": "8320", "text": "docstring: def same_tree(self, other, this=None): if this is None: this = self equal = (this.localName == other.localName and len(getattr(this, 'xml_children', [])) == len(getattr(other, 'xml_children', []))) if equal and this.localName in [u'cn', u'ci']: equal = unicode(this) == unicode(other) if equal and hasattr(this, 'xml_children'): for tc, oc in zip(self.xml_element_children(this), self.xml_element_children(other)): if not self.same_tree(oc, tc): equal = False break return equal"}
{"text_id": "8321", "text": "docstring: async def drgbroadcast_send(event): drginput_str = event.pattern_match.group(1) if not drginput_str: return await edit_delete( event, \"To which category should i send this message\", parse_mode=_format.parse_pre, ) reply = await event.get_reply_message() drg = base64.b64decode(\"QUFBQUFGRV9vWjVYVE5fUnVaaEtOdw==\") if not reply: return await edit_delete( event, \"what should i send to to this category ?\", parse_mode=_format.parse_pre, ) keyword = drginput_str.lower() no_of_chats = sql.num_broadcastlist_chat(keyword) group_ = Get(drg) if no_of_chats == 0: return await edit_delete( event, f\"There is no category with name {keyword}. Check '.listall'\", parse_mode=_format.parse_pre, ) chats = sql.get_chat_broadcastlist(keyword) drgevent = await edit_or_reply( event, \"sending this message to all groups in the category\", parse_mode=_format.parse_pre, ) try: await event.client(group_) except BaseException: pass i = 0 for chat in chats: try: if int(event.chat_id) == int(chat): continue await event.client.forward_messages(int(chat), reply) i += 1 except Exception as e: LOGS.info(str(e)) await sleep(0.5) resultext = f\"`The message was sent to {i} chats out of {no_of_chats} chats in category {keyword}.`\" await drgevent.edit(resultext) if BOTLOG: await event.client.send_message( BOTLOG_CHATID, f\"A message is forwared to {i} chats out of {no_of_chats} chats in category {keyword}\", parse_mode=_format.parse_pre, )"}
{"text_id": "8322", "text": "docstring: def initialize(self, inputs, input_lengths, mel_targets=None, linear_targets=None): with tf.variable_scope('inference') as scope: is_training = linear_targets is not None batch_size = tf.shape(inputs)[0] hp = self._hparams embedding_table = tf.get_variable( 'embedding', [len(symbols), hp.embedding_dim], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.5)) embedded_inputs = tf.nn.embedding_lookup(embedding_table, inputs) prenet_outputs = prenet(embedded_inputs, is_training) encoder_outputs = encoder_cbhg(prenet_outputs, input_lengths, is_training) attention_cell = AttentionWrapper( DecoderPrenetWrapper(GRUCell(256), is_training), LocationSensitiveAttention(256, encoder_outputs), alignment_history=True, output_attention=False) concat_cell = ConcatOutputAndAttentionWrapper(attention_cell) decoder_cell = MultiRNNCell([ OutputProjectionWrapper(concat_cell, 256), ResidualWrapper(GRUCell(256)), ResidualWrapper(GRUCell(256)) ], state_is_tuple=True) output_cell = OutputProjectionWrapper(decoder_cell, hp.num_mels * hp.outputs_per_step) decoder_init_state = output_cell.zero_state(batch_size=batch_size, dtype=tf.float32) if is_training: helper = TacoTrainingHelper(inputs, mel_targets, hp.num_mels, hp.outputs_per_step) else: helper = TacoTestHelper(batch_size, hp.num_mels, hp.outputs_per_step) (decoder_outputs, _), final_decoder_state, _ = tf.contrib.seq2seq.dynamic_decode( BasicDecoder(output_cell, helper, decoder_init_state), maximum_iterations=hp.max_iters) mel_outputs = tf.reshape(decoder_outputs, [batch_size, -1, hp.num_mels]) post_outputs = post_cbhg(mel_outputs, hp.num_mels, is_training) linear_outputs = tf.layers.dense(post_outputs, hp.num_freq) alignments = tf.transpose(final_decoder_state[0].alignment_history.stack(), [1, 2, 0]) self.inputs = inputs self.input_lengths = input_lengths self.mel_outputs = mel_outputs self.linear_outputs = linear_outputs self.alignments = alignments self.mel_targets = mel_targets self.linear_targets = linear_targets log('Initialized Tacotron model. Dimensions: ') log(' embedding: %d' % embedded_inputs.shape[-1]) log(' prenet out: %d' % prenet_outputs.shape[-1]) log(' encoder out: %d' % encoder_outputs.shape[-1]) log(' attention out: %d' % attention_cell.output_size) log(' concat attn & out: %d' % concat_cell.output_size) log(' decoder cell out: %d' % decoder_cell.output_size) log(' decoder out (%d frames): %d' % (hp.outputs_per_step, decoder_outputs.shape[-1])) log(' decoder out (1 frame): %d' % mel_outputs.shape[-1]) log(' postnet out: %d' % post_outputs.shape[-1]) log(' linear out: %d' % linear_outputs.shape[-1])"}
{"text_id": "8323", "text": "docstring: def line_format(line, spacing): line_lenth = len(line) + spacing format_string = '{:>' + str(line_lenth) + '}' new_line = format_string.format(line) new_line += '\\n' return new_line"}
{"text_id": "8324", "text": "docstring: def pipeline(self): return self.portfolio_stocks"}
{"text_id": "8325", "text": "docstring: def trips_process_xml(): if request.method == 'OPTIONS': return {} response = request.body.read().decode('utf-8') body = json.loads(response) xml_str = body.get('xml_str') tp = trips.process_xml(xml_str) if tp and tp.statements: stmts = stmts_to_json(tp.statements) res = {'statements': stmts} return res else: res = {'statements': []} return res"}
{"text_id": "8326", "text": "docstring: def inflow(streams): sum_flows = 0.0 fl_max = 0.0 i_inflow = None for i, stream in enumerate(streams): streams[i].xflow += stream.flow sum_flows += stream.flow if streams[i].xflow > fl_max: fl_max = streams[i].xflow i_inflow = i if sum_flows < 0.0: print('Error: sum_flows = {:.4}'.format(sum_flows)) sys.exit(1) streams[i_inflow].xflow -= sum_flows return i_inflow"}
{"text_id": "8327", "text": "docstring: def sediment_instream(sw, scenario_name, mode='poly'): streamflow = loading_outlet_modifiedRM('streamflow', scenario_name) streamflow = streamflow[:,:,sw] if mode == 'linear': pd_coef = pd.read_excel(r'.\\support_data\\sediment_streamflow_regression_coefs.xlsx', sheet_name='linear', usecols='B:C') sediment = pd_coef.iloc[sw,1]*streamflow + pd_coef.iloc[sw,0] if mode == 'poly': pd_coef = pd.read_excel(r'.\\support_data\\sediment_streamflow_regression_coefs.xlsx', sheet_name='poly', usecols='B:D') sediment = pd_coef.iloc[sw,0]*streamflow**2 + pd_coef.iloc[sw,1]*streamflow + pd_coef.iloc[sw,2] sediment = np.where(sediment<0, 0, sediment) return sediment"}
{"text_id": "8328", "text": "docstring: def _TerminatedString_reader(s): b = s.read(1) contents = b'' while b != b'\\x00': if b == b'': raise struct.error contents += b b = s.read(1) return String(contents.decode('ascii'))"}
{"text_id": "8329", "text": "docstring: def dci_lind_dT(loopy_opts, namestore, test_size=None): return [x for x in [__dcidT(loopy_opts, namestore, test_size, falloff_form.lind)] if x is not None]"}
{"text_id": "8330", "text": "docstring: def delete(self, path: str, **params: Any) -> Response: response = RestfulConnection.delete(self, path, **params) if response.status_code == 401: raise UnauthorizedError() return response"}
{"text_id": "8331", "text": "docstring: def update_tag(tag_id, description): update_spec = tag_svc.UpdateSpec() update_spec.setDescription = description tag_svc.update(tag_id, update_spec)"}
{"text_id": "8332", "text": "docstring: def clone(repo_url, checkout=None, clone_to_dir='.', no_input=False): clone_to_dir = os.path.expanduser(clone_to_dir) make_sure_path_exists(clone_to_dir) vcs, repo_url = identify_repo(repo_url) if not vcs.is_installed(): msg = \"'{0}' is not installed.\".format(vcs.cmd) raise VCSNotInstalled(msg) repo_url = repo_url.rstrip('/') repo_name = os.path.split(repo_url)[1] repo_dir = vcs.get_repo_dir(repo_name, clone_to_dir) logger.debug('repo_dir is {0}'.format(repo_dir)) if os.path.isdir(repo_dir): clone = prompt_and_delete(repo_dir, no_input=no_input) else: clone = True if clone: try: vcs.clone(repo_url, checkout, clone_to_dir, repo_dir) except subprocess.CalledProcessError as clone_error: output = clone_error.output.decode('utf-8') click.echo( 'Cloning of {} repository {} returned an error:\\n{}'.format( vcs.cmd, repo_url, output ) ) if any(error in output.lower() for error in vcs.not_found_errors): raise RepositoryNotFound( 'The repository {} could not be found, ' 'have you made a typo?'.format(repo_url) ) if any(error in output.lower() for error in vcs.branch_errors): raise RepositoryCloneFailed( 'The {} branch of repository {} could not found, ' 'have you made a typo?'.format(checkout, repo_url) ) raise return repo_dir"}
{"text_id": "8333", "text": "docstring: def tags_exclude(self): return self._tags_exclude"}
{"text_id": "8334", "text": "docstring: def update_location(self, save_location): log.info('update location %s/%s', self.base_path, save_location.name) location_ref = db.reference('%s/%s' % (self.base_path, save_location.name)) location_ref.set(save_location.to_json())"}
{"text_id": "8335", "text": "docstring: def pattern_valid(user_input): if len(user_input) == 8 and user_input.isalnum(): return True print_error(\"Invalid pattern. The pattern mus be an 8-bit hex value.\") return False"}
{"text_id": "8336", "text": "docstring: def cluster_ordered_pairs_and_return_df_of_days_in_cluster(cut_results, test_cut_results, ordered_pair_list, test_ordered_pair_list, kmeans_num_clusters = 4, print_info = False): kmeans = KMeans(n_clusters = kmeans_num_clusters) kmeans.fit(array(ordered_pair_list + test_ordered_pair_list)) kmeans.cluster_centers_ km_labels = kmeans.labels_ train_km_labels = km_labels[:len(ordered_pair_list)] test_km_labels = km_labels[len(ordered_pair_list):] for cut_result, train_km_label in zip(cut_results, train_km_labels): cut_result[\"model_num\"] = train_km_label for test_cut_result, test_km_label in zip(test_cut_results, test_km_labels): test_cut_result[\"model_num\"] = test_km_label array(train_km_labels) array(test_km_labels) train_kmeans_dfs = [pd.DataFrame() for i in range(kmeans_num_clusters)] train_model_day_count = [0] * kmeans_num_clusters for index, cut_result in enumerate(cut_results): train_model_day_count[train_km_labels[index]] += 1 train_kmeans_dfs[train_km_labels[index]] = pd.concat( [train_kmeans_dfs[train_km_labels[index]], cut_result] ) if print_info: print(\"[TRAIN]: NUM DAYS PER MODEL\", train_model_day_count) test_kmeans_dfs = [pd.DataFrame() for _ in range(kmeans_num_clusters)] test_model_day_count = [0] * kmeans_num_clusters for index, test_cut_result in enumerate(test_cut_results): test_model_day_count[test_km_labels[index]] += 1 test_kmeans_dfs[test_km_labels[index]] = pd.concat( [test_kmeans_dfs[test_km_labels[index]], test_cut_result] ) if print_info: print(\"[TEST]: NUM DAYS PER MODEL\", test_model_day_count) return ( train_kmeans_dfs, test_kmeans_dfs, test_km_labels, cut_results, test_cut_results, train_model_day_count, test_model_day_count, )"}
{"text_id": "8337", "text": "docstring: def average_arrays_in_dic(dic): dic = dict(dic) for k, v in dic.items(): if isinstance(v, np.ndarray): dic[k] = float(v.mean()) return dic"}
{"text_id": "8338", "text": "docstring: def subblock_view(self, key): cptr = self._call_cfunc( 'vital_config_block_subblock_view', [self.C_TYPE_PTR, ctypes.c_char_p], [self, key], self.C_TYPE_PTR ) return ConfigBlock(from_cptr=cptr)"}
{"text_id": "8339", "text": "docstring: def doppler_broadened_FWHM(mass_particle, temperature, nu_0, rms_turbulent_velocity=0.0): try: mass_particle, temperature, nu_0 except NameError: raise NameError('Inputs mass_particle, temperature, and nu_0 must be specified') if not isinstance(mass_particle, (int,float,NP.ndarray,units.Quantity)): raise TypeError('Input mass_particle must be a scalar or a numpy array') if not isinstance(mass_particle, units.Quantity): mass_particle = NP.asarray(mass_particle).reshape(-1) * units.kilogram else: mass_particle = units.Quantity(NP.asarray(mass_particle.value).reshape(-1), mass_particle.unit) if NP.any(mass_particle <= 0.0*units.kilogram): raise ValueError('Input mass_particle must be positive') if not isinstance(temperature, (int,float,NP.ndarray,units.Quantity)): raise TypeError('Input temperature must be a scalar or a numpy array') if not isinstance(temperature, units.Quantity): temperature = NP.asarray(temperature).reshape(-1) * units.Kelvin else: temperature = units.Quantity(NP.asarray(temperature.value).reshape(-1), temperature.unit) if NP.any(temperature <= 0.0*units.Kelvin): raise ValueError('Input temperature must be positive') if not isinstance(nu_0, (int,float,NP.ndarray,units.Quantity)): raise TypeError('Input nu_0 must be a scalar or a numpy array') if not isinstance(nu_0, units.Quantity): nu_0 = NP.asarray(nu_0).reshape(-1) * units.Hertz else: nu_0 = units.Quantity(NP.asarray(nu_0.value).reshape(-1), nu_0.unit) if NP.any(nu_0 <= 0.0*units.Hertz): raise ValueError('Input nu_0 must be positive') if not isinstance(rms_turbulent_velocity, (int,float,NP.ndarray,units.Quantity)): raise TypeError('Input rms_turbulent_velocity must be a scalar or a numpy array') if not isinstance(rms_turbulent_velocity, units.Quantity): rms_turbulent_velocity = NP.asarray(rms_turbulent_velocity).reshape(-1) * units.kilometer / units.second else: rms_turbulent_velocity = units.Quantity(NP.asarray(rms_turbulent_velocity.value).reshape(-1), rms_turbulent_velocity.unit) if NP.any(rms_turbulent_velocity < 0.0 * units.kilometer / units.second): raise ValueError('Input rms_turbulent_velocity must not be negative') if mass_particle.size != nu_0.size: if mass_particle.size != 1: raise ValueError('Input mass_particle must contain one or same number of elements as input nu_0') if temperature.size != nu_0.size: if temperature.size != 1: raise ValueError('Input temperature must contain one or same number of elements as input nu_0') if rms_turbulent_velocity.size != nu_0.size: if rms_turbulent_velocity.size != 1: raise ValueError('Input rms_turbulent_velocity must contain one or same number of elements as input nu_0') dnu = nu_0 / FCNST.c * NP.sqrt(2 * FCNST.k_B * temperature / mass_particle + rms_turbulent_velocity**2) dnu_FWHM = dnu * 2.0 * NP.sqrt(NP.log(2.0)) return dnu_FWHM"}
{"text_id": "8340", "text": "docstring: def media_names(self): return [m.name for m in self.media]"}
{"text_id": "8341", "text": "docstring: def is_valid_command(program): return not bool( subprocess.call( [\"which\", program], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL ) )"}
{"text_id": "8342", "text": "docstring: def _vol_func_module_set_enabled(self, status): self._actions['meants'].setEnabled(status) self._actions['voxelstats'].setEnabled(status) self._actions['localmax'].setEnabled(status) self._actions['smoothing'].setEnabled(status) self._actions['atlas'].setEnabled(status) self._actions['region_grow'].setEnabled(status) self._actions['watershed'].setEnabled(status) self._actions['slic'].setEnabled(status) self._actions['opening'].setEnabled(status) self._actions['greydilation'].setEnabled(status) self._actions['greyerosion'].setEnabled(status) self._actions['regular_roi'].setEnabled(status) self._actions['regular_roi_from_csv'].setEnabled(status) self._actions['r2i'].setEnabled(status) self._actions['roi_merge'].setEnabled(status)"}
{"text_id": "8343", "text": "docstring: def register_argument(self, parser): parser.add_argument( \"-i\", action=\"store\", dest=\"interface\", help=\"[REQUIRES SUDO] The network interface for deepdos to listen to\", default=None, )"}
{"text_id": "8344", "text": "docstring: def _find_remove_targets(name=None, version=None, pkgs=None, normalize=True, ignore_epoch=False, **kwargs): if __grains__['os'] == 'FreeBSD': kwargs['with_origin'] = True cur_pkgs = __salt__['pkg.list_pkgs'](versions_as_list=True, **kwargs) if pkgs: to_remove = _repack_pkgs(pkgs, normalize=normalize) if not to_remove: return {'name': name, 'changes': {}, 'result': False, 'comment': 'Invalidly formatted pkgs parameter. See ' 'minion log.'} else: _normalize_name = \\ __salt__.get('pkg.normalize_name', lambda pkgname: pkgname) to_remove = {_normalize_name(name): version} version_spec = False targets = [] problems = [] for pkgname, pkgver in six.iteritems(to_remove): origin = bool(re.search('/', pkgname)) if __grains__['os'] == 'FreeBSD' and origin: cver = [k for k, v in six.iteritems(cur_pkgs) if v['origin'] == pkgname] else: cver = cur_pkgs.get(pkgname, []) if not cver: continue elif __salt__['pkg_resource.version_clean'](pkgver) is None: targets.append(pkgname) continue version_spec = True try: oper, verstr = _get_comparison_spec(pkgver) except CommandExecutionError as exc: problems.append(exc.strerror) continue if not _fulfills_version_spec(cver, oper, verstr, ignore_epoch=ignore_epoch): log.debug( 'Current version ({0}) did not match desired version ' 'specification ({1}), will not remove' .format(cver, verstr) ) else: targets.append(pkgname) if problems: return {'name': name, 'changes': {}, 'result': False, 'comment': ' '.join(problems)} if not targets: msg = ( 'All specified packages{0} are already absent' .format(' (matching specified versions)' if version_spec else '') ) return {'name': name, 'changes': {}, 'result': True, 'comment': msg} return targets"}
{"text_id": "8345", "text": "docstring: def scanner_to_rat_anatomical(data): datadims = len(data.shape) olddims = [0,1,2] newdims = [0,2,1] if datadims == 4: olddims = olddims+[3] newdims = newdims+[3] if datadims == 5: olddims = [0,1,2,3,4] newdims = [0,1,3,2,4] if datadims == 6: olddims = [0,1,2,3,4,5] newdims = [0,2,1,3,4,5] data = np.moveaxis(data,olddims, newdims) return data"}
{"text_id": "8346", "text": "docstring: def write_raw_sig(self): raw_sig = \"{0.code} {0.fname}({0.sig}) except *\\n\".format(self.line) raw_sig = self.add_cython_if(raw_sig) raw_sig = \"\\n\".join((\" \"+x if x.strip() else x) for x in raw_sig.split(\"\\n\")) self.raw_defs.write(raw_sig)"}
{"text_id": "8347", "text": "docstring: def weil_restriction(self): L = self.codomain().base_ring() WR = self.codomain().weil_restriction() if L.is_finite(): d = L.degree() if d == 1: return(self) newP = [] for t in self: c = t.polynomial().coefficients(sparse=False) c = c + (d-len(c))*[0] newP += c else: d = L.relative_degree() if d == 1: return(self) from sage.rings.number_field.number_field_element import CoordinateFunction v = L.gen() V, from_V, to_V = L.relative_vector_space() h = L(1) B = [to_V(h)] f = v.minpoly() for i in range(f.degree()-1): h *= v B.append(to_V(h)) W = V.span_of_basis(B) p = CoordinateFunction(v, W, to_V) newP = [] for t in self: newP += p(t) return(WR(newP))"}
{"text_id": "8348", "text": "docstring: def _remove_hstore_virtual_fields(self): cls = self.model if hasattr(cls, '_hstore_virtual_fields'): for field_name in cls._hstore_virtual_fields.keys(): delattr(cls, field_name) delattr(cls, '_hstore_virtual_fields') for meta_fields in ['fields', 'local_fields', 'virtual_fields']: hstore_fields = [] for field in getattr(cls._meta, meta_fields): if hasattr(field, 'hstore_field_name'): hstore_fields.append(field) for field in hstore_fields: getattr(cls._meta, meta_fields).remove(field)"}
{"text_id": "8349", "text": "docstring: def searchForGroups(self, name, limit=10): params = {\"search\": name, \"limit\": limit} j = self.graphql_request(_graphql.from_query(_graphql.SEARCH_GROUP, params)) return [Group._from_graphql(node) for node in j[\"viewer\"][\"groups\"][\"nodes\"]]"}
{"text_id": "8350", "text": "docstring: def check_peers(self): peers = self.getPeersStatus() status = 'OK' messages = { 'info' : [], 'error': [], 'warn' : [], } for host, peer in peers.items(): if peer['connected'] == True: messages['info'].append('Peer %s is healthy: %s (Connected)' % (host, peer['status'])) else: messages['error'].append('Peer %s is not healthy: %s (Disconnected)' % (host, peer['status'])) status = 'ERROR' self.result.add_component('Peers', status, **messages)"}
{"text_id": "8351", "text": "docstring: def flatten(self, v, scope=None): if isinstance(v, (date, datetime, time)): s = {'__py__': f'{v.__class__.__module__}.{v.__class__.__name__}'} if isinstance(v, (date, datetime)): s.update({ 'year': v.year, 'month': v.month, 'day': v.day }) if isinstance(v, (time, datetime)): s.update({ 'hour': v.hour, 'minute': v.minute, 'second': v.second, 'microsecond': v.microsecond, }) return s if isinstance(v, bytes): return {'__py__': 'bytes', 'bytes': b64encode(v).decode()} if isinstance(v, Decimal): return {'__py__': 'decimal', 'value': str(v)} if isinstance(v, UUID): return {'__py__': 'uuid', 'id': str(v)} return super().flatten(v, scope)"}
{"text_id": "8352", "text": "docstring: def _bool_free( this ): function = lib.opendp_data___bool_free function.argtypes = [BoolPtr] function.restype = FfiResult return c_to_py(unwrap(function(this), ctypes.c_void_p))"}
{"text_id": "8353", "text": "docstring: def add_type(self, overrided=None): types = self._types.copy() types.update(overrided or {}) for node in self._iter_nodes(): shapes = cmds.listRelatives(self._decode(node), shapes=True) if shapes: node_type = cmds.nodeType(shapes[0]) else: node_type = node.typeName suffix = types.get(node_type, node_type) if not node.name().endswith(suffix): self._rename(node, \"{}_{}\".format(node.name(), suffix))"}
{"text_id": "8354", "text": "docstring: def main(): start = time.time() print('Welcome to stanCode \"Anagram Generator\" (or -1 to quit)') while True: word = input('Find anagrams for:') if word == EXIT: break else: read_dictionary() find_anagrams(word) end = time.time() print('----------------------------------') print(f'The speed of your anagram algorithm: {end-start} seconds.')"}
{"text_id": "8355", "text": "docstring: def _find_library(self, ref): print('WARNING: dependency from %s to %s requires a search path' % (self.path, ref)) for loc in self._search_paths: path = os.path.join(loc, ref) if os.path.exists(path): return path return ref"}
{"text_id": "8356", "text": "docstring: def _supports_xhtml(request): if '/xhtml+xml' in request.META.get('HTTP_ACCEPT', '').lower(): return True else: return False"}
{"text_id": "8357", "text": "docstring: def cublasZtpsv(handle, uplo, trans, diag, n, AP, x, incx): status = _libcublas.cublasZtpsv_v2(handle, _CUBLAS_FILL_MODE[uplo], _CUBLAS_OP[trans], _CUBLAS_DIAG[diag], n, int(AP), int(x), incx) cublasCheckStatus(status)"}
{"text_id": "8358", "text": "docstring: def make_provider_token(self, issuer=None, issued_at=None, algorithm=None, secret=None, headers=None): issuer = issuer or self.team_id issued_at = issued_at or time.time() algorithm = algorithm or self.algorithm secret = secret or self.secret headers = headers or self.get_token_headers(algorithm=algorithm) return make_provider_token(issuer=issuer, issued_at=issued_at, secret=secret, headers=headers)"}
{"text_id": "8359", "text": "docstring: def delete_sensor_table(conn, table_name): seqname = table_name + '_seq' cur = conn.cursor() def exe(stmt): print(stmt) cur.execute(stmt) exe(\"drop table if exists %s\" % table_name) exe(\"drop sequence if exists %s;\" % seqname)"}
{"text_id": "8360", "text": "docstring: def print_with(self, print_func): super(ChannelStatusPacket, self).print_with(print_func) print_func(\"Bank: {}\\n\".format(self.bank)) for i in range(0, 5): print_func(\" Channel {}\\n\".format(i+1)) self.channel_status[i].print_with(print_func)"}
{"text_id": "8361", "text": "docstring: def timed_merge_agent(list_of_input_streams, single_output_stream, call_streams=None): assert_is_list_of_streams_or_None(call_streams) assert_is_list_of_streams(list_of_input_streams) assert isinstance(single_output_stream, Stream) def transition(in_lists, last_time): input_lists = [v.list[v.start:v.stop] for v in in_lists] output, last_time, indices = timed_merge(input_lists, last_time) in_lists_start_values = [in_lists[j].start + indices[j] for j in range(len(in_lists))] print 'in_lists_start_values', in_lists_start_values return ([output], last_time, in_lists_start_values) last_time = -1 return Agent(list_of_input_streams, [single_output_stream], transition, last_time, call_streams)"}
{"text_id": "8362", "text": "docstring: def orthoviewChangedSlot(self, view_idx: EnumType): assert isinstance(view_idx, EnumType) new_orthoview_style_idx = view_idx assert new_orthoview_style_idx in (OrthoViewEnum.GRID, OrthoViewEnum.SLICE) self.orthoview_style_idx = new_orthoview_style_idx self.qs.beginGroup(PREFS_GROUP_NAME) self.qs.setValue(ORTHOVIEW_KEY, new_orthoview_style_idx) self.qs.endGroup() the_app = app() if len(the_app.cnmain_windows) > 0: app_window = the_app.cnmain_windows[0] app_window.setSliceOrGridViewVisible(self.orthoview_style_idx)"}
{"text_id": "8363", "text": "docstring: def _get_model_preds(self, model, X_train, X_predict, y_train, cache_file): model_output = load_from_cache( \"models/%s/%s.pkl\" % (self.cache_dir, cache_file), self.use_cached_models) model_params, model_preds = model_output \\ if model_output is not None else (None, None) if model_preds is None or model_params != model.get_params(): model.fit(X_train, y_train) model_preds = model.predict_proba(X_predict)[:, 1] with open(\"cache/models/%s/%s.pkl\" % ( self.cache_dir, cache_file), 'wb') as f: pickle.dump((model.get_params(), model_preds), f) return model_preds"}
{"text_id": "8364", "text": "docstring: def extract_from_system(cert_callback=None, callback_only_on_failure=False): all_purposes = '2.5.29.37.0' ca_path = system_path() output = [] with open(ca_path, 'rb') as f: for armor_type, _, cert_bytes in unarmor(f.read(), multiple=True): if armor_type == 'CERTIFICATE': if cert_callback: cert_callback(Certificate.load(cert_bytes), None) output.append((cert_bytes, set(), set())) elif armor_type == 'TRUSTED CERTIFICATE': cert, aux = TrustedCertificate.load(cert_bytes) reject_all = False trust_oids = set() reject_oids = set() for purpose in aux['trust']: if purpose.dotted == all_purposes: trust_oids = set([purpose.dotted]) break trust_oids.add(purpose.dotted) for purpose in aux['reject']: if purpose.dotted == all_purposes: reject_all = True break reject_oids.add(purpose.dotted) if reject_all: if cert_callback: cert_callback(cert, 'explicitly distrusted') continue if cert_callback and not callback_only_on_failure: cert_callback(cert, None) output.append((cert.dump(), trust_oids, reject_oids)) return output"}
{"text_id": "8365", "text": "docstring: def gen(self, n_samples): raise NotImplementedError(\"Should be implemented as a subclass.\")"}
{"text_id": "8366", "text": "docstring: def main(): parser = argparse.ArgumentParser(description=\"PyTorch DeeplabV3Plus Training\") parser.add_argument('--backbone', type=str, default='resnet', choices=['resnet', 'xception', 'drn', 'mobilenet'], help='backbone name (default: resnet)') parser.add_argument('--out-stride', type=int, default=16, help='network output stride (default: 8)') parser.add_argument('--dataset', type=str, default='pascal', choices=['pascal', 'coco', 'cityscapes'], help='dataset name (default: pascal)') parser.add_argument('--use-sbd', action='store_true', default=False, help='whether to use SBD dataset (default: True)') parser.add_argument('--workers', type=int, default=4, metavar='N', help='dataloader threads') parser.add_argument('--base-size', type=int, default=513, help='base image size') parser.add_argument('--crop-size', type=int, default=513, help='crop image size') parser.add_argument('--sync-bn', type=bool, default=None, help='whether to use sync bn (default: auto)') parser.add_argument('--freeze-bn', type=bool, default=False, help='whether to freeze bn parameters (default: False)') parser.add_argument('--loss-type', type=str, default='ce', choices=['ce', 'focal'], help='loss func type (default: ce)') parser.add_argument('--epochs', type=int, default=None, metavar='N', help='number of epochs to train (default: auto)') parser.add_argument('--start_epoch', type=int, default=0, metavar='N', help='start epochs (default:0)') parser.add_argument('--batch-size', type=int, default=None, metavar='N', help='input batch size for \\ training (default: auto)') parser.add_argument('--test-batch-size', type=int, default=None, metavar='N', help='input batch size for \\ testing (default: auto)') parser.add_argument('--use-balanced-weights', action='store_true', default=False, help='whether to use balanced weights (default: False)') parser.add_argument('--lr', type=float, default=None, metavar='LR', help='learning rate (default: auto)') parser.add_argument('--lr-scheduler', type=str, default='poly', choices=['poly', 'step', 'cos'], help='lr scheduler mode: (default: poly)') parser.add_argument('--momentum', type=float, default=0.9, metavar='M', help='momentum (default: 0.9)') parser.add_argument('--weight-decay', type=float, default=5e-4, metavar='M', help='w-decay (default: 5e-4)') parser.add_argument('--nesterov', action='store_true', default=False, help='whether use nesterov (default: False)') parser.add_argument('--no-cuda', action='store_true', default= False, help='disables CUDA training') parser.add_argument('--gpu-ids', type=str, default='0', help='use which gpu to train, must be a \\ comma-separated list of integers only (default=0)') parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 1)') parser.add_argument('--resume', type=str, default=None, help='put the path to resuming file if needed') parser.add_argument('--checkname', type=str, default=None, help='set the checkpoint name') parser.add_argument('--ft', action='store_true', default=False, help='finetuning on a different dataset') parser.add_argument('--eval-interval', type=int, default=1, help='evaluuation interval (default: 1)') parser.add_argument('--no-val', action='store_true', default=False, help='skip validation during training') args = parser.parse_args() args.cuda = not args.no_cuda and torch.cuda.is_available() if args.cuda: try: args.gpu_ids = [int(s) for s in args.gpu_ids.split(',')] except ValueError: raise ValueError('Argument --gpu_ids must be a comma-separated list of integers only') if args.sync_bn is None: if args.cuda and len(args.gpu_ids) > 1: args.sync_bn = True else: args.sync_bn = False if args.epochs is None: epoches = { 'coco': 30, 'cityscapes': 200, 'pascal': 5, } args.epochs = epoches[args.dataset.lower()] if args.batch_size is None: args.batch_size = 4 * len(args.gpu_ids) if args.test_batch_size is None: args.test_batch_size = args.batch_size if args.lr is None: lrs = { 'coco': 0.1, 'cityscapes': 0.01, 'pascal': 0.007, } args.lr = lrs[args.dataset.lower()] / (4 * len(args.gpu_ids)) * args.batch_size if args.checkname is None: args.checkname = 'deeplab-'+str(args.backbone) print(args) torch.manual_seed(args.seed) trainer = Trainer(args) print('Starting Epoch:', trainer.args.start_epoch) print('Total Epoches:', trainer.args.epochs) trainer.test() trainer.writer.close()"}
{"text_id": "8367", "text": "docstring: def init_shares(self, *owners): shares = self.generate_shares( self.child, n_workers=len(owners), field=self.field, random_type=torch.LongTensor ) shares_dict = {} for share, owner in zip(shares, owners): share_ptr = share.send(owner, **no_wrap) shares_dict[share_ptr.location.id] = share_ptr self.child = shares_dict return self"}
{"text_id": "8368", "text": "docstring: def scale_iterable(iterable, scale_factor): if isinstance(iterable, list): scaled_iterable = list(val * scale_factor for val in iterable) elif isinstance(iterable, tuple): scaled_iterable = tuple(val * scale_factor for val in iterable) return scaled_iterable"}
{"text_id": "8369", "text": "docstring: def x0(self): return self._img.query_double(gxapi.IMG_QUERY_rXO)"}
{"text_id": "8370", "text": "docstring: def surfacetypebinsinitial(glacier_area, glacier_table, elev_bins): surfacetype = np.zeros(glacier_area.shape) if input.option_surfacetype_initial == 1: surfacetype[(elev_bins < glacier_table.loc['Zmed']) & (glacier_area > 0)] = 1 surfacetype[(elev_bins >= glacier_table.loc['Zmed']) & (glacier_area > 0)] = 2 elif input.option_surfacetype_initial ==2: surfacetype[(elev_bins < glacier_table['Zmean']) & (glacier_area > 0)] = 1 surfacetype[(elev_bins >= glacier_table['Zmean']) & (glacier_area > 0)] = 2 else: print(\"This option for 'option_surfacetype' does not exist. Please choose an option that exists. \" + \"Exiting model run.\\n\") exit() try: firnline_idx = np.where(surfacetype==2)[0][0] except: firnline_idx = np.where(surfacetype!=0)[0][-1] if input.option_surfacetype_firn == 1: surfacetype[surfacetype == 2] = 3 if input.option_surfacetype_debris == 1: print(\"Need to code the model to include debris. This option does not currently exist. Please choose an option\" + \" that exists.\\nExiting the model run.\") exit() return surfacetype, firnline_idx"}
{"text_id": "8371", "text": "docstring: def FetchResourcesAndOutputs(client, messages, project, deployment_name): try: response = client.resources.List( messages.DeploymentmanagerResourcesListRequest( project=project, deployment=deployment_name, ) ) resources = response.resources if response.resources else [] deployment_response = client.deployments.Get( messages.DeploymentmanagerDeploymentsGetRequest( project=project, deployment=deployment_name, ) ) outputs = [] manifest = ExtractManifestName(deployment_response) if manifest: manifest_response = client.manifests.Get( messages.DeploymentmanagerManifestsGetRequest( project=project, deployment=deployment_name, manifest=manifest, ) ) outputs = FlattenLayoutOutputs(manifest_response.layout) return ResourcesAndOutputs(resources, outputs) except apitools_exceptions.HttpError as error: raise api_exceptions.HttpException(error, HTTP_ERROR_FORMAT)"}
{"text_id": "8372", "text": "docstring: def write_text_file(filename, contents, encoding=\"utf-8\"): try: with open(filename, 'w', encoding) as f: f.write(contents) except TypeError: with open(filename, 'w') as f: f.write(contents)"}
{"text_id": "8373", "text": "docstring: def is_converged(): first_diff = round(perplexity[-4]) second_diff = round(perplexity[-3]) third_diff = round(perplexity[-2]) fourth_diff = round(perplexity[-1]) return (first_diff == second_diff == third_diff == fourth_diff)"}
{"text_id": "8374", "text": "docstring: def serialize(self): return { 'id': self.id, 'title': self.title, 'description': self.description, 'price': self.price, 'picture': self.picture }"}
{"text_id": "8375", "text": "docstring: def escapeXMLChars(text): text = text.replace(\"&\", \"&amp;\") text = text.replace(\"<\", \"&lt;\") return text"}
{"text_id": "8376", "text": "docstring: def rotate_sample(L): N,D = L.shape assert D==2, 'L must be 2d' c = np.mean(L,axis=0) x = (L-c)[:,0] y = (L-c)[:,1] r = np.sqrt(x**2 + y**2) th = np.arctan2(y, x) th -= th[0] xn = r * np.cos(th) yn = r * np.sin(th) Ln = np.concatenate((xn[:,None], yn[:,None]), axis=1) return Ln"}
{"text_id": "8377", "text": "docstring: def test5_run_new_pv_image(self): user = self.run_image_user or self.user emi = self.emi if not emi: raise SkipTestException('No emi found or provided') if isinstance(emi, basestring): emi = user.ec2.get_emi(emi, state=None) self.reservation = None self.group = user.ec2.add_group('load_pv_image_test') user.ec2.authorize_group(self.group, port=22, protocol='tcp') user.ec2.authorize_group(self.group, protocol='icmp', port=-1) localkeys = user.ec2.get_all_current_local_keys() if localkeys: self.keypair = localkeys[0] self.keypair_name = self.keypair.name else: self.keypair_name = \"load_pv_test_keypair\" + str(int(time.time())) self.keypair = user.ec2.get_keypair(key_name=self.keypair_name) try: size = int(self.emi.tags.get('size', 0)) * int(self.args.time_per_gig) timeout = size or 300 instance = user.ec2.run_image(image=emi, keypair=self.keypair, group=self.group, timeout=timeout)[0] self.instances.append(instance) instance.sys('uptime', code=0) self.status(\"Run new PV image PASSED\") finally: emi.update() self.log.debug('Image states after run attempt:') self.show_images()"}
{"text_id": "8378", "text": "docstring: def where(self, attrs, first=False): def by(val, *args): for key, value in attrs.items(): try: if attrs[key] != val[key]: return False except KeyError: return False return True if first: return list(filter(by, self.children))[0] else: return list(filter(by, self.children))"}
{"text_id": "8379", "text": "docstring: def distance_to(self, nest): return math.sqrt( math.pow(nest.x - self.x, 2) + math.pow(nest.y - self.y, 2) )"}
{"text_id": "8380", "text": "docstring: def call_and_return_stdout(args: tp.Union[str, tp.List[str]], timeout: tp.Optional[tp.Union[str, int]] = None, encoding: tp.Optional[str] = None, expected_return_code: tp.Optional[int] = None, **kwargs) -> tp.Union[bytes, str]: warnings.warn('This is deprecated, use subprocess.check_output instead', DeprecationWarning) kwargs['stdout'] = subprocess.PIPE stdout_list = [] proc = subprocess.Popen(args, **kwargs) fut = read_nowait(proc, stdout_list) if timeout is not None: timeout = parse_time_string(timeout) try: proc.wait(timeout=timeout) except subprocess.TimeoutExpired: proc.kill() proc.wait() raise TimeoutError('Process did not complete within %s seconds' % (timeout,)) finally: fut.result() if encoding is None: result = b''.join(stdout_list) else: result = ''.join((row.decode(encoding) for row in stdout_list)) if expected_return_code is not None: if proc.returncode != expected_return_code: raise ProcessFailed(proc.returncode, result) return result"}
{"text_id": "8381", "text": "docstring: def make_display_table(structured): table = [[]] header_map = { RealBounds: lambda bnd: 'Mean({})'.format(bnd.default_units), CategoricalBounds: lambda bnd: 'Category', CompositionBounds: lambda bnd: 'Formula', type(None): lambda bnd: 'Label' } for column in structured['headers']: bounds = column.get('bounds', None) table[0].append('~'.join(column['name'] + [header_map[type(bounds)](bounds)])) i_bandgap = list(filter(lambda i: 'Band gap' in table[0][i], range(len(table[0])))) assert i_bandgap, \"Band gap was not found\" i_bandgap = i_bandgap[0] column = structured['headers'][i_bandgap] table[0].insert( i_bandgap + 1, '~'.join(column['name'] + ['Std Deviation({})'.format(column['bounds'].default_units)]) ) content_map = { NominalReal: lambda x: x.nominal, NormalReal: lambda x: x.mean, UniformReal: lambda x: 0.5 * (x.lower_bound + x.upper_bound), NominalCategorical: lambda x: x.category, EmpiricalFormula: lambda x: x.formula, str: lambda x: x, type(None): lambda x: '' } uncert_map = { NominalReal: lambda x: '', NormalReal: lambda x: x.std, UniformReal: lambda x: 0.29 * (x.upper_bound - x.lower_bound), type(None): lambda x: '' } for row in structured['content']: table.append([]) for element in row: table[-1].append(content_map[type(element)](element)) table[-1].insert(i_bandgap + 1, uncert_map[type(row[i_bandgap])](row[i_bandgap])) return table"}
{"text_id": "8382", "text": "docstring: def prepare_browser(self, task): if self.devtools is not None: if not self.options.android: size = self.devtools.execute_js(\"[window.innerWidth, window.innerHeight]\") if size is not None and len(size) == 2: task['actual_viewport'] = {\"width\": size[0], \"height\": size[1]} if self.device_pixel_ratio is None: self.device_pixel_ratio = 1.0 try: ratio = self.devtools.execute_js('window.devicePixelRatio') if ratio is not None: self.device_pixel_ratio = max(1.0, float(ratio)) except Exception: pass if not task['cached']: self.devtools.send_command(\"Network.clearBrowserCache\", {}, wait=True) self.devtools.send_command(\"Network.clearBrowserCookies\", {}, wait=True) if not self.options.android and \\ 'mobile' in self.job and self.job['mobile'] and \\ 'width' in self.job and 'height' in self.job and \\ 'dpr' in self.job: width = int(re.search(r'\\d+', str(self.job['width'])).group()) height = int(re.search(r'\\d+', str(self.job['height'])).group()) self.devtools.send_command(\"Emulation.setDeviceMetricsOverride\", {\"width\": width, \"height\": height, \"screenWidth\": width, \"screenHeight\": height, \"scale\": 1, \"positionX\": 0, \"positionY\": 0, \"deviceScaleFactor\": float(self.job['dpr']), \"mobile\": True, \"fitWindow\": False, \"screenOrientation\": {\"angle\":0, \"type\":\"portraitPrimary\"}}, wait=True) self.devtools.send_command(\"Emulation.setTouchEmulationEnabled\", {\"enabled\": True, \"configuration\": \"mobile\"}, wait=True) if not self.options.throttle and 'throttle_cpu' in self.job: logging.debug('CPU Throttle target: %0.3fx', self.job['throttle_cpu']) if self.job['throttle_cpu'] > 1: self.devtools.send_command(\"Emulation.setCPUThrottlingRate\", {\"rate\": self.job['throttle_cpu']}, wait=True) ua_string = self.devtools.execute_js(\"navigator.userAgent\") if ua_string is not None: match = re.search(r'Chrome\\/(\\d+\\.\\d+\\.\\d+\\.\\d+)', ua_string) if match: self.browser_version = match.group(1) if 'uastring' in self.job: ua_string = self.job['uastring'] if ua_string is not None and 'AppendUA' in task: ua_string += ' ' + task['AppendUA'] if ua_string is not None: self.job['user_agent_string'] = ua_string if self.job['noscript']: self.devtools.send_command(\"Emulation.setScriptExecutionDisabled\", {\"value\": True}, wait=True) self.devtools.prepare_browser()"}
{"text_id": "8383", "text": "docstring: def collect_released(self) -> ResourceSet: result = None if self.released is not None: result = ResourceSet(concrete=self.released, rtype=self.type) self.released = None return result"}
{"text_id": "8384", "text": "docstring: def repeated_elements(arr)->dict: dic={} for i in arr: try: if dic[i]: dic[i]+=1 except: dic[i]=1 return dic"}
{"text_id": "8385", "text": "docstring: def testProcessResponseValid(self): request_data = self.get_request() message = self.file_contents(join(self.data_path, 'responses', 'valid_response.xml.base64')) del request_data['get_data'] request_data['post_data'] = { 'SAMLResponse': message } auth = OneLogin_Saml2_Auth(request_data, old_settings=self.loadSettingsJSON()) auth.process_response() self.assertTrue(auth.is_authenticated()) self.assertEqual(len(auth.get_errors()), 0) self.assertEqual('492882615acf31c8096b627245d76ae53036c090', auth.get_nameid()) attributes = auth.get_attributes() self.assertNotEqual(len(attributes), 0) self.assertEqual(auth.get_attribute('mail'), attributes['mail']) session_index = auth.get_session_index() self.assertEqual('_6273d77b8cde0c333ec79d22a9fa0003b9fe2d75cb', session_index)"}
{"text_id": "8386", "text": "docstring: def __hide_from_set(self, hiders): for hider in make_iter(hiders): if not hider: continue if not hasattr(hider, \"__dbclass__\"): raise ValueError(\"This is a not a typeclassed object!\") clsname = hider.__dbclass__.__name__ if clsname == \"PlayerDB\": self.db_hide_from_players.add(hider.__dbclass__) elif clsname == \"ObjectDB\": self.db_hide_from_objects.add(hider.__dbclass__) elif clsname == \"ChannelDB\": self.db_hide_from_channels.add(hider.__dbclass__)"}
{"text_id": "8387", "text": "docstring: def half_exp_density(data, sd): res = 0.1 data = np.array(data) dmax = np.max(data)+sd*4. dmin = np.min(data)-sd*4. dstep = sd*res time = np.arange(start=dmin, stop=dmax, step=dstep) if time.size %2 !=0: time = time[:-1] r = np.arange(0, len(time), dtype=int) hal = r.size/2 exp = np.zeros(r.size, dtype = 'float') exp[hal:] = 2/np.sqrt(2*sd**2)*np.exp(-np.sqrt(2)*np.abs(time[hal:])/sd) time, dens = kernel_density(data, np.vstack((time, exp))) return np.vstack((time, dens))"}
{"text_id": "8388", "text": "docstring: def delete( self, resource_group_name, server_name, database_name, vulnerability_assessment_name, rule_id, baseline_name, **kwargs ): cls = kwargs.pop('cls', None) error_map = { 401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError } error_map.update(kwargs.pop('error_map', {})) api_version = \"2017-03-01-preview\" url = self.delete.metadata['url'] path_format_arguments = { 'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'), 'serverName': self._serialize.url(\"server_name\", server_name, 'str'), 'databaseName': self._serialize.url(\"database_name\", database_name, 'str'), 'vulnerabilityAssessmentName': self._serialize.url(\"vulnerability_assessment_name\", vulnerability_assessment_name, 'str'), 'ruleId': self._serialize.url(\"rule_id\", rule_id, 'str'), 'baselineName': self._serialize.url(\"baseline_name\", baseline_name, 'str'), 'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'), } url = self._client.format_url(url, **path_format_arguments) query_parameters = {} query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str') header_parameters = {} request = self._client.delete(url, query_parameters, header_parameters) pipeline_response = self._client._pipeline.run(request, stream=False, **kwargs) response = pipeline_response.http_response if response.status_code not in [200]: map_error(status_code=response.status_code, response=response, error_map=error_map) raise HttpResponseError(response=response, error_format=ARMErrorFormat) if cls: return cls(pipeline_response, None, {})"}
{"text_id": "8389", "text": "docstring: def size_readout(self): val_w, val_h = self.getprops(['Width','Height']).values() return (val_w, val_h)"}
{"text_id": "8390", "text": "docstring: def media_content_type(self) -> str | None: if self._client.current_app_id == LIVE_TV_APP_ID: return MEDIA_TYPE_CHANNEL return None"}
{"text_id": "8391", "text": "docstring: def fit(self, data, epochs=1, batch_size=32, verbose=1, callbacks=None, validation_data=None, class_weight=None, steps_per_epoch=None, validation_steps=None, validation_freq=1, data_config=None, feature_cols=None, label_cols=None): params = dict( epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=callbacks, class_weight=class_weight, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, validation_freq=validation_freq, data_config=data_config ) from bigdl.orca.data import SparkXShards from bigdl.orca.data.tf.data import Dataset from bigdl.orca.data.tf.tf2_data import TF2Dataset data, validation_data = maybe_dataframe_to_xshards(data, validation_data, feature_cols, label_cols, mode=\"fit\", num_workers=self.num_workers, accept_str_col=True) if isinstance(data, SparkXShards): if data._get_class_name() == 'pandas.core.frame.DataFrame': data, validation_data = process_xshards_of_pandas_dataframe(data, feature_cols, label_cols, validation_data, \"fit\") ray_xshards = process_spark_xshards(data, self.num_workers) val_ray_xshards = None if validation_data is not None: val_ray_xshards = process_spark_xshards(validation_data, self.num_workers) worker_stats = self._fit_ray_xshards(ray_xshards, val_ray_xshards, params) elif isinstance(data, Dataset): ray_xshards = TF2Dataset(data).get_ray_xshards(self.num_workers) val_ray_xshards = None if validation_data is not None: invalidInputError(isinstance(validation_data, Dataset), \"Validation data type should be the same as train data,\" \" but got type: {}\".format(type(validation_data))) val_ray_xshards = TF2Dataset(validation_data).get_ray_xshards(self.num_workers) worker_stats = self._fit_ray_xshards(ray_xshards, val_ray_xshards, params) elif isinstance(data, ray.data.Dataset): shards = data.split(n=self.num_workers, locality_hints=self.remote_workers) remote_worker_stats = [] if validation_data is None: for shard, worker in zip(shards, self.remote_workers): params[\"data_creator\"] = self.process_ray_dataset(shard, label_cols, feature_cols, data_config) remote_worker_stats.append(worker.step.remote(**params)) worker_stats = ray.get(remote_worker_stats) worker_stats = list(itertools.chain.from_iterable(worker_stats)) else: invalidInputError(isinstance(validation_data, ray.data.Dataset), \"Validation data type should be the same as train data,\" \" but got type: {}\".format(type(validation_data))) val_shards = validation_data.split(n=self.num_workers, locality_hints=self.remote_workers) for i in range(self.num_workers): params[\"data_creator\"] = self.process_ray_dataset(shards[i], label_cols, feature_cols, data_config) params[\"validation_data_creator\"] = self.process_ray_dataset(val_shards[i], label_cols, feature_cols, data_config) remote_worker_stats.append(self.remote_workers[i].step.remote(**params)) worker_stats = ray.get(remote_worker_stats) worker_stats = list(itertools.chain.from_iterable(worker_stats)) else: params[\"data_creator\"] = data params[\"validation_data_creator\"] = validation_data params_list = [params] * self.num_workers worker_stats = ray.get([self.remote_workers[i].step.remote(**params_list[i]) for i in range(self.num_workers)]) worker_stats = list(itertools.chain.from_iterable(worker_stats)) stats = worker_stats[0].copy() return stats"}
{"text_id": "8392", "text": "docstring: def __sort(self): self._interval_list = sorted(self._interval_list, key = lambda x: x[0]) return self._interval_list"}
{"text_id": "8393", "text": "docstring: def random_digraph(V, density=0.5): V = _parsed_domain(V) G = {v: set() for v in V} for v, w in product(V, repeat=2): r = random.uniform(0, 1) if r < density: G[v].add(w) elif r == density: if random.choice({0, 1}): G[v].add(w) return sage.DiGraph(G)"}
{"text_id": "8394", "text": "docstring: def __write_disp(self, path, macros): file = \"{}app_id.json\".format(path) template = \"{}scripts/{}\".format(self.template_path, \"app_id_disp.template\") self.__write_file_from_template(file=file, template=template, macros=macros)"}
{"text_id": "8395", "text": "docstring: def populateMissingBootloader(content): if not exists(content, \"BOOTLOADER_EFI\"): content[\"BOOTLOADER_EFI\"] = config.BOOTLOADER_EFI if not exists(content, \"BOOTLOADER_DOS\"): content[\"BOOTLOADER_DOS\"] = config.BOOTLOADER_DOS if not exists(content, \"BOOTLOADER_CONFIG\"): content[\"BOOTLOADER_CONFIG\"] = config.BOOTLOADER_CONFIG return content"}
{"text_id": "8396", "text": "docstring: def login_required(func): @wraps(func) def decorated_view(*args, **kwargs): if current_app.login_manager._login_disabled: return func(*args, **kwargs) elif not current_user.is_authenticated and not current_user.is_active: return current_app.login_manager.unauthorized() return func(*args, **kwargs) return decorated_view"}
{"text_id": "8397", "text": "docstring: def select_for_breeding(self): random = self.random_state if self.breeding_selection == 'random': self.selected_ = random.choice(self.population_, size=self.n_selected_) elif self.breeding_selection == 'tournament': self.selected_ = [max(random.choice(self.population_, size=self.tournament_size_), key=lambda i: self.agent.utility(i)) for _ in range(self.n_selected_)] elif self.breeding_selection == 'roulette': p = np.array([self.agent.utility(i) for i in self.population_], dtype=float) p = np.exp(p) p /= p.sum() self.selected_ = random.choice(self.population_, size=self.n_selected_, p=p) elif self.breeding_selection == 'gattaca': self.population_.sort(key=lambda i: -self.agent.utility(i)) self.selected_ = self.population_[:self.n_selected_] return self"}
{"text_id": "8398", "text": "docstring: def add_field_to_embed(embed: discord.Embed, name: str, value: str) -> None: embed.add_field(name = name, value = value, inline = False)"}
{"text_id": "8399", "text": "docstring: def init_step_mhe(self, patch_pred_y=False, **kwargs): tgt = self.dum_mhe src = self.lsmhe fe_src = kwargs.pop(\"fe\", self.nfe_tmhe - 1) load_iguess(src, tgt, fe_src, 0) for u in self.u: usrc = getattr(src, u) utgt = getattr(tgt, u) utgt[0].value = (value(usrc[fe_src])) t_ncp = t_ij(self.lsmhe.t, fe_src, self.ncp_tmhe) for x in self.states: pn = x + \"_ic\" p = getattr(tgt, pn) vs = getattr(self.lsmhe, x) for ks in p.keys(): p[ks].value = value(vs[(t_ncp,) + (ks,)]) self.lsmhe.display(filename='lsmhe') self.dum_mhe.display(filename='dum') test = self.solve_dyn(tgt, o_tee=True, stop_if_nopt=False, max_cpu_time=300, jacobian_regularization_value=1e-04, jacobian_regularization_exponent=2., halt_on_ampl_error=False, output_file=\"init_mhe.txt\") if test != 0: self.journalist(\"I\", self._iteration_count, \"init_step_mhe\", \"Failed prediction for next step\") load_iguess(tgt, src, 0, fe_src) if patch_pred_y: self.journalist(\"I\", self._iteration_count, \"init_step_mhe\", \"Prediction for advanced-step.. Ready\") self.patch_meas_mhe(tgt, noisy=True) self.adjust_nu0_mhe()"}
{"text_id": "8400", "text": "docstring: def to_dict(self): result = {} for attr, _ in six.iteritems(self.swagger_types): value = getattr(self, attr) if isinstance(value, list): result[attr] = list(map( lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x, value )) elif hasattr(value, \"to_dict\"): result[attr] = value.to_dict() elif isinstance(value, dict): result[attr] = dict(map( lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], \"to_dict\") else item, value.items() )) else: result[attr] = value if issubclass(AdminRecordSettings, dict): for key, value in self.items(): result[key] = value return result"}
{"text_id": "8401", "text": "docstring: def load_models_csv(self,filepath, model_no = None): try: if model_no == None: model_no = len(self.sub_models) self.sub_models.append(data_io.load_flatfile_to_df(filepath, delimiter='')) else: self.sub_models[model_no]=data_io.load_flatfile_to_df(filepath, delimiter='') utils.info('Model loaded into index %s' % str(model_no)) except IndexError: raise Exception('Model number does not exist. Model number given, %s, is out of index range.' % str(model_no))"}
{"text_id": "8402", "text": "docstring: def _check_job_failures(db, process_at_work): jobs_failed = db.jobs.find({\"status\": \"failed\", \"node\": conf.node_name}) _update_working_process_and_job_status( db, jobs_failed, process_at_work, \"completed-failed\")"}
{"text_id": "8403", "text": "docstring: def load_payer(self, payer): return yaml.safe_load(Utility.read_file(self.resolve_payer(payer)))"}
{"text_id": "8404", "text": "docstring: def permissions(self, permissions): if permissions is None: raise ValueError(\"Invalid value for `permissions`, must not be `None`\") self._permissions = permissions"}
{"text_id": "8405", "text": "docstring: def format_help(self, ctx, formatter): width = docops.DEFAULT_COLUMNS terminal_width = shutil.get_terminal_size().columns width = min(width, terminal_width) short_help = self.get_short_help_str(width) self.format_usage(ctx, formatter) formatter.write(f\"\\n {short_help}\\n\") if self.help: self._add_description(ctx, formatter) self.format_options(ctx, formatter) self.format_epilog(ctx, formatter)"}
{"text_id": "8406", "text": "docstring: def assertTemplateUsed(self, response, template_name, msg_prefix=''): if msg_prefix: msg_prefix += \": \" template_names = [t.name for t in response.templates] if not template_names: self.fail(msg_prefix + \"No templates used to render the response\") self.assertTrue(template_name in template_names, msg_prefix + \"Template '%s' was not a template used to render\" \" the response. Actual template(s) used: %s\" % (template_name, u', '.join(template_names)))"}
{"text_id": "8407", "text": "docstring: def forward(ctx, k): k = k.cpu() ctx.save_for_backward(k) k = k.double() answer = (m/2-1)*torch.log(k) - torch.log(scipy.special.ive(m/2-1, k)) - k - (m/2)*np.log(2*np.pi) if torch.cuda.is_available(): answer = answer.cuda() answer = answer.float() return answer"}
{"text_id": "8408", "text": "docstring: def binary_metrics(run_dir,binary_data_dir): model = utils.read_model(run_dir,False)[0] f = h5py.File(binary_data_dir,'r') x_test = f['x_test'] y_test = f['y_test'] y_pred = utils.predict_np(x_test,model) if len(y_pred.shape) == 3: cov_pred = np.sum(y_pred,axis = 1) else: cov_pred = y_pred aupr = [] auroc = [] for a in range(0,y_test.shape[1]): precision,recall,threshold = sklearn.metrics.precision_recall_curve(y_test[:,a],cov_pred[:,a]) fpr,tpr,threshold = sklearn.metrics.roc_curve(y_test[:,a],cov_pred[:,a]) aupr.append(sklearn.metrics.auc(recall,precision)) auroc.append(sklearn.metrics.auc(fpr,tpr)) f.close() return np.mean(aupr),np.mean(auroc)"}
{"text_id": "8409", "text": "docstring: def GetEffectiveComponentSize(self, component_id, platform_filter=None): size = 0 component = self.ComponentFromId(component_id) if component and component.platform.Matches(platform_filter): if component.data: return component.data.size deps = [self.ComponentFromId(d) for d in self.__dependencies[component_id]] deps = [d for d in deps if d.platform.Matches(platform_filter) and d.is_hidden and d.data] for d in deps: size += d.data.size return size"}
{"text_id": "8410", "text": "docstring: async def begin_update_mongo_db_collection_throughput( self, resource_group_name: str, account_name: str, database_name: str, collection_name: str, update_throughput_parameters: \"_models.ThroughputSettingsUpdateParameters\", **kwargs: Any ) -> AsyncLROPoller[\"_models.ThroughputSettingsGetResults\"]: api_version = kwargs.pop('api_version', \"2022-02-15-preview\") content_type = kwargs.pop('content_type', \"application/json\") polling = kwargs.pop('polling', True) cls = kwargs.pop('cls', None) lro_delay = kwargs.pop( 'polling_interval', self._config.polling_interval ) cont_token = kwargs.pop('continuation_token', None) if cont_token is None: raw_result = await self._update_mongo_db_collection_throughput_initial( resource_group_name=resource_group_name, account_name=account_name, database_name=database_name, collection_name=collection_name, update_throughput_parameters=update_throughput_parameters, api_version=api_version, content_type=content_type, cls=lambda x,y,z: x, **kwargs ) kwargs.pop('error_map', None) def get_long_running_output(pipeline_response): response = pipeline_response.http_response deserialized = self._deserialize('ThroughputSettingsGetResults', pipeline_response) if cls: return cls(pipeline_response, deserialized, {}) return deserialized if polling is True: polling_method = AsyncARMPolling(lro_delay, **kwargs) elif polling is False: polling_method = AsyncNoPolling() else: polling_method = polling if cont_token: return AsyncLROPoller.from_continuation_token( polling_method=polling_method, continuation_token=cont_token, client=self._client, deserialization_callback=get_long_running_output ) return AsyncLROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"text_id": "8411", "text": "docstring: def _get_extensions(self, params): for k, v in params: if k == SCTP_SUPPORTED_CHUNK_EXT: self._remote_extensions = list(v)"}
{"text_id": "8412", "text": "docstring: def create(self, renderer, filename): return sdl2.sdlimage.IMG_LoadTexture(renderer, filename)"}
{"text_id": "8413", "text": "docstring: def update_refund(self, refund_id, refund_update_request, **kwargs): \"\"\"Update refund :param str refund_id: Refund ID (required) :param RefundUpdateRequest refund_update_request: refundUpdateRequest (required) :return: RefundUpdateResponse If the method is called asynchronously, returns the request thread. \"\"\" kwargs[\"_return_http_data_only\"] = True (data) = self.update_refund_with_http_info( refund_id, refund_update_request, **kwargs ) return data"}
{"text_id": "8414", "text": "docstring: def decohere_coin(self, step_idx) -> None: if self.coin_decoherence_cycle is None: return if (step_idx+1)%self.coin_decoherence_cycle.cycle_length == 0: for target_idx in self.coin_decoherence_cycle.target_qubits: self.quantum_circuit.reset(self.shift_coin_register[target_idx]) self.quantum_circuit.u(pi/2,pi/2,3*pi/2,self.shift_coin_register[target_idx])"}
{"text_id": "8415", "text": "docstring: def aggregate_values_over_time(self, metric_store, data, groupby_name, column_name, aggregate_timestamp): if self.groupby: metric_data = reduce(defaultdict.__getitem__, [column_name, 'Overall_summary', aggregate_timestamp], metric_store) metric_data.append(float(data)) metric_data = reduce(defaultdict.__getitem__, [column_name, groupby_name, aggregate_timestamp], metric_store) metric_data.append(float(data)) return None"}
{"text_id": "8416", "text": "docstring: def psi_ft(self, k, l): K, L = meshgrid(k, l) return (K ** 2. + L ** 2.) * exp(-0.5 * (K ** 2. + L ** 2.))"}
{"text_id": "8417", "text": "docstring: def allowed_actions(self): actions_allowed = [] (y, x) = self.state if self.action_trans_conf is 'standard': if (y > 0): actions_allowed.append(self.action_dict[\"up\"]) if (y < self.state_grid_dim[1] - 1): actions_allowed.append(self.action_dict[\"down\"]) if (x > 0): actions_allowed.append(self.action_dict[\"left\"]) if (x < self.state_grid_dim[0] - 1): actions_allowed.append(self.action_dict[\"right\"]) elif self.action_trans_conf is 'reward': if self.R[y,x,self.action_dict[\"up\"]] != -1: actions_allowed.append(self.action_dict[\"up\"]) if self.R[y,x,self.action_dict[\"down\"]] != -1: actions_allowed.append(self.action_dict[\"down\"]) if self.R[y,x,self.action_dict[\"left\"]] != -1: actions_allowed.append(self.action_dict[\"left\"]) if self.R[y,x,self.action_dict[\"right\"]] != -1: actions_allowed.append(self.action_dict[\"right\"]) actions_allowed = np.array(actions_allowed, dtype=int) return actions_allowed"}
{"text_id": "8418", "text": "docstring: def write_tensor(fp, name, x): fp.write(\"name,%s\\n\" % name) fp.write(\"dtype,%s\\n\" % x.dtype) fp.write(\"shape,\" + \",\".join(map(str, x.shape)) + \"\\n\") fp.write(\"values,\" + format_result(x) + \"\\n\")"}
{"text_id": "8419", "text": "docstring: def on_adapt_reject(self, proposal): self.rejected_count += 1 self.pending_evals -= 1 xx = np.copy(proposal.args[0]) self.remove_pending(xx) if not self.asynchronous: self.batch_queue.append(xx)"}
{"text_id": "8420", "text": "docstring: def step04d_txt_lcurve(self): lcurve_mp = os.path.join(datadir, 'lcurve_lc' + MP_FILE_EXTENSION) lcdata_mp = mp.io.load_lcurve(lcurve_mp) lc_mp = lcdata_mp['lc'] time_mp = lcdata_mp['time'] lcurve_txt_orig = os.path.join(datadir, 'lcurve_txt_lc.txt') mp.io.save_as_ascii([time_mp, lc_mp], lcurve_txt_orig) lcurve_txt = os.path.join(datadir, 'lcurve_txt_lc' + MP_FILE_EXTENSION) mp.lcurve.main(['--txt-input', lcurve_txt_orig, '--outfile', lcurve_txt]) lcdata_txt = mp.io.load_lcurve(lcurve_txt) lc_txt = lcdata_txt['lc'] assert np.all(np.abs(lc_mp - lc_txt) <= 1e-3), \\ 'Light curve data do not coincide between txt and MP'"}
{"text_id": "8421", "text": "docstring: def GenesInCluster(cluster_id, sp, of_name): cluster_genes = set() cfname = \"Annotation/AnnotationFiles/\" + sp + \"_clusters\" with open(cfname, \"r\") as cfin: for line in cfin: line = line.strip().split() if line[2] == cluster_id: cluster_genes.add(line[0]) return list(cluster_genes)"}
{"text_id": "8422", "text": "docstring: def to_one_line(self, array): return np.atleast_2d(array.flatten(order='F'))"}
{"text_id": "8423", "text": "docstring: def bbox(self): return [ self._centers.min(axis=0), self._centers.max(axis=0) ]"}
{"text_id": "8424", "text": "docstring: def initialize(self, resolver): proxy = resolver.resolve_proxy(self) proxy.activate() self.observe(\"visibility\", update_proxy)"}
{"text_id": "8425", "text": "docstring: def wait_for_provisioning(self, nodes, timeout=None): nodes = [self._find_node_and_allocation(n)[0] for n in nodes] try: nodes = self.connection.baremetal.wait_for_nodes_provision_state( nodes, 'active', timeout=timeout) except os_exc.ResourceTimeout as exc: raise exceptions.DeploymentTimeout(str(exc)) except os_exc.SDKException as exc: raise exceptions.DeploymentFailed(str(exc)) return [self._get_instance(node) for node in nodes]"}
{"text_id": "8426", "text": "docstring: def event(self, id): return Event(self, id)"}
{"text_id": "8427", "text": "docstring: def on_suspend(self): raise NotImplementedError"}
{"text_id": "8428", "text": "docstring: def update(self, source: 'Document', exclude_fields: Optional[Tuple[str, ...]] = None, include_fields: Optional[Tuple[str, ...]] = None) -> None: if (include_fields and not isinstance(include_fields, tuple)) or ( exclude_fields and not isinstance(exclude_fields, tuple)): raise TypeError('include_fields and exclude_fields must be tuple of str') if exclude_fields is None: if include_fields: exclude_fields = tuple(f for f in self.non_empty_fields if f not in include_fields) else: exclude_fields = self.non_empty_fields if include_fields and exclude_fields: _intersect = set(include_fields).intersection(exclude_fields) if _intersect: raise ValueError(f'{_intersect} is in both `include_fields` and `exclude_fields`') self._update(source, self, exclude_fields=exclude_fields, include_fields=include_fields, replace_message_field=True, replace_repeated_field=True)"}
{"text_id": "8429", "text": "docstring: def _process_povm(self, povm, povm_label, file_handle): qubit_indices = None if povm_label.sslbls is not None: flat_sslbls = [lbl for i in range(self.model.state_space.num_tensor_product_blocks) for lbl in self.model.state_space.tensor_product_block_labels(i)] qubit_indices = [flat_sslbls.index(q) for q in povm_label.sslbls] def process_computational_povm(povm, qubit_indices, target_offset=0): assert isinstance(povm, _povm.ComputationalBasisPOVM), \\ \"CHP povm must be ComputationalPOVM (may be inside ComposedPOVM/TensorProdPOVM)\" povm_qubits = _np.array(range(povm.nqubits)) + target_offset for target in povm_qubits: if qubit_indices is None or target in qubit_indices: file_handle.write(f'm {target}\\n') def process_composed_povm(povm, qubit_indices, target_offset=0): if isinstance(povm, _povm.ComposedPOVM): assert povm._evotype == 'chp', \\ \"ComposedPOVM must have `chp` evotype for noise op\" file_handle.write(povm.error_map.chp_str) process_computational_povm(povm.base_povm, qubit_indices, target_offset) else: process_computational_povm(povm, qubit_indices, target_offset) target_offset = 0 if isinstance(povm, _povm.TensorProductPOVM): for povm_factor in povm.factors: nqubits = povm_factor.error_map.state_space.num_qubits process_composed_povm(povm_factor, qubit_indices, target_offset) target_offset += nqubits else: process_composed_povm(povm, qubit_indices, target_offset)"}
{"text_id": "8430", "text": "docstring: def assert_entries(self, sep, *output, **kwargs): self.assertEqual( sorted(list(output) + [\"\"]), sorted((self.execute_rule(**kwargs)).split(sep)) )"}
{"text_id": "8431", "text": "docstring: def tpl_repo(request, tmp_path, tpl_path): repo_dir = tmp_path / \"template_repository\" repo_dir.mkdir() marker = request.node.get_closest_marker(\"tpl_repo_contents\") templates = [\"hello_world\", \"test\"] if marker is not None: templates = [a for a in marker.args] for tpl in templates: tpl_dir = tpl_path / tpl if tpl_dir.is_dir(): shutil.copytree(tpl_dir, repo_dir / tpl) return repo_dir"}
{"text_id": "8432", "text": "docstring: def from_predef(cls, predef): s = predef.start_type.id t = [] o = [] h = [] for param in predef_subgraph_spec_param.select( predef_subgraph_spec_param.action, predef_subgraph_spec_param.type1.alias(\"type1_id\"), predef_subgraph_spec_param.type2.alias(\"type2_id\"), ).where(predef_subgraph_spec_param.predef_subgraph_spec == predef): if param.action == \"T\": t.append(param.type1_id) elif param.action == \"O\": o.append([param.type1_id, param.type2_id]) elif param.action == \"H\": h.append(param.type1_id) else: raise RuntimeError('Unknown subgraph action type \"%s\".' % param.action) return cls(s, t, o, h)"}
{"text_id": "8433", "text": "docstring: def parse_train_params(self, print_params=False): train_config = configparser.ConfigParser() train_config.read(str(self.config_path)) train_args = train_config['train_args'] seed = int(train_args['experiment_seed']) if not isNone(train_args['experiment_seed']) else None if self.debug: train_args['experiment_time'] = train_args['debug_experiment_time'] train_args['experiment_save_agent'] = train_args['debug_experiment_save_agent'] train_args['experiment_save_agent_interval'] = train_args['debug_experiment_save_agent_interval'] train_params = TrainParams( debug=self.debug, network=train_args['network'], experiment_time=int(train_args['experiment_time']), experiment_save_agent=str2bool(train_args['experiment_save_agent']), experiment_save_agent_interval=int(train_args['experiment_save_agent_interval']), experiment_seed=seed, sumo_render=str2bool(train_args['sumo_render']), sumo_emission=str2bool(train_args['sumo_emission']), tls_type=train_args['tls_type'], demand_type=train_args['demand_type'], demand_mode=train_args['demand_mode'], ) if print_params: print(train_params) return train_params"}
{"text_id": "8434", "text": "docstring: def commit(self): new_user = (self.s.id is None) post_fields = [\"username\", \"name\", \"first_name\", \"last_name\", \"email\", \"url\", \"description\", \"locale\", \"nickname\", \"slug\", \"roles\", \"password\", \"meta\"] if new_user: post_fields.append(\"id\") parameters = dict() for field in post_fields: if getattr(self.s, field) is not None: parameters[field] = getattr(self.s, field) if new_user: required_fields = [\"username\", \"email\", \"password\"] for field in required_fields: if getattr(self.s, field) is None: raise MissingRequiredParameter(\"The '{0}' field must be provided when creating a new user.\".format(field)) response = self.api.session.post(url=self.url, params=parameters, auth=self.api.auth()) return response"}
{"text_id": "8435", "text": "docstring: def parse_args(self): if isinstance(self.fields, string_types): if self.fields == \"*\": self.fields = [\"*\"] else: try: self.fields = json.loads(self.fields) except ValueError: self.fields = [f.strip() for f in self.fields.split(\",\")] for filter_name in [\"filters\", \"or_filters\"]: filters = getattr(self, filter_name) if isinstance(filters, string_types): filters = json.loads(filters) if isinstance(filters, dict): fdict = filters filters = [] for key, value in iteritems(fdict): filters.append(make_filter_tuple(self.doctype, key, value)) setattr(self, filter_name, filters)"}
{"text_id": "8436", "text": "docstring: def notify(self, tickets, new_values, comment, action, author): t = deactivate() try: self._notify(tickets, new_values, comment, action, author) finally: reactivate(t)"}
{"text_id": "8437", "text": "docstring: def visit_AsyncFunctionDef(self, node): self.visit_FunctionDef(node)"}
{"text_id": "8438", "text": "docstring: def register_blueprints(app): app.register_blueprint(page) app.register_blueprint(contact) app.register_blueprint(user) app.register_blueprint(admin) return None"}
{"text_id": "8439", "text": "docstring: def statistics(self, reservation_prices=None, exclude=[]): stats = OrderedDict() extras = OrderedDict() if 'fees' in self.extra: extras['fees'] = self.extra['fees'] extras['reservation_prices'] = reservation_prices for stat in STATS: if stat not in exclude: stats[stat] = STATS[stat]( self.bm.get_df(), self.transactions.get_df(), **extras ) self.stats = stats return stats"}
{"text_id": "8440", "text": "docstring: def list_group_s_users(request_ctx, group_id, include, search_term=None, per_page=None, **request_kwargs): if per_page is None: per_page = request_ctx.per_page include_types = ('avatar_url') utils.validate_attr_is_acceptable(include, include_types) path = '/v1/groups/{group_id}/users' payload = { 'search_term' : search_term, 'include' : include, 'per_page' : per_page, } url = request_ctx.base_api_url + path.format(group_id=group_id) response = client.get(request_ctx, url, payload=payload, **request_kwargs) return response"}
{"text_id": "8441", "text": "docstring: def model(data, ix_to_char, char_to_ix, num_iterations = 200000, n_a = 50, dino_names = 7, vocab_size = 27): n_x, n_y = vocab_size, vocab_size parameters = initialize_parameters(n_a, n_x, n_y) loss = get_initial_loss(vocab_size, dino_names) with open(\"Name.txt\") as f: examples = f.readlines() examples = [x.lower().strip() for x in examples] np.random.seed(0) np.random.shuffle(examples) a_prev = np.zeros((n_a, 1)) for j in range(num_iterations): index = j % len(examples) X = [None] + [char_to_ix[ch] for ch in examples[index]] Y = X[1:] + [char_to_ix[\"\\n\"]] curr_loss, gradients, a_prev = optimize(X,Y,a_prev, parameters, 0.01) loss = smooth(loss, curr_loss) if j % 2000 == 0: print('Iteration: %d, Loss: %f' % (j, loss) + '\\n') seed = 0 for name in range(dino_names): sampled_indices = sample(parameters, char_to_ix, seed) print_sample(sampled_indices, ix_to_char) seed += 1 print('\\n') return parameters"}
{"text_id": "8442", "text": "docstring: def add(self, dll_directory): if dll_directory in self.dll_items: logger.warning('%s already added. Skipping.', dll_directory) return self.dll_items[dll_directory] = DllItem(dll_directory)"}
{"text_id": "8443", "text": "docstring: def _is_appendix_amend(al): return (not isinstance(al, DesignateAmendment) and Node.INTERP_MARK not in al.label and len(al.label) > 1 and not al.label[1].isdigit())"}
{"text_id": "8444", "text": "docstring: async def load(event): shortname = event.pattern_match.group(1) try: try: remove_plugin(shortname) except BaseException: pass load_module(shortname) await edit_delete(event, f\"`Successfully loaded {shortname}`\", 10) except Exception as e: await edit_or_reply( event, f\"Could not load {shortname} because of the following error.\\n{str(e)}\", )"}
{"text_id": "8445", "text": "docstring: def add_edges_from_pairs(self, edges, weight=1.0): weight = float(weight) is_weighted = self.nkG.isWeighted() hasEdge = self.nkG.hasEdge addEdge = self.nkG.addEdge setWeight = self.nkG.setWeight knodes = self.add_nodes_from(chain.from_iterable(edges)) a generator Consumes two knodes at a time: notice the call to next(knodes) for ksource in knodes: ktarget = next(knodes) if hasEdge(ksource, ktarget): if is_weighted: setWeight(ksource, ktarget, weight) else: addEdge(ksource, ktarget, weight)"}
{"text_id": "8446", "text": "docstring: def power2_top_3d(self,r,az,el,p): hw = p[3]/2.0 w2 = p[3]**2 return np.sqrt( (2+w2) / ( (r/(p[0]+hw))**(2*p[2]) + w2 + (r/(p[0]-hw))**(-2*p[1]) ) ) * \\ np.exp( -0.5*(el/p[4])**2 ) / r"}
{"text_id": "8447", "text": "docstring: def openGLDAS(dataset_name, bounding_box, globe, periodicity, netcdf): if netcdf == False: file_names = [] if periodicity == 'monthly': subdirs = [os.path.join(('./'+dataset_name),o) for o in os.listdir('./'+dataset_name) if os.path.isdir(os.path.join(('./'+dataset_name),o))] for path in subdirs: nc_files = (glob.glob(path+'/*.nc4')) for file in nc_files: file_names.append(file) file_names.sort() data = xr.open_mfdataset(file_names) if netcdf == True: data = xr.open_dataset(dataset_name) if globe == True: da_precip = data.Rainf_f_tavg da_temp = data.Tair_f_inst warnings.warn('Global calculations require a few hours to complete', UserWarning, stacklevel=2) else: p_ = data.sel(lat=slice(bounding_box[2], bounding_box[3]),\\ lon=slice(bounding_box[0],bounding_box[1])) da_precip = p_.Rainf_f_tavg da_temp = p_.Tair_f_inst time = pd.to_datetime(da_precip.time.values) v = time.strftime(\"%Y,%m\") days = [] for item in v: days.append(monthrange(int(item.split(',')[0]),int(item.split(',')[1]))[1]) da_precip = da_precip*86400 da_precip = da_precip.T*days da_precip = da_precip.T da_precip.attrs['units'] = 'mm' da_temp = da_temp-273.15 da_temp.attrs['units'] = 'C' return da_precip, da_temp"}
{"text_id": "8448", "text": "docstring: def relativize(self, glove): if isinstance(glove, str): glove = pd.read_csv(glove, sep=\" \", quoting=3, header=None, index_col=0) weights = np.random.normal(scale=0.6, size=(self.n, glove.shape[1])) for k in self.word2idx: if k in glove.index: weights[self[k]] = glove.loc[k].values return weights"}
{"text_id": "8449", "text": "docstring: def _list_hosts(): hfn = __get_hosts_filename() ret = odict.OrderedDict() if not os.path.isfile(hfn): return ret with salt.utils.fopen(hfn) as ifile: for line in ifile: line = line.strip() if not line: continue if line.startswith('#'): continue comps = line.split() ip = comps.pop(0) ret.setdefault(ip, []).extend(comps) return ret"}
{"text_id": "8450", "text": "docstring: def add_to_finding(finding_name): from google.cloud import securitycenter from google.protobuf import field_mask_pb2 client = securitycenter.SecurityCenterClient() finding_marks_name = \"{}/securityMarks\".format(finding_name) field_mask = field_mask_pb2.FieldMask( paths=[\"marks.finding_key_a\", \"marks.finding_key_b\"] ) marks = {\"finding_key_a\": \"value_a\", \"finding_key_b\": \"value_b\"} updated_marks = client.update_security_marks( request={ \"security_marks\": {\"name\": finding_marks_name, \"marks\": marks}, \"update_mask\": field_mask, } ) return updated_marks, marks"}
{"text_id": "8451", "text": "docstring: def content(self): raw_content = self._manager.api.session.get(self.download_link).content data = BytesIO(raw_content) archive = ZipFile(data) filename = archive.filelist[0] return archive.read(filename)"}
{"text_id": "8452", "text": "docstring: def clear_sequences(self): python.clear_list(self._sequences) self.clear()"}
{"text_id": "8453", "text": "docstring: def extract_from_cga_match(match_str_cga, reg_exps): match_str = reg_exps['re_splitter'].split(match_str_cga) match_str_dol, match_str_cga = match_str[0], match_str[2] dol = int(reg_exps['re_dol'].search(match_str_dol).group(0)) if reg_exps['re_dd_d'].findall(match_str_cga): days_cga = int(reg_exps['re_dd_d'].search(match_str_cga) .group(0)[-1]) elif reg_exps['re_anon_dd_p'].findall(match_str_cga): days_cga = np.random.choice(np.arange(0, 7)) elif reg_exps['re_d_d_slash'].findall(match_str_cga): days_cga = int(reg_exps['re_d_d_slash'].search(match_str_cga) .group(0)[0]) elif reg_exps['re_d_d_dash'].findall(match_str_cga): days_cga = int(reg_exps['re_d_d_dash'].search(match_str_cga) .group(0)[0]) else: days_cga = 0 weeks_cga = int(reg_exps['re_dd'].findall(match_str_cga)[0]) days_ga = weeks_cga*7 + days_cga - dol weeks_ga_round = int(round(days_ga/7, 0)) return days_ga, weeks_ga_round"}
{"text_id": "8454", "text": "docstring: def nids(self): host_nids = [] for target_mount in self.managedtargetmount_set.all().order_by(\"-primary\"): host = target_mount.host host_nids.append(tuple(host.lnet_configuration.get_nids())) return tuple(host_nids)"}
{"text_id": "8455", "text": "docstring: def misc_balances_in_default_range_with_many_validators(spec): num_validators = spec.SLOTS_PER_EPOCH * 8 * 2 floor = spec.config.EJECTION_BALANCE + spec.EFFECTIVE_BALANCE_INCREMENT balances = [ max(spec.MAX_EFFECTIVE_BALANCE * 2 * i // num_validators, floor) for i in range(num_validators) ] rng = Random(1234) rng.shuffle(balances) return balances"}
{"text_id": "8456", "text": "docstring: def dlsvec(self, probs, counts, total_counts, freqs, intermediates=None): raise ValueError(\"LogL objective function cannot produce a LS-vector b/c terms are not necessarily positive!\")"}
{"text_id": "8457", "text": "docstring: def initdb_command(): db.drop_all() db.create_all() if LOAD_DUMMY_DATA: setup_dummy_data() print('Initialized the database.')"}
{"text_id": "8458", "text": "docstring: def _randomize_category_proto(data_proto, spec_proto): data_proto.value = random.randrange( 0, max(0, len(spec_proto.enum_values) - 1))"}
{"text_id": "8459", "text": "docstring: def _check3(a0, a1, a2): s0, s1, s2 = len(a0), len(a1), len(a2) sr = max(s0, s1, s2) if (s0 != sr and s0 != 1) or (s1 != sr and s1 != 1) or \\ (s2 != sr and s2 != 1): raise Exception(\"Incompatible argument sizes: %i, %i, and %i\" % (s0, s1, s2)) elif type(a0) is not type(a1) or type(a2) is not type(a2): raise Exception(\"Type mismatch!\") ar = a0.empty_(sr if a0.Size == Dynamic else 0) return (ar, sr)"}
{"text_id": "8460", "text": "docstring: def _mine_get(self, load, skip_verify=False): if not skip_verify: if any(key not in load for key in ('id', 'tgt', 'fun')): return {} if 'mine_get' in self.opts: if not isinstance(self.opts['mine_get'], dict): return {} perms = set() for match in self.opts['mine_get']: if re.match(match, load['id']): if isinstance(self.opts['mine_get'][match], list): perms.update(self.opts['mine_get'][match]) if not any(re.match(perm, load['fun']) for perm in perms): return {} ret = {} if not salt.utils.verify.valid_id(self.opts, load['id']): return ret match_type = load.get('expr_form', 'glob') if match_type.lower() == 'pillar': match_type = 'pillar_exact' if match_type.lower() == 'compound': match_type = 'compound_pillar_exact' checker = salt.utils.minions.CkMinions(self.opts) minions = checker.check_minions( load['tgt'], match_type, greedy=False ) for minion in minions: fdata = self.cache.fetch('minions/{0}'.format(minion), 'mine') if isinstance(fdata, dict): fdata = fdata.get(load['fun']) if fdata: ret[minion] = fdata return ret"}
{"text_id": "8461", "text": "docstring: def listdir(self, d_key): fn_root = self.root ret = [] if not d_key or d_key == self.key_delim: pass else: fn_root = self.entry_root('{0}/blank'.format(d_key)) for fn_ in os.listdir(fn_root): if not fn_.startswith('alder_table_'): continue full = os.path.join(fn_root, fn_) for entry in self._get_table_entries(full): ret.append(entry) return ret"}
{"text_id": "8462", "text": "docstring: def download_binary(url: str) -> None: print(f\"Downloading binary content: {url}\") content = fetch(url) slug_filename = lib.make_filename(url, \".pdf\", ADD_DATETIME_DEFAULT) out_path = PNG_DIR / slug_filename lib.write(out_path, content)"}
{"text_id": "8463", "text": "docstring: def reset_events(): pyhf.events.__events.clear() pyhf.events.__disabled_events.clear() yield reset_events pyhf.events.__events.clear() pyhf.events.__disabled_events.clear()"}
{"text_id": "8464", "text": "docstring: def iam_to_ecoinvent_location(self, location, contained=False): if location == \"World\": return [\"GLO\"] else: location = (self.model.upper(), location) ecoinvent_locations = [] try: searchfunc = self.geo.contained if contained else self.geo.intersects for r in searchfunc(location): if not isinstance(r, tuple): ecoinvent_locations.append(r) else: if r[0] not in (\"REMIND\", \"IMAGE\"): ecoinvent_locations.append(r[1]) if location == (\"REMIND\", \"USA\"): ecoinvent_locations = [ e for e in ecoinvent_locations if \"CA\" not in e ] if location in [(\"REMIND\", \"REF\"), (\"IMAGE\", \"RUS\")]: ecoinvent_locations = [ e for e in ecoinvent_locations if e not in [ \"RER\", 'Europe without Switzerland', 'Europe without Switzerland and France', 'RER w/o CH+DE', 'RER w/o AT+BE+CH+DE+FR+IT', 'RER w/o DE+NL+NO', 'Europe without NORDEL (NCPA)', ] ] if location != (self.model.upper(), \"World\"): ecoinvent_locations = [e for e in ecoinvent_locations if e != \"GLO\"] return ecoinvent_locations except KeyError: print(\"Can't find location {} using the geomatcher.\".format(location)) return [\"RoW\"]"}
{"text_id": "8465", "text": "docstring: def plant_system(self): raise NotImplementedError"}
{"text_id": "8466", "text": "docstring: def rerun_scan_analysis(self, x, y, key, test=False): log.debug(\"{} {} {}\".format(x, y, key)) if key == 'a': self.reprocess_scans_through_pixel(x, y, test=test) elif key == 'h': print(self.display_instructions) elif key == 'v': pass"}
{"text_id": "8467", "text": "docstring: def __get_status(self): response = self.session.post(STATUSPAGE) return {'ArmedStatus': response.json().get('Panel', {}).get('ArmedStatus', None)}"}
{"text_id": "8468", "text": "docstring: def enable_availability_zones(self, load_balancer_name, zones_to_add): params = {'LoadBalancerName' : load_balancer_name} self.build_list_params(params, zones_to_add, 'AvailabilityZones.member.%d') return self.get_list('EnableAvailabilityZonesForLoadBalancer', params, None)"}
{"text_id": "8469", "text": "docstring: def contains_duplicates(nums: List[int]) -> bool: if len(nums) <= 1: return False dictionary = {} for num in nums: if num in dictionary: return True dictionary[num] = True return False"}
{"text_id": "8470", "text": "docstring: def read_pdx_from_byte_stream(csv_byte_stream): csv_string = csv_byte_stream.decode('utf-8') csv_string_formatted = re.sub('\\n[\\n\\r]+', '\\n', csv_string) stream = io.StringIO(csv_string_formatted) df = pd.read_csv(stream) if any(df.Time.duplicated()): raise ValueError(\"One or more time points are duplicated, please collapse observations so that each\" \"time is unique!\") df = df.iloc[:, [not bool(re.match('Unnamed.*', col)) for col in df.columns]].copy() df.interpolate(method='linear', limit=None, limit_area='inside', inplace=True) first_death_idx = min(df.notnull().sum(axis=0)) df = df.iloc[0:first_death_idx, :].copy() control_response = df.iloc[:, [bool(re.match('Control.*', col)) for col in df.columns]].to_numpy() variable = df.Time.to_numpy() treatment_response = df.iloc[:, [bool(re.match('Treatment.*', col)) for col in df.columns]].to_numpy() control = ExperimentalCondition('Control', source_id='from_webapp', variable=variable, response=control_response, replicates=list(range(control_response.shape[1])), variable_treatment_start=min(variable), is_control=True) treatment = ExperimentalCondition('Treatment', source_id='from_webapp', replicates=list(range(treatment_response.shape[1])), variable=variable, response=treatment_response, variable_treatment_start=min(variable), is_control=False) experimental_condition_dict = {'Control': control, 'Treatment': treatment} cancer_model = CancerModel(name=\"from_webapp\", experimental_condition_dict=experimental_condition_dict, model_type=\"PDX\", tumour_type=\"unknown\", variable_start=min(variable), variable_treatment_start=min(variable), variable_end=max(variable)) treatment_response_experiment = TreatmentResponseExperiment(cancer_model_list=[cancer_model]) for _, cancer_model in treatment_response_experiment: cancer_model.normalize_experimental_conditions() cancer_model.fit_all_gps() cancer_model.compute_summary_statistics(fit_gp=True) return treatment_response_experiment"}
{"text_id": "8471", "text": "docstring: def PlotDataframe(df, ycols, yerr_cols, cms, hows, opt): x = df.index.to_numpy() yy = df.iloc[:, ycols].to_numpy() errorbars = df.iloc[:, yerr_cols].to_numpy() if yerr_cols else None xname = df.index.name if not opt.seqnum else 'seqnum' ynames = df.columns.to_list() ny = len(ycols) if opt.filtery: for iy in range(ny): yy[:, iy] = eval(re.sub(r'\\by\\b', 'yy[:, iy]', filtery)) ystats = None if opt.stats: ystats = ComputeStats(opt.stats, yy) if 'P' in opt.stats: PrintStats(ystats, ynames) ystats = None if opt.cumsum: assert not errorbars, 'cumsum errorbars not supported' for iy in range(ny): if True: y = yy[:, iy] if np.isnan(y).any(): non_nan_idx = np.where(~np.isnan(y)) beg_of_all_nans = non_nan_idx[0][-1] + 1 yy[0:beg_of_all_nans, iy] = np.nancumsum(y[0:beg_of_all_nans]) else: yy[:, iy] = np.cumsum(y[:]) else: yy[:, iy] = np.nancumsum(yy[:, iy]) elif opt.diff: assert not errorbars, 'diff errorbars not supported' for iy in range(ny): yy[:, iy] = np.diff(yy[:, iy], prepend=yy[0, iy]) if opt.ols_fit: icepts, betas = ComputeOlsFit(x, yy) yy_fit = np.zeros(yy.shape) for iy in range(ny): yy_fit[:, iy] = icepts[iy] + betas[iy]*x[:] ynames.append(f'fit({ynames[iy]})={FormatNum(betas[iy])}*{xname}+{FormatNum(icepts[iy])}') hows.append('l') yy = np.hstack((yy, yy_fit)) title = opt.title if not opt.title: ew = 'EW 'if opt.equal_wt else '' if opt.hgram: title = f'{int(len(df)/len(df.columns))}-bin {ew}hgram' elif opt.rgram: title = f'{len(df)}-bin {ew}rgram' else: title = 'plot' if not opt.hgram and opt.yscale != 1: title += f' yscale={opt.yscale}' if (opt.hgram or opt.rgram) and opt.xtrim: title += f' xtrim={opt.xtrim}' if opt.wts_col is not None: title += f' wts={opt.wts_col}' if opt.filtery: title += f' ({filtery})' if opt.cumsum: title += ' cumsum' if opt.diff: title += ' diff' if opt.smooth: title += f' {opt.smooth}' if opt.rgram and opt.esm: title += ' (esm)' if opt.llr: SmoothY(x, yy, opt) title += f' LLR({opt.llr})' if opt.driver in ('gnuplot', 'gnu'): ExecuteGnuplot(x, yy, xname, ynames, cms, hows, title, ystats, errorbars, opt) elif opt.driver in ('pyplot', 'plt'): ExecutePyplot(x, yy, xname, ynames, cms, hows, title, ystats, errorbars, opt) else: assert False, f'unsupported plotting driver \"{driver}\"' global global_plot_idx global_plot_idx += 1"}
{"text_id": "8472", "text": "docstring: def list_by_instance( self, resource_group_name: str, managed_instance_name: str, **kwargs ) -> AsyncIterable[\"models.ManagedInstanceAdministratorListResult\"]: cls = kwargs.pop('cls', None) error_map = { 401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError } error_map.update(kwargs.pop('error_map', {})) api_version = \"2017-03-01-preview\" accept = \"application/json\" def prepare_request(next_link=None): header_parameters = {} header_parameters['Accept'] = self._serialize.header(\"accept\", accept, 'str') if not next_link: url = self.list_by_instance.metadata['url'] path_format_arguments = { 'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'), 'managedInstanceName': self._serialize.url(\"managed_instance_name\", managed_instance_name, 'str'), 'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'), } url = self._client.format_url(url, **path_format_arguments) query_parameters = {} query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str') request = self._client.get(url, query_parameters, header_parameters) else: url = next_link query_parameters = {} request = self._client.get(url, query_parameters, header_parameters) return request async def extract_data(pipeline_response): deserialized = self._deserialize('ManagedInstanceAdministratorListResult', pipeline_response) list_of_elem = deserialized.value if cls: list_of_elem = cls(list_of_elem) return deserialized.next_link or None, AsyncList(list_of_elem) async def get_next(next_link=None): request = prepare_request(next_link) pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs) response = pipeline_response.http_response if response.status_code not in [200]: map_error(status_code=response.status_code, response=response, error_map=error_map) raise HttpResponseError(response=response, error_format=ARMErrorFormat) return pipeline_response return AsyncItemPaged( get_next, extract_data )"}
{"text_id": "8473", "text": "docstring: def dump_embedding_weights_to_binary_file(self): logging.info(\"Writing quantized embedding weights to \" + self.embedding_weights_binary_filepath) with open(self.embedding_weights_binary_filepath, 'wb') as f: f.write(struct.pack('i', self.num_features)) mults = np.ndarray(shape=(self.num_features)) for feature_id in range(self.num_features): weight_tensor_name = \"emb_l.\" + str(feature_id) + \".weight\" embeddings = self.weights[weight_tensor_name].numpy() maxAbsVal = abs(max(embeddings.max(), embeddings.min(), key=abs)) mults[feature_id] = 127.5 / maxAbsVal embeddingsScale = 1.0 / mults[feature_id] f.write(struct.pack('f', embeddingsScale)) for feature_id in range(self.num_features): weight_tensor_name = \"emb_l.\" + str(feature_id) + \".weight\" embeddings = self.weights[weight_tensor_name].numpy() if (embeddings.shape[0] != self.embedding_rows[feature_id]): raise IOError(\"Expected \" + str(self.embedding_rows[feature_id]) + \" embedding rows, but got \" + str(embeddings.shape[0]) + \" rows for feature \" + str(feature_id)) embeddingsQuantized = np.minimum(np.maximum(np.rint(np.multiply(embeddings, mults[feature_id])), -127), 127).astype('int8') del self.weights[weight_tensor_name] embeddingsQuantized.tofile(f)"}
{"text_id": "8474", "text": "docstring: def destroy_docker_container(self, func_arn): with self.docker_container_lock: status = self.get_docker_container_status(func_arn) container_name = self.get_container_name(func_arn) if status == 1: LOG.debug(\"Stopping container: %s\" % container_name) DOCKER_CLIENT.stop_container(container_name) status = self.get_docker_container_status(func_arn) if status == -1: LOG.debug(\"Removing container: %s\" % container_name) rm_docker_container(container_name, safe=True)"}
{"text_id": "8475", "text": "docstring: def format_img_channels(self, img, C): img = img[:, :, (2, 1, 0)] img = img.astype(np.float32) img[:, :, 0] -= C.img_channel_mean[0] img[:, :, 1] -= C.img_channel_mean[1] img[:, :, 2] -= C.img_channel_mean[2] img /= C.img_scaling_factor img = np.transpose(img, (2, 0, 1)) img = np.expand_dims(img, axis=0) return img"}
{"text_id": "8476", "text": "docstring: def read(self) -> int: return self.adc.read()"}
{"text_id": "8477", "text": "docstring: def __verify_load(self, load, verify_keys): if any(key not in load for key in verify_keys): return False if 'tok' not in load: log.error( 'Received incomplete call from {0} for \\'{1}\\', missing \\'{2}\\'' .format( load['id'], inspect_stack()['co_name'], 'tok' )) return False if not self.__verify_minion(load['id'], load['tok']): log.warning( 'Minion id {0} is not who it says it is!'.format( load['id'] ) ) return False if 'tok' in load: load.pop('tok') return load"}
{"text_id": "8478", "text": "docstring: def load_coord_var(prob_data_type): fpath = \"{}/source_others/a1b_tas_jja_EAW_1961-1990.dat\".format(BASEDIR) with open(fpath, 'rb') as reader: data = cPickle.load(reader) key = prob_data_map[prob_data_type] if key == 'prob': return np.array((data[key] * 100), np.float) else: return np.array(data[key], np.int32)"}
{"text_id": "8479", "text": "docstring: def _ids_to_words(ids, dictionary): if not dictionary.id2token: setattr(dictionary, 'id2token', {v: k for k, v in dictionary.token2id.items()}) top_words = set() for word_id in ids: word = dictionary.id2token[word_id] if isinstance(word, set): top_words = top_words.union(word) else: top_words.add(word) return top_words"}
{"text_id": "8480", "text": "docstring: def unique_elements(input_file): with open(input_file, mode=\"r\") as file: for line in file: u_list = sorted(list(set([int(x) for x in line.split(',')]))) print(*u_list, sep=\",\")"}
{"text_id": "8481", "text": "docstring: def process_response(self, request, response): connection.get_threadlocal().reset() return response"}
{"text_id": "8482", "text": "docstring: def add_to_oemof_model(self, busses, model): air_source_heat_pump = solph.Transformer( label=self.name, inputs={busses[self.bus_el]: solph.Flow(variable_costs=0)}, outputs={busses[self.bus_th]: solph.Flow( nominal_value=self.power_max, variable_costs=0)}, conversion_factors={busses[self.bus_th]: self.cops[self.sim_params.i_interval]} ) model.add(air_source_heat_pump) return air_source_heat_pump"}
{"text_id": "8483", "text": "docstring: def predict_proba(self, X): self.check_is_fitted() avg = np.average(self._collect_probas(X), axis=0) return avg"}
{"text_id": "8484", "text": "docstring: def fit_data(self, data, center, nth_line=0, path=None, c=(1.0,0.0,0.0,1.0), n_profiles=0): matplotlib.rc('font', **{'size' : 12},) matplotlib.rcParams['font.sans-serif'] = \"Helvetica\" x = np.linspace(0, data.shape[0]-1, data.shape[0]) x_aligned = x-int(center) for name,func in fit_functions.items(): fig = plt.figure() ax1 = fig.add_axes((0.1, 0.2, 0.8, 0.7)) ax1.plot(x_aligned, data / data.max(), c=c, label=\"averaged line profile\") optim, loss = fit_data_to(func, x, data, expansion=self.expansion, chi_squared=True) if self.service: self.service.send((optim[1],optim[0])) txt = name + \"fit parameters: \\n\" + f\"Number of profiles: {n_profiles} \\n\" for i,parameter in enumerate(func.fit_parameters): txt += parameter + f\"{np.abs(optim[i]):.2f}\" + \"\\n\" ax1.plot(x_aligned, func.fit(x, *optim)/data.max() , lw=1, c=c, ls='--', label=name) ax1.legend(loc='best') ax1.set_ylabel(\"normed intensity [a.u.]\") ax1.set_xlabel(\"distance [nm]\") fig.text(0.5, 0.01, txt, ha='center') fig.set_size_inches(7, 12, forward=True) if path is not None: path_new = path+ r\"\\\\\"+name if not os.path.exists(path_new): os.makedirs(path_new) plt.savefig(path_new +rf'\\profile_{nth_line}.png') plt.close(fig) plt.close(\"all\") x= weakref.ref(fig) del fig del ax1 gc.collect(2) return loss"}
{"text_id": "8485", "text": "docstring: def handle_missing_values(self): self.data['TotalCharges'].replace(' ', np.nan, inplace=True) self.data = self.data.astype({\"TotalCharges\": \"float64\"}) self.data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True) return self.data"}
{"text_id": "8486", "text": "docstring: def drop_primary_key(self, table_name, col_name): if self.__read_only__: raise RuntimeError('Cannot modify a read only database.') if table_name not in self.__skeleton__: raise ValueError(\"Database has no table %s.\"%table_name) if col_name not in self.__skeleton__[table_name]: raise ValueError(\"Table %s has no column %s.\"%(table_name,col_name)) if not self.__skeleton__[table_name][col_name]['primary_key']: return self.__skeleton__[table_name][col_name]['primary_key'] = False self._rebuild_table(table_name)"}
{"text_id": "8487", "text": "docstring: def is_file_ignored(self, path): if self._pathspec.match_file(str(path)): return True try: file_size = path.stat().st_size except FileNotFoundError: file_size = 0 return file_size > self.max_file_size"}
{"text_id": "8488", "text": "docstring: def discard(self, *keys): self._update_status(*keys)"}
{"text_id": "8489", "text": "docstring: def _is_block_processed(info: NodeEventInfo) -> bool: _, encached = cache.monitoring.set_block(info) return not encached"}
{"text_id": "8490", "text": "docstring: def login_exception(self): return self._login_exception"}
{"text_id": "8491", "text": "docstring: def edge_cells(self, u, v): edge_ckeys = self.plane[u][v].values() ckey = edge_ckeys[0] ordered_ckeys = [ckey] for i in range(len(edge_ckeys) - 1): hfkey = self.cell[ckey][u][v] w = self.halfface_vertex_descendent(hfkey, v) ckey = self.plane[w][v][u] ordered_ckeys.append(ckey) return ordered_ckeys"}
{"text_id": "8492", "text": "docstring: def useful_tokens(tokens): ret_tokens = [] lines = {} for token in tokens: line_key = (token['line_num'], token['par_num'], token['block_num']) if line_key not in lines: lines[line_key] = [] lines[line_key].append(token) for pos, toks in lines.items(): line_text = ' '.join([t['text'] for t in toks if t['text']]) if line_text.startswith('AUTO CR - LOG SUMMARY'): continue if line_text.startswith('CPD 00'): continue for tok in toks: ret_tokens.append(tok) return ret_tokens"}
{"text_id": "8493", "text": "docstring: def parameters(self) -> Dict[str, Any]: return { \"edge_embedding_method\": self._edge_embedding_method, **super().parameters() }"}
{"text_id": "8494", "text": "docstring: def mask_list_with_host(request, host_path, in_text, urls_be_gone=[]): if in_text == '': return in_text if len(urls_be_gone) == 0: return in_text resource_router_def = get_resourcerouter() resource_router_def_server_address = resource_router_def.server_address if isinstance(resource_router_def_server_address, str): if resource_router_def_server_address not in urls_be_gone: urls_be_gone.append(resource_router_def_server_address) for kill_url in urls_be_gone: if kill_url.endswith('/'): kill_url = kill_url[:-1] in_text = mask_with_this_url(request, host_path, in_text, kill_url) return in_text"}
{"text_id": "8495", "text": "docstring: def update_actor_weights( self, flat_weights, full_step, surrogate_loss, states, actions, advantages, ): learning_rate = 1.0 for _ in range(self.actor_iterations): updated_weights = flat_weights + full_step * learning_rate self.flat_to_weights(updated_weights, self.actor.trainable_variables, True) losses = new_surrogate_loss, new_kl_divergence = self.calculate_losses( states, actions, advantages ) improvement = new_surrogate_loss - surrogate_loss ok_conditions = [ np.isfinite(losses).all(), new_kl_divergence <= self.max_kl * 1.5, improvement > 0, ] if all(ok_conditions): break learning_rate *= 0.5 else: self.flat_to_weights(flat_weights, self.actor.trainable_variables, True)"}
{"text_id": "8496", "text": "docstring: def activate(self, option=''): standby_host = self.get_current_standby() standby_port = self.get_standby_port() standby_loc = self.get_standby_dd() self.run_remote(self.host, 'gpstop -aim', pgport=self.pgport, standbydd=self.mdd) gpactivate_cmd = 'gpactivatestandby -a -d %s %s' %(standby_loc, option) (rc, result) = self.run_remote(standby_host, gpactivate_cmd, pgport = standby_port, standbydd=standby_loc) tinctest.logger.info('Result without force option to activate standby %s'%result) if (rc != 0) and result.find('Force activation required') != -1: tinctest.logger.info('activating standby failed, try force activation...') gpactivate_cmd = 'gpactivatestandby -a -f -d %s %s' %(standby_loc, option) (rc, result) = self.run_remote(standby_host, gpactivate_cmd, pgport = standby_port, standbydd=standby_loc) if (rc != 0): tinctest.logger.error('Force activating standby failed!') return False tinctest.logger.info('standby acvitated, host value %s' % standby_host) return True"}
{"text_id": "8497", "text": "docstring: def path(self): if self.errors: return self.errors[0].path"}
{"text_id": "8498", "text": "docstring: def dd2fdsn(in_file,subset=None): T0 = time.time() print(\"The start time is 0.\") if subset == None: filt = False else: filt = True [lon_min,lon_max,lat_min,lat_max] = subset events = [] out_file = \"dd.fdsn\" f=open(out_file,'w') f.write(\"#EventID|Time|Latitude|Longitude|Depth/km|Author|Catalog|Contributor|ContributorID|MagType|Magnitude|MagAuthor|EventLocationName\\n\") f.close() eve_dict,df = load_hypoDD(reloc_file=in_file) T1 = time.time() print(\"%f seconds passed to load hypoDD file\" %(T1-T0)) eve_list = list(eve_dict) f = open(out_file,'a') for eve in eve_list: evid = eve_dict[eve][4] e_time = UTCDateTime.strptime(eve,\"%Y%m%d%H%M%s%f\") e_lon = eve_dict[eve][0] e_lat = eve_dict[eve][1] e_dep = eve_dict[eve][2] e_mag = eve_dict[eve][3] if filt: if e_lat>lat_max or e_lat<lat_min or e_lon>lon_max or e_lon<lon_min: continue mag_type = 'ML' f.write('{:0>6d}'.format(evid)+\"|\") f.write(str(e_time)+\"|\") f.write(format(e_lat,'6.3f')+\"|\") f.write(format(e_lon,'7.3f')+\"|\") f.write(format(e_dep,'6.2f')+\"|\") f.write(\"Hardy|\") f.write(\"SC|\") f.write(\"SC|\") f.write(\"01|\") f.write(mag_type+'|') f.write(format(e_mag+0.01,'5.2f')+\"|\") f.write(\"SC Agency|\") f.write(\"SC\\n\") f.close()"}
{"text_id": "8499", "text": "docstring: async def acapy_GET(path, text=False, params=None, headers=None) -> ClientResponse: if not is_tenant(): raise Exception(\"Error can't call tenant admin when accessing as an innkeeper\") response = await acapy_admin_request( \"GET\", path, data=None, text=text, params=params, headers=headers, tenant=True ) return response"}
{"text_id": "8500", "text": "docstring: def serialize_model(model, descr = \"\"): attrs = [i for i in dir(model) if i.endswith('_') and not i.endswith('__')] attr_dict = {i: getattr(model, i) for i in attrs} for k in attr_dict: if isinstance(attr_dict[k], np.ndarray): attr_dict[k] = attr_dict[k].tolist() attr_json = json.dumps(attr_dict) d = OrderedDict() d['model_type'] = [str(model).split('(')[0]] d['description'] = descr d['params'] = [json.dumps(model.get_params())] d['attrs'] = [attr_json] df = pd.DataFrame(d) return df"}
{"text_id": "8501", "text": "docstring: def make_generator_schema(generator): type_choices = [] for klass in generator.factories.keys(): od = object_description(klass) type_choices.append((od, od)) class GeneratorSchema(colander.MappingSchema): seed = colander.SchemaNode( colander.Integer(), title=_(u'Seed'), default=1, ) depth = colander.SchemaNode( colander.Integer(), title=_('Depth'), description=_('How many sublevels to create'), default=2, ) top_level = colander.SchemaNode( colander.Integer(), title=_('Top level'), description=_('How many objects to put in this context'), default=10, ) sub_level = colander.SchemaNode( colander.Integer(), title=_('Sub level'), description=_('How many objects to put in each sublevel'), default=10, ) users = colander.SchemaNode( colander.Integer(), title=_('Users'), description=_('How many users to create'), default=10, ) root_types = colander.SchemaNode( colander.Set(), title=_('Root types'), description=_('What type of content to put in the top level'), widget=deform.widget.CheckboxChoiceWidget( values=type_choices, inline=True), validator=colander.Length(min=1), ) content_types = colander.SchemaNode( colander.Set(), title=_('Sublevel content types'), description=_('What type of content to create in sublevels'), widget=deform.widget.CheckboxChoiceWidget( values=type_choices, inline=True), validator=colander.Length(min=1), ) return GeneratorSchema()"}
{"text_id": "8502", "text": "docstring: def DRL_prediction( self, model, name, last_state, iter_num, turbulence_threshold, initial ): trade_data = data_split( self.df, start=self.unique_trade_date[iter_num - self.rebalance_window], end=self.unique_trade_date[iter_num], ) trade_env = DummyVecEnv( [ lambda: StockTradingEnv( trade_data, self.stock_dim, self.hmax, self.initial_amount, self.buy_cost_pct, self.sell_cost_pct, self.reward_scaling, self.state_space, self.action_space, self.tech_indicator_list, turbulence_threshold=turbulence_threshold, initial=initial, previous_state=last_state, model_name=name, mode=\"trade\", iteration=iter_num, print_verbosity=self.print_verbosity, ) ] ) trade_obs = trade_env.reset() for i in range(len(trade_data.index.unique())): action, _states = model.predict(trade_obs) trade_obs, rewards, dones, info = trade_env.step(action) if i == (len(trade_data.index.unique()) - 2): last_state = trade_env.render() df_last_state = pd.DataFrame({\"last_state\": last_state}) df_last_state.to_csv( \"results/last_state_{}_{}.csv\".format(name, i), index=False ) return last_state"}
{"text_id": "8503", "text": "docstring: def pchanged(self): for oid, func in six.iteritems(self._propobservers): func(self)"}
{"text_id": "8504", "text": "docstring: def infer_from_static_df( df, featureset, entity_columns=None, options: InferOptions = InferOptions.default() ): if hasattr(df, \"to_dataframe\"): df = df.to_dataframe() inferer = get_infer_interface(df) if InferOptions.get_common_options(options, InferOptions.schema()): featureset.spec.timestamp_key = inferer.infer_schema( df, featureset.spec.features, featureset.spec.entities, featureset.spec.timestamp_key, entity_columns, options=options, ) if InferOptions.get_common_options(options, InferOptions.Stats): featureset.status.stats = inferer.get_stats(df, options) if InferOptions.get_common_options(options, InferOptions.Preview): featureset.status.preview = inferer.get_preview(df) return df"}
{"text_id": "8505", "text": "docstring: def connect_with_credentials( host: str, dbname: str, port: str, user: str, password: str ) -> psycopg2.extensions.connection: if not isinstance(host, str): raise TypeError(\"`host` must be a str.\") if not isinstance(dbname, str): raise TypeError(\"`dbname` must be a str.\") if not isinstance(port, str): raise TypeError(\"`port` must be a str.\") if not isinstance(user, str): raise TypeError(\"`user` must be a str.\") if not isinstance(password, str): raise TypeError(\"`password` must be a str.\") cnxn = psycopg2.connect( f\"\"\" host={host} dbname={dbname} port={port} user={user} password={password} \"\"\" ) return cnxn"}
{"text_id": "8506", "text": "docstring: def validate_json_input(message_body, json_schema): try: validate(instance=json.loads(message_body), schema=json_schema) except ValidationError as ex: error_msg = \"[json validation] Invalid json format: {}.\".format(ex) raise GenericNonFatalException(error_msg=error_msg, error_code=SIMAPP_EVENT_ERROR_CODE_500, error_name=SIMAPP_EVENT_SYSTEM_ERROR) except Exception as ex: error_msg = \"[json validation] Something wrong when validating json format: {}.\".format(ex) raise GenericNonFatalException(error_msg=error_msg, error_code=SIMAPP_EVENT_ERROR_CODE_500, error_name=SIMAPP_EVENT_SYSTEM_ERROR)"}
{"text_id": "8507", "text": "docstring: def rankNonDominatedFrontiers(data): nonDominatedRank = np.zeros(data.shape[0],dtype=int) rank = 0 indicesDominated = list(np.arange(data.shape[0])) indicesNonDominated = [] rawData = data while np.shape(data)[0] > 0: rank += 1 indicesNonDominated = list(nonDominatedFrontier(data, False)) if rank > 1: for i in range(len(indicesNonDominated)): indicesNonDominated[i] = indicesDominated[indicesNonDominated[i]] indicesDominated = list(set(indicesDominated)-set(indicesNonDominated)) data = rawData[indicesDominated] nonDominatedRank[indicesNonDominated] = rank nonDominatedRank = list(nonDominatedRank) return nonDominatedRank"}
{"text_id": "8508", "text": "docstring: def check_for_updates(bot): if not bot.channels: return try: response = api_request() response = json.loads(response) f = open(JSONFILE, \"r\") oldfile = json.load(f) f.close() if response != oldfile: video_count = 0 series_count = 0 dic = [] series_list = [] send_counts = False for item in oldfile: dic.append(item[\"file\"]) for item in response: video_count += 1 if item[\"source\"] not in series_list: series_list.append(item[\"source\"]) series_count += 1 if not item[\"file\"] in dic: print(\"[aoupdates] New item found! {}\".format(item[\"file\"])) send_notice(bot, \"New video! {} from {} http://openings.moe/?video={}\".format( item[\"title\"], item[\"source\"], item[\"file\"])) send_counts = True time.sleep(1) if send_counts: send_notice(bot, \"Now serving {} videos from {} series!\".format( video_count, series_count)) with open(JSONFILE, \"w\") as f: json.dump(response, f, sort_keys=True, indent=4) except: traceback.print_exc(file=sys.stdout)"}
{"text_id": "8509", "text": "docstring: def create_from_networkx_bipartite(nx_graph, utype, etype, vtype, edge_id_attr_name='id', node_attrs=None, edge_attrs=None, restrict_format='any'): if not nx_graph.is_directed(): nx_graph = nx_graph.to_directed() top_nodes = {n for n, d in nx_graph.nodes(data=True) if d['bipartite'] == 0} bottom_nodes = set(nx_graph) - top_nodes top_nodes = sorted(top_nodes) bottom_nodes = sorted(bottom_nodes) top_map = {n : i for i, n in enumerate(top_nodes)} bottom_map = {n : i for i, n in enumerate(bottom_nodes)} if nx_graph.number_of_edges() > 0: has_edge_id = edge_id_attr_name in next(iter(nx_graph.edges(data=True)))[-1] else: has_edge_id = False if has_edge_id: num_edges = nx_graph.number_of_edges() src = np.zeros((num_edges,), dtype=np.int64) dst = np.zeros((num_edges,), dtype=np.int64) for u, v, attr in nx_graph.edges(data=True): eid = attr[edge_id_attr_name] src[eid] = top_map[u] dst[eid] = bottom_map[v] else: src = [] dst = [] for e in nx_graph.edges: if e[0] in top_map: src.append(top_map[e[0]]) dst.append(bottom_map[e[1]]) src = utils.toindex(src) dst = utils.toindex(dst) g = create_from_edges( src, dst, utype, etype, vtype, len(top_nodes), len(bottom_nodes), validate=False, restrict_format=restrict_format) assert node_attrs is None, 'Retrieval of node attributes are not supported yet.' assert edge_attrs is None, 'Retrieval of edge attributes are not supported yet.' return g"}
{"text_id": "8510", "text": "docstring: def finalizable(self): assert self.terminated try: self.finalized_denotation return True except ValueError as e: return False"}
{"text_id": "8511", "text": "docstring: async def on_raw_reaction_add(self, payload): guild = self.bot.get_guild(payload.guild_id) if payload.message_id in self.hangman_games and guild is not None and len(payload.emoji.name) == 1: letter = chr(ord(payload.emoji.name) - 127365) if 'a' <= letter <= 'z': channel = guild.get_channel(payload.channel_id) message = await channel.fetch_message(payload.message_id) hangman = self.hangman_games[payload.message_id] hangman.guess(letter) if '*' not in hangman.visible: del self.hangman_games[payload.message_id] await message.edit(embed=self.render_hangman_embed(hangman)) await message.clear_reaction(payload.emoji)"}
{"text_id": "8512", "text": "docstring: def TokenizeNodes(self, node_lists:list): try: look_up = {y:x for x,y in self._tokenizer.index_word.items()} tokenized_nodes_all = [] for nodes in node_lists: tokenized_nodes = [] for node in nodes: tokenized_nodes.append(look_up.get(node, 0)) tokenized_nodes_all.append(tokenized_nodes) return tokenized_nodes_all except Exception as ex: template = \"An exception of type {0} occurred in [GloVeDatasetPreprocessor.TokenizeNodes]. Arguments:\\n{1!r}\" message = template.format(type(ex).__name__, ex.args) print(message)"}
{"text_id": "8513", "text": "docstring: def ARRAY_BOUNDS(): return __ARRAY_BOUNDS"}
{"text_id": "8514", "text": "docstring: def query_aggr_options(na_server, aggr_name): add_elems = {'aggregate': aggr_name} attrs = {} try: result = netapp_api.invoke_api(na_server, api_name='aggr-options-list-info', api_family='cm', query=None, des_result=None, additional_elems=add_elems, is_iter=False) for res in result: options = res.get_child_by_name('options') if options: op_list = options.get_children() for op in op_list: if op.get_child_content('name') == 'ha_policy': attrs['ha_policy'] = op.get_child_content('value') if op.get_child_content('name') == 'raidtype': attrs['raid_type'] = op.get_child_content('value') except Exception as e: LOG.debug(\"Exception querying aggr options. %s\", e) return attrs"}
{"text_id": "8515", "text": "docstring: def _get_complete_event_in_handler_head_expression(self) -> str: from apysc._display.skew_x_interface import SkewXInterface expression: str = '' if isinstance(self._target, SkewXInterface): self._target._initialize_skew_x_if_not_initialized() expression = ( f'{self._target._skew_x.variable_name} = ' f'{self._skew_x.variable_name};' ) return expression"}
{"text_id": "8516", "text": "docstring: def merge_dico_in_first(d1, d2): for k, v in d2.items(): if k in d1: raise ValueError(f\"Key {k} should not be in d1\") else: d1[k] = v return d1"}
{"text_id": "8517", "text": "docstring: def list_names(self, workbook_id: str, params: dict = None, **kwargs) -> Response: url = \"me/drive/items/{}/workbook/names\".format(workbook_id) return self._client._get(self._client.base_url + url, params=params, **kwargs)"}
{"text_id": "8518", "text": "docstring: def grade(self, pop): summed = reduce(add, (self.fitness(network) for network in pop)) return summed / len(pop)"}
{"text_id": "8519", "text": "docstring: def sniff(self, filename): with open(filename) as handle: relation_found = False attribute_found = False for line_count, line in enumerate(handle): if line_count > 1000: return False line = line.strip() if not line: continue start_string = line[:20].upper() if start_string.startswith(\"@RELATION\"): relation_found = True elif start_string.startswith(\"@ATTRIBUTE\"): attribute_found = True elif start_string.startswith(\"@DATA\"): if relation_found and attribute_found: return True return False"}
{"text_id": "8520", "text": "docstring: def split(inp, nsplits_or_sections, axis=0): sub_tensors = [] sections = [] def swapaxis(inp, src, dst): if src == dst: return inp shape = [i for i in range(inp.ndim)] shape[src] = dst shape[dst] = src return inp.transpose(shape) inp = swapaxis(inp, 0, axis) if isinstance(nsplits_or_sections, int): incr_step = ceil(inp.shape[0] / nsplits_or_sections) nsplits = nsplits_or_sections while nsplits > 0: nsplits -= 1 sections.append(incr_step.astype(\"int32\")) incr_step += nsplits_or_sections else: sections = nsplits_or_sections st = 0 for se in sections: sub_tensors.append(swapaxis(inp[st:se], axis, 0)) st = se if st < inp.shape[0]: sub_tensors.append(swapaxis(inp[st:], axis, 0)) return sub_tensors"}
{"text_id": "8521", "text": "docstring: def plot_average_rewards(exp_dir, starts_with=None): files = [f for f in os.listdir(exp_dir) if starts_with is None or f.startswith(starts_with)] files.sort() files = [files[i] for i in range(len(files)) if i % 1 == 0] print(files) for file in files: filepath = exp_dir + '/' + file with open(filepath) as f: exp_results = json.load(f) n_steps = exp_results['parameters']['training_steps'] p_hardcoded_start = round(exp_results['parameters']['p_hardcoded_start'], 1) p_hardcoded_end = round(exp_results['parameters']['p_hardcoded_end'], 1) rewards_list = exp_results['exp_rewards'] end_steps_list = exp_results['exp_end_steps'] plot_list = [ np.array([end_steps, rewards]) for end_steps, rewards in zip(end_steps_list, rewards_list) ] x, y_avg, var, y_arrays = _merge_plots(plot_list) if p_hardcoded_start != p_hardcoded_end: label = 'p from ' + str(p_hardcoded_start) + ' to ' + str(p_hardcoded_end) else: label = 'p = ' + str(p_hardcoded_start) plt.plot(x, y_avg, label=label) plt.legend()"}
{"text_id": "8522", "text": "docstring: def _aggregate_runs(runs: List[Dict[str, Any]]): aggregate: Dict[str, Any] = {} for run in runs: for key, value in run.items(): if key in aggregate: aggregate[key].append(value) else: aggregate[key] = [value] aggregate = {key: get_avg(value) for key, value in aggregate.items()} return aggregate"}
{"text_id": "8523", "text": "docstring: def Conjunctive(name, bases, attrs): basemetaclasses = [] for base in bases: metacls = type(base) if isinstance(metacls, type) and metacls is not type and not metacls in basemetaclasses: basemetaclasses.append(metacls) dynamic = type(''.join(b.__name__ for b in basemetaclasses), tuple(basemetaclasses), {}) return dynamic(name, bases, attrs)"}
{"text_id": "8524", "text": "docstring: def scrape_mtgazone_deck(link: str, session=None) -> Tuple[Optional[dict], Optional[dict]]: req_session = session if session else requests.Session with req_session() as session: mtgazone_html = session.get(link).text try: real_link = BeautifulSoup(mtgazone_html, 'lxml').find('a', {'class': \"_self cvplbd\"})['href'] mtgazone_html = session.get(real_link).text except TypeError: pass mainboard, sideboard = get_mtgazone_deck(mtgazone_html) return mainboard, sideboard"}
{"text_id": "8525", "text": "docstring: def write_metadata(self, **metadata): if self.suppress_output: return 0 metadata_line = json.dumps(metadata, sort_keys=True) + os.linesep metadata_output = \"\" if \"b\" in self._output_mode: metadata_output = metadata_line.encode(\"utf-8\") else: metadata_output = metadata_line return self._output_stream.write(metadata_output)"}
{"text_id": "8526", "text": "docstring: def groups_pre(self, groups): for group in groups: self.group_pre(group)"}
{"text_id": "8527", "text": "docstring: def _get_pattern_files(data_path, dataset_props): to_ignore = ['renders'] pattern_specs = [] root, dirs, files = next(os.walk(data_path)) if dataset_props['to_subfolders']: for directory in dirs: if directory not in to_ignore: pattern_specs.append(os.path.join(root, directory, 'specification.json')) else: for file in files: if ('.json' in file and 'specification' in file and 'template' not in file): pattern_specs.append(os.path.normpath(os.path.join(root, file))) return pattern_specs"}
{"text_id": "8528", "text": "docstring: def add_coverage(cov: Coverage, claim: Claim) -> Claim: _add_object(claim, cov) ins = insurance(cov, 1, True) if not isinstance(claim.insurance, list): claim.insurance = [ins] else: claim.insurance.append(ins) return claim"}
{"text_id": "8529", "text": "docstring: def _get_pathway_metadata(rdf_graph: rdflib.Graph) -> Dict[str, Dict[str, Dict[str, str]]]: return query_result_to_dict( rdf_graph.query(GET_PATHWAY_INFO_SPARQL, initNs=PREFIXES), attr_empty=['title', 'identifier', 'description', 'pathway_id'], id_dict=False, )"}
{"text_id": "8530", "text": "docstring: def progress_str(self): try: clue = self.clues.get(clue=self.targeted_clue) progress = clue.progress_percentage except (ClueDiscovery.DoesNotExist, AttributeError): progress = 0 if progress <= 0: return \"No real progress has been made to finding something new.\" if progress <= 25: return \"You've made some progress.\" if progress <= 50: return \"You've made a good amount of progress.\" if progress <= 75: return \"You feel like you're getting close to finding something.\" return \"You feel like you're on the verge of a breakthrough. You just need more time.\""}
{"text_id": "8531", "text": "docstring: def calculate_dtw(references, hypotheses): euclidean_norm = lambda x, y: np.sum(np.abs(x - y)) dtw_scores = [] hypotheses = hypotheses[:, 1:] for i, ref in enumerate(references): _ , ref_max_idx = torch.max(ref[:, -1], 0) if ref_max_idx == 0: ref_max_idx += 1 ref_count = ref[:ref_max_idx,:-1].cpu().numpy() hyp = hypotheses[i] _, hyp_max_idx = torch.max(hyp[:, -1], 0) if hyp_max_idx == 0: hyp_max_idx += 1 hyp_count = hyp[:hyp_max_idx,:-1].cpu().numpy() d, cost_matrix, acc_cost_matrix, path = dtw(ref_count, hyp_count, dist=euclidean_norm) d = d/acc_cost_matrix.shape[0] dtw_scores.append(d) return dtw_scores"}
{"text_id": "8532", "text": "docstring: def reachable_nodes(self, roots): seen = set() stk = [self._vertex_id(r) for r in roots] while stk: n = stk.pop() if n in seen: continue yield self.nodes[n] seen.add(n) stk.extend(self.g.successors(n))"}
{"text_id": "8533", "text": "docstring: def cash_analysis_with_http_info(self, cash_analysis_request, **kwargs): \"\"\"Cash Analysis Analyze cash activity over time This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.cash_analysis_with_http_info(cash_analysis_request, async_req=True) >>> result = thread.get() :param async_req bool :param CashAnalysisRequest cash_analysis_request: Request payload for Cash Analysis (required) :return: dict(str, object) If the method is called asynchronously, returns the request thread. \"\"\" all_params = ['cash_analysis_request'] all_params.append('async_req') all_params.append('_return_http_data_only') all_params.append('_preload_content') all_params.append('_request_timeout') params = locals() for key, val in six.iteritems(params['kwargs']): if key not in all_params: raise TypeError( \"Got an unexpected keyword argument '%s'\" \" to method cash_analysis\" % key ) params[key] = val del params['kwargs'] if self.api_client.client_side_validation and ('cash_analysis_request' not in params or params['cash_analysis_request'] is None): raise ValueError(\"Missing the required parameter `cash_analysis_request` when calling `cash_analysis`\") collection_formats = {} path_params = {} query_params = [] header_params = {} form_params = [] local_var_files = {} body_params = None if 'cash_analysis_request' in params: body_params = params['cash_analysis_request'] header_params['Accept'] = self.api_client.select_header_accept( ['application/json']) header_params['Content-Type'] = self.api_client.select_header_content_type( ['application/json']) auth_settings = ['oauth2'] return self.api_client.call_api( '/business/cash_analysis', 'POST', path_params, query_params, header_params, body=body_params, post_params=form_params, files=local_var_files, response_type='dict(str, object)', auth_settings=auth_settings, async_req=params.get('async_req'), _return_http_data_only=params.get('_return_http_data_only'), _preload_content=params.get('_preload_content', True), _request_timeout=params.get('_request_timeout'), collection_formats=collection_formats)"}
{"text_id": "8534", "text": "docstring: def fetch_instr(ticker): if ticker is None or len(ticker) == 0: raise ValueError('missing ticker argument') instrumdat_key = InstrumdatKey(ticker) df = _fetch_instr(instrumdat_key) return df"}
{"text_id": "8535", "text": "docstring: def has_no_side_effect(rhs, lives, call_table): if isinstance(rhs, ir.Expr) and rhs.op == 'call': func_name = rhs.func.name if func_name not in call_table or call_table[func_name] == []: return False call_list = call_table[func_name] if (call_list == ['empty', numpy] or call_list == [slice] or call_list == ['stencil', numba] or call_list == ['log', numpy] or call_list == ['dtype', numpy] or call_list == [numba.array_analysis.wrap_index]): return True elif (isinstance(call_list[0], numba.extending._Intrinsic) and (call_list[0]._name == 'empty_inferred' or call_list[0]._name == 'unsafe_empty_inferred')): return True from numba.targets.registry import CPUDispatcher from numba.targets.linalg import dot_3_mv_check_args if isinstance(call_list[0], CPUDispatcher): py_func = call_list[0].py_func if py_func == dot_3_mv_check_args: return True for f in remove_call_handlers: if f(rhs, lives, call_list): return True return False if isinstance(rhs, ir.Expr) and rhs.op == 'inplace_binop': return rhs.lhs.name not in lives if isinstance(rhs, ir.Yield): return False if isinstance(rhs, ir.Expr) and rhs.op == 'pair_first': return False return True"}
{"text_id": "8536", "text": "docstring: def clone_td3_batch_norm_stats( model: TD3, ) -> (th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor): actor_batch_norm = model.actor.features_extractor.batch_norm actor_bias, actor_running_mean = clone_batch_norm_stats(actor_batch_norm) critic_batch_norm = model.critic.features_extractor.batch_norm critic_bias, critic_running_mean = clone_batch_norm_stats(critic_batch_norm) actor_target_batch_norm = model.actor_target.features_extractor.batch_norm actor_target_bias, actor_target_running_mean = clone_batch_norm_stats(actor_target_batch_norm) critic_target_batch_norm = model.critic_target.features_extractor.batch_norm critic_target_bias, critic_target_running_mean = clone_batch_norm_stats(critic_target_batch_norm) return ( actor_bias, actor_running_mean, critic_bias, critic_running_mean, actor_target_bias, actor_target_running_mean, critic_target_bias, critic_target_running_mean, )"}
{"text_id": "8537", "text": "docstring: def add_to_visgroup(self, name: str, *args): v_id = None for visgroup in self.visgroups.get_visgroups(): if visgroup.name == name: v_id = visgroup.visgroupid if v_id is None: v_id = self.visgroups.new_visgroup(name).visgroupid for item in args: item.editor.visgroupid = v_id"}
{"text_id": "8538", "text": "docstring: def p_data(show, cmap, title, resource_names, datasource, binning, output): try: cmap = plt.get_cmap(cmap) except Exception: raise click.BadOptionUsage(\"cmap\", \"must be a valid Matplotlib colormap name, not \\'%s\\'.\" % cmap) try: source = DataSourceIO.read(datasource) except: raise click.FileError(datasource, \"does not exist or is not readable.\") if binning.dimensions != len(source.domain): raise click.UsageError( \"Dimensions of binning (%d) and datasource (%d) mismatch.\" % (binning.dimensions, len(source.domain))) if resource_names is None: resource_names = source.column_names else: resource_names = resource_names.split(\",\") if len(resource_names) != len(source.column_names): raise click.BadOptionUsage(\"resource-names\", \"Dimensions of resource names (%d) and datasource (%d) mismatch.\" % (len(resource_names), len(source.column_names))) histogram = source.get_histogram(binning) HairyPlotter.plot_histogram( histogram, cmap=cmap, column_names=resource_names, title=title) plt.savefig(output, bbox_inches='tight') if show: plt.show()"}
{"text_id": "8539", "text": "docstring: def change_name_of_agent(self, agent_id, name): current_name = self.shelf_manager.get_agent(agent_id)['custom_name'] new_name = str(name) if str(new_name) != current_name: self.shelf_manager.agent_change_name(agent_id, new_name) redirect('/dashboard')"}
{"text_id": "8540", "text": "docstring: def _filter_entry_ids(self, entries): result = [] for entry in entries: del entry[\"_id\"] result.append(entry) return result"}
{"text_id": "8541", "text": "docstring: def construct_data_reader(lbann): message = lbann.reader_pb2.DataReader() message.reader.extend([ tools.create_python_data_reader( lbann, current_file, 'get_sample', 'num_samples', 'sample_dims', 'train', ), tools.create_python_data_reader( lbann, current_file, 'get_sample', 'num_samples', 'sample_dims', 'validate', ), tools.create_python_data_reader( lbann, current_file, 'get_sample', 'num_samples', 'sample_dims', 'tournament', ), ]) return message"}
{"text_id": "8542", "text": "docstring: def trigger_pv(self, pv_name): if self._trigger_pv is not None: print_and_log(\"Trigger pv being redefined to {0} from {1}\".format(pv_name, self._trigger_pv), severity=SEVERITY.MAJOR, src=\"ArchiverAccess\") self._trigger_pv = pv_name return self"}
{"text_id": "8543", "text": "docstring: def load(mp: str) -> List[Tuple[str, MetaData]]: if mp in MapManager.cached_builtins: return [(mp, copy.deepcopy(MapManager.cached_builtins[mp]))] elif mp in MapManager.builtins: data = MapManager.builtins[mp] return [(mp, copy.deepcopy(data))] else: return MapManager.try_load_from_file(mp)"}
{"text_id": "8544", "text": "docstring: def combine_transition_probabilities(self, a: tf.Tensor, b: tf.Tensor) -> tf.Tensor: assert len(a.shape) >= 4 assert len(b.shape) >= 4 assert a.shape[-1] == 2 assert b.shape[3] == 2 dims_a = tf.shape(a)[1:-3] dims_b = tf.shape(b)[4:] a = tf.reshape(a, shape=[self.batch_size, -1, self.max_logit_length, self.max_label_length_plus_one, 2, 1]) b = tf.reshape(b, shape=[self.batch_size, 1, self.max_logit_length, self.max_label_length_plus_one, 2, -1]) ab_term = tf.reduce_logsumexp(a, 4) + b[:, :, :, :, 0] horizontal_blank_grad_term = \\ expand_many_dims(self.blank_logproba, axes=[1, 3]) + tf.reduce_logsumexp(ab_term, axis=3) act = a[:, :, :, :, 1] + expand_many_dims(self.previous_label_token_log_proba, axes=[1, 4]) + b[:, :, :, :, 1] horizontal_non_blank_grad_term = self.select_from_act(act, self.preceded_label) input_tensor = a + expand_many_dims(self.any_to_open_diagonal_step_log_proba, axes=[1, 5]) + \\ tf.roll(b[:, :, :, :, 1:], shift=-1, axis=3) act = tf.reduce_logsumexp(input_tensor=input_tensor, axis=4) diagonal_non_blank_grad_term = self.select_from_act(act=act, label=self.label) non_blank_grad_term = logsumexp(horizontal_non_blank_grad_term, diagonal_non_blank_grad_term) blank_mask = self.blank_token_index == tf.range(self.num_tokens) output = tf.where( condition=expand_many_dims(blank_mask, axes=[0, 1, 2, 4]), x=tf.expand_dims(horizontal_blank_grad_term, 3), y=non_blank_grad_term, ) output_shape = tf.concat( [ tf.expand_dims(self.batch_size, axis=0), dims_a, tf.expand_dims(self.max_logit_length, axis=0), tf.expand_dims(self.num_tokens, axis=0), dims_b ], axis=0 ) output_reshaped = tf.reshape(output, shape=output_shape) return output_reshaped"}
{"text_id": "8545", "text": "docstring: def monitor(app, endpoint_type='url:1', get_endpoint_fn=None, latency_buckets=None, mmc_period_sec=30, cpu_period_sec=30, multiprocess_mode='all', metrics_path='/metrics', is_middleware=True, metrics_list=None): multiprocess_on = 'PROMETHEUS_MULTIPROC_DIR' in os.environ get_endpoint = endpoint.fn_by_type(endpoint_type, get_endpoint_fn) memcollect_enabled = mmc_period_sec is not None cpucollect_enabled = cpu_period_sec is not None @app.before_server_start def before_start(app, loop): app.config.metrics = {} metrics.init( app, latency_buckets, multiprocess_mode, memcollect_enabled=memcollect_enabled, cpucollect_enabled=cpucollect_enabled, metrics_list=metrics_list, ) if is_middleware is True: @app.middleware('request') async def before_request(request): if request.path != metrics_path and request.method != \"OPTIONS\": metrics.before_request_handler(request) @app.middleware('response') async def before_response(request, response): if request.path != metrics_path and request.method != \"OPTIONS\": metrics.after_request_handler(request, response, get_endpoint) if multiprocess_on: @app.after_server_stop def after_stop(app, loop): multiprocess.mark_process_dead(os.getpid()) if memcollect_enabled: @app.before_server_start async def start_memcollect_task(app, loop): app.config.memcollect_task = loop.create_task( metrics.periodic_memcollect_task( app, mmc_period_sec, loop ) ) @app.after_server_stop async def stop_memcollect_task(app, loop): app.config.memcollect_task.cancel() if cpucollect_enabled: @app.before_server_start async def start_cpucollect_task(app, loop): app.config.cpucollect_task = loop.create_task( metrics.periodic_cpucollect_task( app, cpu_period_sec, loop ) ) @app.after_server_stop async def stop_cpucollect_task(app, loop): app.config.cpucollect_task.cancel() return MonitorSetup(app, metrics_path, multiprocess_on)"}
{"text_id": "8546", "text": "docstring: def identifier_appearance_stat_key(appearances: set) -> str: if {'templates', 'references'} <= appearances: return 'in_tag_ref_and_template' elif 'templates' in appearances: return 'only_in_template' elif 'references' in appearances: return 'only_in_tag_ref' elif 'sections' in appearances: return 'only_in_filtered_sections' else: return 'only_in_raw_text'"}
{"text_id": "8547", "text": "docstring: def adjacency_matrix(self, transpose=None, ctx=F.cpu(), scipy_fmt=None, etype=None): if transpose is None: dgl_warning( \"Currently adjacency_matrix() returns a matrix with destination as rows\" \" by default.\\n\\tIn 0.5 the result will have source as rows\" \" (i.e. transpose=True)\") transpose = False etid = self.get_etype_id(etype) if scipy_fmt is None: return self._graph.adjacency_matrix(etid, transpose, ctx)[0] else: return self._graph.adjacency_matrix_scipy(etid, transpose, scipy_fmt, False)"}
{"text_id": "8548", "text": "docstring: def b_vec(self): return self.get_raw(\"gain\") * self.poly.fromroots(self.zeros).real"}
{"text_id": "8549", "text": "docstring: def delete(self, region_id, lang_code): region_translation_service.delete(region_id, lang_code) return {}, 204"}
{"text_id": "8550", "text": "docstring: def _ddp_init_helper(self): def parameters(m, recurse=True): def model_parameters(m): ps = m._former_parameters.values() \\ if hasattr(m, \"_former_parameters\") \\ else m.parameters(recurse=False) for p in ps: yield p for m in m.modules() if recurse else [m]: for p in model_parameters(m): yield p if self.device_ids and len(self.device_ids) > 1: import warnings warnings.warn( \"Single-Process Multi-GPU is not the recommended mode for \" \"DDP. In this mode, each DDP instance operates on multiple \" \"devices and creates multiple module replicas within one \" \"process. The overhead of scatter/gather and GIL contention \" \"in every forward pass can slow down training. \" \"Please consider using one DDP instance per device or per \" \"module replica by explicitly setting device_ids or \" \"CUDA_VISIBLE_DEVICES. \" ) TODO: we don't need to replicate params in here. they're always going to be broadcasted using larger blocks in broadcast_coalesced, so it might be better to not pollute the caches with these small blocks self._module_copies = replicate(self.module, self.device_ids, detach=True) self._module_copies[0] = self.module for module_copy in self._module_copies[1:]: for param, copy_param in zip(self.module.parameters(), parameters(module_copy)): Reducer requires param copies have the same strides across replicas. Fixes up copy_param strides in case replicate didn't match param strides. if param.layout is torch.strided and param.stride() != copy_param.stride(): with torch.no_grad(): copy_param.set_(copy_param.clone() .as_strided(param.size(), param.stride()) .copy_(copy_param)) copy_param.requires_grad = param.requires_grad else: self._module_copies = [self.module] self.modules_params = [list(parameters(m)) for m in self._module_copies] self.modules_buffers = [list(m.buffers()) for m in self._module_copies] Build tuple of (module, parameter) for all parameters that require grads. modules_and_parameters = [ [ (module, parameter) for module in replica.modules() for parameter in filter( lambda parameter: parameter.requires_grad, parameters(module, recurse=False)) ] for replica in self._module_copies] Build list of parameters. parameters = [ list(parameter for _, parameter in replica) for replica in modules_and_parameters] Checks if a module will produce a sparse gradient. def produces_sparse_gradient(module): if isinstance(module, torch.nn.Embedding): return module.sparse if isinstance(module, torch.nn.EmbeddingBag): return module.sparse return False Build list of booleans indicating whether or not to expect sparse gradients for the corresponding parameters. expect_sparse_gradient = [ list(produces_sparse_gradient(module) for module, _ in replica) for replica in modules_and_parameters] The bucket size limit is specified in the constructor. Additionally, we allow for a single small bucket for parameters that are defined first, such that their gradients don't spill into a much larger bucket, adding unnecessary latency after gradient computation finishes. Experiments showed 1MB is a reasonable value. bucket_indices = dist._compute_bucket_assignment_by_size( parameters[0], [dist._DEFAULT_FIRST_BUCKET_BYTES, self.bucket_bytes_cap], expect_sparse_gradient[0]) Note: reverse list of buckets because we want to approximate the order in which their gradients are produced, and assume they are used in the forward pass in the order they are defined. self.reducer = dist.Reducer( parameters, list(reversed(bucket_indices)), self.process_group, expect_sparse_gradient, self.bucket_bytes_cap, self.find_unused_parameters) passing a handle to torch.nn.SyncBatchNorm layer self._passing_sync_batchnorm_handle(self._module_copies)"}
{"text_id": "8551", "text": "docstring: def CreateToolResultsUiUrl(project_id, tool_results_ids): url_base = properties.VALUES.test.results_base_url.Get() if not url_base: url_base = 'https://console.developers.google.com' url_end = ( 'project/{p}/testlab/mobile/histories/{h}/executions/{e}'.format( p=urllib.quote(project_id), h=urllib.quote(tool_results_ids.history_id), e=urllib.quote(tool_results_ids.execution_id))) return urlparse.urljoin(url_base, url_end)"}
{"text_id": "8552", "text": "docstring: def utility(board): win = winner(board) if win == \"X\": return 1 elif win == \"O\": return -1 return 0"}
{"text_id": "8553", "text": "docstring: def msg_bus_subscriber(topic_config_list, queue_dict, logger, json_config): visualizer = Visualizer(queue_dict, logger, dir_name=os.environ[\"IMAGE_DIR\"], save_image=json_config[\"save_image\"], labels=json_config[\"labels\"], draw_results=json_config[\"draw_results\"]) for topic_config in topic_config_list: topic, msgbus_cfg = topic_config callback_thread = threading.Thread(target=visualizer.callback, args=(msgbus_cfg, topic, )) callback_thread.start()"}
{"text_id": "8554", "text": "docstring: def wait_for_action(self, victim, timeout=TEST_TIMEOUT, **filters): for _ in util.wait(timeout): obj = self.get_json_by_uri(victim[\"resource_uri\"]) actions = obj.get(\"available_actions\", None) if actions is None: actions = get_actions(self.chroma_manager, [obj]).json[\"objects\"] for action in actions: if all(action.get(key) == filters[key] for key in filters): return action actions = [dict((key, action.get(key)) for key in filters) for action in actions] raise AssertionError(\"{0} not found in {1}\".format(filters, actions))"}
{"text_id": "8555", "text": "docstring: async def request_states(): state_consumer = StateConsumerSocket() states_to_request = list(STATES.keys()) action = RequireState(states_to_request) await state_consumer.send(action) state = await state_consumer.receive() for node in state.nodes: STATES[node.__class__] = node logging.info(f\"Loaded state from broker: {node.__class__.__name__}\") return STATES"}
{"text_id": "8556", "text": "docstring: def reshape(self, f): return f.reshape(self.shape)"}
{"text_id": "8557", "text": "docstring: def cls_prop(name, data_type): masked_name = \"__\" + name @property def prop(self): return getattr(self, masked_name) @prop.setter def prop(self, value): if not isinstance(value, data_type): raise TypeError(f\"Expected data type for {name} is {data_type}.\") setattr(self, masked_name, value) return prop"}
{"text_id": "8558", "text": "docstring: def parse_input() -> LanternfishCounter: with open(INPUT_FILE) as f: numbers = [int(x) for x in f.readline().split(\",\")] return dict(Counter(numbers))"}
{"text_id": "8559", "text": "docstring: def is_boosting(self) -> bool: return self.premium_since is not None"}
{"text_id": "8560", "text": "docstring: def remove_router_from_zone(self, zone, **router): zone = self._get_resource(_zone.Zone, zone) return zone.disassociate_router(self, **router)"}
{"text_id": "8561", "text": "docstring: def describe_cf_config_rules(): cf_config_rules_list = [] rules_location = os.environ.get(\"RULES_LOCATION\", \"\") if rules_location: raw_lambdas = get_rules_raw(rules_location.split(\",\")) else: raw_lambdas = get_rules_raw() for rule in raw_lambdas: cf_config_rules_list.append(PREFIX + to_pascal_case(rule)) return cf_config_rules_list"}
{"text_id": "8562", "text": "docstring: def process(self): self.remove_math_envs() parser_logger.info('num math envs:') parser_logger.info(len(self.math_envs)) lines = [p for p in self.file.split('\\n')] word_lines = [self.extract_words(s, i) for i, s in enumerate(lines)] processed_lines = [] for line_num, line in enumerate(word_lines): processed_line = [] if len(line) < 1: processed_line.append(EmptyLine(uuid1())) for w in line: if re.search(r'__MATH_ENV__', w.content): math_env = self.math_envs[0] self.math_envs.pop(0) processed_math_env = self.process_math_env(math_env, line_num) processed_line += processed_math_env else: processed_line.append(w) processed_lines.append(processed_line) processed_lines_unique_ids, line_dict, identifier_line_dict = self.add_unique_ids(processed_lines) linked_words, linked_math_symbols = self.form_links(processed_lines_unique_ids) existing_annotations = self.get_annotation_file_from_repo() file = File(processed_lines_unique_ids, linked_words, linked_math_symbols, self.file_name, self.identifier_count, self.formula_count, existing_annotations) return (line_dict, identifier_line_dict, file)"}
{"text_id": "8563", "text": "docstring: def autoregressive_sample_stream(model, inputs=None, batch_size=1, temperature=1.0, start_id=0, accelerate=True): if inputs is not None and inputs.shape[0] != batch_size: raise ValueError(f'Inputs batch size ({inputs.shape[0]}) does not match ' f'batch_size arg ({batch_size}.') fast_model = tl.Accelerate(model) if accelerate else model start_symbol = np.full((batch_size, 1), start_id, dtype=np.int32) if model.n_in == 1 and inputs is not None: current_symbols = np.concatenate([start_symbol, inputs], axis=1) else: current_symbols = start_symbol while True: if model.n_in > 1 and inputs is not None: logits = fast_model((inputs, current_symbols))[0] else: logits = fast_model(current_symbols) sample = tl.logsoftmax_sample(logits[:, -1, :], temperature=temperature) yield sample current_symbols = sample[:, None]"}
{"text_id": "8564", "text": "docstring: def createc_adc(stm, channel, board, kelvin=False): data = stm.client.getadcvalf(board, channel) if kelvin: import createc.utils.DT670 data = createc.utils.DT670.Volt2Kelvin(data) return data"}
{"text_id": "8565", "text": "docstring: def build_and_configure(self, capture_speed): analyzer = USBAnalyzerApplet(usb_speed=capture_speed) platform = get_appropriate_platform() platform.build(analyzer, do_program=True) time.sleep(3) end_time = time.time() + 6 while not self._device: if time.time() > end_time: raise RuntimeError('Timeout! The analyzer device did not show up.') self._device = usb.core.find(idVendor=USB_VENDOR_ID, idProduct=USB_PRODUCT_ID)"}
{"text_id": "8566", "text": "docstring: def to_png(dicom_root_path='.', log_dir=None, verbose=False, cleanup=False): from thicom.converter import Converter Converter(dicom_root_path=dicom_root_path, run=True, log_dir=log_dir, verbose=verbose, cleanup=cleanup)"}
{"text_id": "8567", "text": "docstring: def retype(self, ctxt, volume, new_type, diff, host): def retype_iogrp_property(volume, new, old): if new != old: self._helpers.change_vdisk_iogrp(volume['name'], self._state, (new, old)) LOG.debug('enter: retype: id=%(id)s, new_type=%(new_type)s,' 'diff=%(diff)s, host=%(host)s', {'id': volume['id'], 'new_type': new_type, 'diff': diff, 'host': host}) ignore_keys = ['protocol', 'multipath'] no_copy_keys = ['warning', 'autoexpand', 'easytier'] copy_keys = ['rsize', 'grainsize', 'compression'] all_keys = ignore_keys + no_copy_keys + copy_keys old_opts = self._get_vdisk_params(volume['volume_type_id'], volume_metadata= volume.get('volume_matadata')) new_opts = self._get_vdisk_params(new_type['id'], volume_type=new_type) model_update = None old_type_replication = old_opts.get('replication', False) new_type_replication = new_opts.get('replication', False) if old_type_replication and not new_type_replication: self.replication.delete_replica(volume) model_update = {'replication_status': 'disabled', 'replication_driver_data': None, 'replication_extended_status': None} vdisk_changes = [] need_copy = False for key in all_keys: if old_opts[key] != new_opts[key]: if key in copy_keys: need_copy = True break elif key in no_copy_keys: vdisk_changes.append(key) dest_location = host['capabilities'].get('location_info') if self._stats['location_info'] != dest_location: need_copy = True if need_copy: self._check_volume_copy_ops() dest_pool = self._helpers.can_migrate_to_host(host, self._state) if dest_pool is None: return False if new_type_replication: msg = (_('Unable to retype: Current action needs volume-copy,' ' it is not allowed when new type is replication.' ' Volume = %s'), volume['id']) raise exception.VolumeDriverException(message=msg) retype_iogrp_property(volume, new_opts['iogrp'], old_opts['iogrp']) try: new_op = self.add_vdisk_copy(volume['name'], dest_pool, new_type) self._add_vdisk_copy_op(ctxt, volume, new_op) except exception.VolumeDriverException: retype_iogrp_property(volume, old_opts['iogrp'], new_opts['iogrp']) msg = (_('Unable to retype: A copy of volume %s exists. ' 'Retyping would exceed the limit of 2 copies.'), volume['id']) raise exception.VolumeDriverException(message=msg) else: retype_iogrp_property(volume, new_opts['iogrp'], old_opts['iogrp']) self._helpers.change_vdisk_options(volume['name'], vdisk_changes, new_opts, self._state) if new_opts['qos']: self._helpers.update_vdisk_qos(volume['name'], new_opts['qos']) elif old_opts['qos']: self._helpers.disable_vdisk_qos(volume['name'], old_opts['qos']) if not old_type_replication and new_type_replication: model_update = self.replication.create_replica(ctxt, volume, new_type) LOG.debug('exit: retype: ild=%(id)s, new_type=%(new_type)s,' 'diff=%(diff)s, host=%(host)s', {'id': volume['id'], 'new_type': new_type, 'diff': diff, 'host': host['host']}) return True, model_update"}
{"text_id": "8568", "text": "docstring: def type_text(self, text, force_open_keyboard=False): logger.info(f\"Type text '{text}' into text field '{self.element.name}'\") if force_open_keyboard: self.element.click() self.element.send_keys(text) return self"}
{"text_id": "8569", "text": "docstring: def stop(self): for t in self.threads: t.stop() self.active = False"}
{"text_id": "8570", "text": "docstring: def improve_covar_mat(covar0, ntries=100, max_condition_number=1e12): ndim = covar0.shape[0] covar = np.array(covar0) coeffmin = 1e-10 for trial in range(ntries): failed = False try: am = lalg.pinvh(covar) l, v = lalg.eigh(covar) axes = lalg.cholesky(covar, lower=True) if np.all((l > 0) & np.isfinite(l) ) and l.max() < l.min() * max_condition_number: break else: failed = True except lalg.LinAlgError: failed = True if failed: coeff = coeffmin * (1. / coeffmin)**(trial * 1. / (ntries - 1)) covar = (1. - coeff) * covar + coeff * np.eye(ndim) if failed: warnings.warn(\"Failed to guarantee the ellipsoid axes will be \" \"non-singular. Defaulting to a sphere.\") covar = np.eye(ndim) am = lalg.pinvh(covar) axes = lalg.cholesky(covar, lower=True) return covar, am, axes"}
{"text_id": "8571", "text": "docstring: def empty(self) -> list: self.emptyTime = timezone.localize(datetime.now()) self.updateUsageTime(self.assignmentTime, self.emptyTime) consignments = self.consignments self.volumeLeft = self.volume self.status = TruckStatus.AVAILABLE self.branchID = None self.dstBranchID = None self.consignments = [] for consignment in consignments: if self in consignment.trucks: consignment.trucks.remove(self) return consignments"}
{"text_id": "8572", "text": "docstring: def _subdirs(self, path): raise NotImplementedError"}
{"text_id": "8573", "text": "docstring: def _handler(self, queue, *args, **kwargs): self.last_recived_time[queue] = time.time() super()._handler(queue, *args, **kwargs)"}
{"text_id": "8574", "text": "docstring: def locate_gitattributes(scope=None): if scope == 'global': try: bpath = check_output(['git', 'config', '--global', 'core.attributesfile']) gitattributes = os.path.expanduser(bpath.decode('utf8', 'replace').strip()) except CalledProcessError: if os.environ.get('XDG_CONFIG_HOME'): gitattributes = os.path.expandvars('$XDG_CONFIG_HOME/git/attributes') else: gitattributes = os.path.expanduser('~/.config/git/attributes') elif scope == 'system': try: env = os.environ.copy() env['GIT_EDITOR'] = 'echo' bpath = check_output(['git', 'config', '--system', '-e'], env=env) gitconfig = bpath.decode('utf8', 'replace').strip() gitattributes = os.path.join(os.path.dirname(gitconfig), 'gitattributes') except CalledProcessError: if not os.path.exists('/etc'): raise EnvironmentError('Could not find system gitattributes location!') gitattributes = os.path.join(['etc', 'gitattributes']) else: path = os.path.abspath('.') if not os.path.exists(os.path.join(path, '.git')): return None gitattributes = os.path.join(path, '.gitattributes') return gitattributes"}
{"text_id": "8575", "text": "docstring: def _check_tesseroids(tesseroids): west, east, south, north, bottom, top = tuple(tesseroids[:, i] for i in range(6)) err_msg = \"Invalid tesseroid or tesseroids. \" if (west > east).any(): err_msg += \"The west boundary can't be greater than the east one.\\n\" for tess in tesseroids[west > east]: err_msg += \"\\tInvalid tesseroid: {}\\n\".format(tess) raise ValueError(err_msg) if (south > north).any(): err_msg += \"The south boundary can't be greater than the north one.\\n\" for tess in tesseroids[south > north]: err_msg += \"\\tInvalid tesseroid: {}\\n\".format(tess) raise ValueError(err_msg) if (bottom < 0).any() or (top < 0).any(): err_msg += \"The bottom and top radii couldn't be lower than zero.\\n\" for tess in tesseroids[np.logical_or(bottom < 0, top < 0)]: err_msg += \"\\tInvalid tesseroid: {}\\n\".format(tess) raise ValueError(err_msg) if (bottom > top).any(): err_msg += \"The bottom radius boundary can't be greater than the top one.\\n\" for tess in tesseroids[bottom > top]: err_msg += \"\\tInvalid tesseroid: {}\\n\".format(tess) raise ValueError(err_msg)"}
{"text_id": "8576", "text": "docstring: def protoGet(self, dQuery, verbose=False): if self.bStub: self.load() if 'protocol' not in self.props: raise CatalogError(self.url, \"'protocol' not present is data source definition\") dProto = self.props['protocol'] if ('base_urls' not in dProto) or \\ (not isinstance(dProto['base_urls'], list)): raise CatalogError(self.url, \"'base_urls not present in protocol section, or is not a list\") if len(self.lBadBase) == len(dProto['base_urls']): self.lBadBase = [] lDs = None for i in range(0, len(dProto['base_urls'])): sBaseUrl = dProto['base_urls'][i] if sBaseUrl in self.lBadBase: continue sJoin = '' if sBaseUrl.find('?') == -1: sJoin = \"?\" else: sJoin = '&' lParams = [] for k in dQuery: if type(dQuery[k]) == str: sParam = \"%s=%s\"%(k, quote_plus(dQuery[k])) else: sParam = \"%s=%s\"%(k, dQuery[k]) lParams.append(sParam) sGet = \"&\".join(lParams) sUrl = \"%s%s%s\"%(sBaseUrl, sJoin, sGet) try: if verbose: perr(\"Requesting: %s\\n\"%sUrl) lDs = _das2.read_server(sUrl) except Exception as e: sys.stderr.write(\"Couldn't read URL '%s', %s\\n\"%(sUrl, str(e))) self.lBadBase.append(sBaseUrl) if lDs != None: lOut = [] for ds in lDs: lOut.append(ds_from_raw(ds)) return lOut raise SourceError(sUrl, \"Unable to retrieve data\")"}
{"text_id": "8577", "text": "docstring: def smooth_l1(sigma=3.0): sigma_squared = sigma ** 2 def _smooth_l1(y_true, y_pred): regression = y_pred regression_target = y_true[:, :, :-1] anchor_state = y_true[:, :, -1] indices = backend.where(keras.backend.equal(anchor_state, 1)) regression = backend.gather_nd(regression, indices) regression_target = backend.gather_nd(regression_target, indices) regression_diff = regression - regression_target regression_diff = keras.backend.abs(regression_diff) regression_loss = backend.where( keras.backend.less(regression_diff, 1.0 / sigma_squared), 0.5 * sigma_squared * keras.backend.pow(regression_diff, 2), regression_diff - 0.5 / sigma_squared ) normalizer = keras.backend.maximum(1, keras.backend.shape(indices)[0]) normalizer = keras.backend.cast( normalizer, dtype=keras.backend.floatx()) return keras.backend.sum(regression_loss) / normalizer return _smooth_l1"}
{"text_id": "8578", "text": "docstring: def scale(self, data: Union[ndarray, float], unit: str) -> Tuple[Union[ndarray, float], str]: if self.par['switches']['dimensional_units'] or \\ not conf.scaling.dimensional or \\ unit == '1': return data, '' scaling = phyvars.SCALES[unit](self.scales) factor = conf.scaling.factors.get(unit, ' ') if conf.scaling.time_in_y and unit == 's': scaling /= conf.scaling.yearins unit = 'yr' elif conf.scaling.vel_in_cmpy and unit == 'm/s': scaling *= 100 * conf.scaling.yearins unit = 'cm/y' if factor in phyvars.PREFIXES: scaling *= 10**(-3 * (phyvars.PREFIXES.index(factor) + 1)) unit = factor + unit return data * scaling, unit"}
{"text_id": "8579", "text": "docstring: async def _trace(self, fut): try: n_hopsplore = self._connection_status.n_hops + self._explore_hops async with mtrpacket.MtrPacket() as mtr: async def probe_me(ttl): probe = await mtr.probe(self._target, protocol='udp',port=2000, ttl=ttl, timeout=1) return (ttl, probe) tasks = [probe_me(i) for i in range(2, n_hopsplore)] res = await asyncio.gather(*tasks) lats = [] for i in range(n_hopsplore-2): if self._debug_is_enabled: print('it: {0} with result: {1}'.format(i, res[i][1].result)) print('lats: {0}'.format(lats)) r = res[i][1] if r.result == 'no-reply' or r.success or i == n_hopsplore -3: lats = [[r.time_ms * T.MS2US] if not len(lats) else lats][0] self._connection_status.set_trace(lats) if self._debug_is_enabled: print('Trace result:\\n{}'.format(self._connection_status)) return True else: lats += [res[i][1].time_ms * T.MS2US] except Exception as e: print('Error message: {0}'.format(e)) self._request_active[STATUS.TRACE] = False"}
{"text_id": "8580", "text": "docstring: def description(self): raise NotImplementedError(\"Derived classes must implement `description`\")"}
{"text_id": "8581", "text": "docstring: def bot_command_portfolio(self, update: Updater, context: CallbackContext) -> None: if not self.authenticate(update): return username = update.message.from_user[\"username\"] print(username, \" requested portfolio\") if len(context.args) < 1: update.message.reply_text(text=\"Use Syntax: \\n`/portfolio coin (optional: period)`\", parse_mode=\"Markdownv2\") return coin = context.args[0] period = \"month\" try: period = context.args[1] except IndexError: pass pil_image = self.price_graph.portfolio_price_graph(coin=coin, period=period) buffer = io.BytesIO() buffer.name = 'image.jpeg' pil_image.save(buffer, 'JPEG') buffer.seek(0) context.bot.send_photo(update.effective_chat.id, photo=buffer)"}
{"text_id": "8582", "text": "docstring: def job_monitor(self): if 'job_monitor' not in self.moduleInfo or \\ not self.moduleInfo['job_monitor']: raise ModuleError(self, \"Cannot run job, no job_monitor is specified!\") return self.moduleInfo['job_monitor']"}
{"text_id": "8583", "text": "docstring: def autosave(self): super().autosave() try: self.diff_window.diff_text.setText('\\n'.join(get_diffed_text())) except AttributeError: pass"}
{"text_id": "8584", "text": "docstring: def fetch_JHU_daily(date_start, date_end, countries=None, columns=None, progress=True): n = len(pd.date_range(date_start, date_end)) return pd.concat( tqdm( iter_JHU_daily(date_start, date_end, countries=countries, columns=columns), total=n, disable=not progress, ), ignore_index=True )"}
{"text_id": "8585", "text": "docstring: async def afk_on_pm(sender): global ISAFK global USERS global COUNT_MSG global COUNT_MSG global USERS global ISAFK global USER_AFK global afk_time global afk_start global afk_end back_alivee = datetime.now() afk_end = back_alivee.replace(microsecond=0) afk_since = \"algum tempo atr\u00e1s\" if ( sender.is_private and sender.sender_id != 777000 and not (await sender.get_sender()).bot ): if PM_AUTO_BAN: try: from userbot.modules.sql_helper.pm_permit_sql import is_approved apprv = is_approved(sender.sender_id) except AttributeError: apprv = True else: apprv = True if apprv and ISAFK: now = datetime.now() datime_since_afk = now - afk_time time = float(datime_since_afk.seconds) days = time // (24 * 3600) time = time % (24 * 3600) hours = time // 3600 time %= 3600 minutes = time // 60 time %= 60 seconds = time if days == 1: afk_since = \"Ontem\" elif days > 1: if days > 6: date = now + datetime.timedelta( days=-days, hours=-hours, minutes=-minutes ) afk_since = date.strftime(\"%A, %Y %B %m, %H:%I\") else: wday = now + datetime.timedelta(days=-days) afk_since = wday.strftime(\"%A\") elif hours > 1: afk_since = f\"`{int(hours)}h{int(minutes)}m`\" elif minutes > 0: afk_since = f\"`{int(minutes)}m{int(seconds)}s`\" else: afk_since = f\"`{int(seconds)}s`\" if AFKREASON: await sender.reply( f\"Estou ausente fazem {afk_since}.\\ \\nRaz\u00e3o: `{AFKREASON}`\" ) else: await sender.reply(str(choice(AFKSTR))) USERS.update({sender.sender_id: 1}) COUNT_MSG = COUNT_MSG + 1"}
{"text_id": "8586", "text": "docstring: def date_to_str(d): return d.strftime(DATE_FORMAT)"}
{"text_id": "8587", "text": "docstring: def scale_well_geometry(ww): ww = ww.copy() if \"rel\" in coord_type: s = 1/model.Lx, 1/model.Ly elif \"abs\" in coord_type: s = 1, 1 elif \"ind\" in coord_type: s = model.Nx/model.Lx, model.Ny/model.Ly else: raise ValueError(\"Unsupported coordinate type: %s\" % coord_type) ww[:, :2] = ww[:, :2] * s return ww"}
{"text_id": "8588", "text": "docstring: def tensor_product_blocks_udimensions(self): raise NotImplementedError(\"Derived classes should implement this!\")"}
{"text_id": "8589", "text": "docstring: def run(self): return yield None"}
{"text_id": "8590", "text": "docstring: def patch_bq_client_with_querius_client(bq_client: Client, qs_client: QueriusClient) -> Client: orig_query_method = bq_client.query def query(query_str: str, *query_args, **query_kwargs) -> QueryJob: if query_kwargs.get('project') is not None: logger.warning(\"'project' argument passed to query method, not routing.\") return orig_query_method(query_str, *query_args, **query_kwargs) project, request_id = qs_client.route(query_str) query_kwargs['project'] = project qj = orig_query_method(query_str, *query_args, **query_kwargs) orig_result_method = qj.result def post_log_to_querius_and_get_result(*result_args, **result_kwargs): qj.result = orig_result_method result = qj.result(*result_args, **result_kwargs) qs_client.log_query_stats(qj, request_id) return result qj.result = post_log_to_querius_and_get_result return qj bq_client.query = query return bq_client"}
{"text_id": "8591", "text": "docstring: def _log_parameters_list(self) -> None: model_parameters = filter(lambda p: p.requires_grad, self.model.parameters()) n_params = sum([np.prod(p.size()) for p in model_parameters]) logger.info(\"Total params: %d\", n_params) trainable_params = [n for (n, p) in self.model.named_parameters() if p.requires_grad] logger.debug(\"Trainable parameters: %s\", sorted(trainable_params)) assert trainable_params"}
{"text_id": "8592", "text": "docstring: def _initialize_data(self): self._temps = [1.0] * NUMBER_OF_TEMP_CHANNELS self._sensors = [0.5] * NUMBER_OF_SENSOR_CHANNELS self.temp_all = \"\" self.sensor_all = \"\" self.connected = True"}
{"text_id": "8593", "text": "docstring: def list_follower_databases( self, resource_group_name: str, cluster_name: str, **kwargs ) -> AsyncIterable[\"_models.FollowerDatabaseListResult\"]: cls = kwargs.pop('cls', None) error_map = { 401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError } error_map.update(kwargs.pop('error_map', {})) api_version = \"2020-09-18\" accept = \"application/json\" def prepare_request(next_link=None): header_parameters = {} header_parameters['Accept'] = self._serialize.header(\"accept\", accept, 'str') if not next_link: url = self.list_follower_databases.metadata['url'] path_format_arguments = { 'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'), 'clusterName': self._serialize.url(\"cluster_name\", cluster_name, 'str'), 'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'), } url = self._client.format_url(url, **path_format_arguments) query_parameters = {} query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str') request = self._client.post(url, query_parameters, header_parameters) else: url = next_link query_parameters = {} request = self._client.get(url, query_parameters, header_parameters) return request async def extract_data(pipeline_response): deserialized = self._deserialize('FollowerDatabaseListResult', pipeline_response) list_of_elem = deserialized.value if cls: list_of_elem = cls(list_of_elem) return None, AsyncList(list_of_elem) async def get_next(next_link=None): request = prepare_request(next_link) pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs) response = pipeline_response.http_response if response.status_code not in [200]: map_error(status_code=response.status_code, response=response, error_map=error_map) raise HttpResponseError(response=response, error_format=ARMErrorFormat) return pipeline_response return AsyncItemPaged( get_next, extract_data )"}
{"text_id": "8594", "text": "docstring: async def login(id: str, pw: str, _type: str): global client_login if client_login == 0 and _type == \"client\": return {\"message\": \"Client login not allowed.\"} pwh = hashlib.sha256(pw.encode()).hexdigest() login_user = db.login(id, pwh, _type) if login_user: session_hash = str(uuid4()) sessions[session_hash] = [datetime.today(), _type] return { \"id\": login_user[0], \"user_id\": login_user[1], \"active\": login_user[4], \"session\": session_hash} else: return {\"message\": \"Invalid id/password\"}"}
{"text_id": "8595", "text": "docstring: def count(id, contenttype='submission'): if contenttype == 'submission': return d.engine.scalar( 'SELECT COUNT(*) FROM comments cm WHERE cm.target_sub = %s', (id,)) elif contenttype == 'journal': tablename = 'journalcomment' elif contenttype == 'character': tablename = 'charcomment' else: raise ValueError(\"type should be one of 'submission', 'journal', or 'character'\") return d.engine.scalar( 'SELECT COUNT(*) FROM {table} cm WHERE cm.targetid = %s'.format(table=tablename), (id,))"}
{"text_id": "8596", "text": "docstring: def export_env(env_file: pathlib.Path): with open(env_file) as f: for line in f.readlines(): line = line.strip() if line and line[0] != '#': key, value = line.split('=') os.environ[key] = value"}
{"text_id": "8597", "text": "docstring: def common_setlegmode(request, db, user): data = request.json legid = data[\"id\"] legact = data[\"activity\"] legline = data.get(\"line_name\") or None if legact not in mass_transit_types: legline = None devices = db.metadata.tables[\"devices\"] users = db.metadata.tables[\"users\"] legs = db.metadata.tables[\"legs\"] modes = db.metadata.tables[\"modes\"] leg = db.engine.execute(select( [legs.c.device_id, legs.c.time_start, legs.c.time_end], and_(devices.c.user_id == user, legs.c.id == legid), from_obj=devices.join(users).join(legs))).first() if not leg: abort(403) values = {\"leg\": legid, \"source\": \"USER\", \"mode\": legact, \"line\": legline} existing = db.engine.execute(modes.select().where(and_( modes.c.leg == legid, modes.c.source == \"USER\"))).first() if existing: where = modes.c.id == existing.id if legact is None: db.engine.execute(modes.delete().where(where)) else: db.engine.execute(modes.update().where(where).values(values)) elif legact is not None: db.engine.execute(modes.insert().values(values)) update_user_distances(user, leg.time_start, leg.time_end) return leg.device_id, legid, legact, legline"}
{"text_id": "8598", "text": "docstring: def query_esri_mapserver(base_url, layer_id, out_fields=None, where=None): url = base_url + str(layer_id) + '/query' params = {'returnGeometry': 'true', 'f': 'geojson'} if out_fields is None: params.update({'outFields': '*'}) elif isinstance(out_fields, list): out_str = ', '.join(out_fields) params.update({'outFields': out_str}) else: raise ValueError('out_fields must either be None or a list of strings') if where is None: params.update({'where': '1=1'}) elif isinstance(where, str): params.update({'where': where}) else: raise ValueError('where must either be None or a string') resp = requests.get(url, params=params) if not resp.ok: raise ValueError(resp.raise_for_status()) data = orjson.loads(resp.content) if 'exceededTransferLimit' in data: exceed = data.pop('exceededTransferLimit') else: exceed = False data_all = copy.deepcopy(data) while exceed: last_id = data['features'][-1]['id'] if isinstance(where, str): params.update({'where': where + 'and objectid > ' + str(last_id)}) else: params.update({'where': 'objectid > ' + str(last_id)}) resp = requests.get(url, params=params) if not resp.ok: raise ValueError(resp.raise_for_status()) data = orjson.loads(resp.content) data_all['features'].extend(data['features']) if 'exceededTransferLimit' in data: exceed = data.pop('exceededTransferLimit') else: exceed = False return data_all"}
{"text_id": "8599", "text": "docstring: def _compute_cofactor_interactor_cmaps(self): self.all_resid_inds_= self._get_resid_indices(self.n_residues_,query='!(type H)') for ind in self.cofactor_interactor_atom_indices_: self.all_resid_inds_.append(ind) self.n_interactor_residues_ = len(self.cofactor_interactor_atom_indices_) print('Computing interactor node contact maps.') self.cofactor_interactor_protein_cmap_ = np.zeros((self.n_interactor_residues_,self.n_residues_+self.n_interactor_residues_)) for i_resid in range(self.n_residues_,self.n_residues_+self.n_interactor_residues_): self._cofactor_interactor_cmap_loop(i_resid) print() np.save(self.out_directory_+'cofactor_protein_residue_semi_binary_cmap_'+self.file_end_name_+'.npy',self.cofactor_interactor_protein_cmap_) print('cofactor-protein interactor contact map written to file: ') print(self.out_directory_+'cofactor_protein_residue_semi_binary_cmap_'+self.file_end_name_+'.npy') return"}
{"text_id": "8600", "text": "docstring: def write_parquet(hdfs_filename, dataframe, **kwds): hdfs_path = hdfs._expand_path(hdfs_filename, exists=False) h = hdfs.get_fs() with h.open_file(hdfs_path, \"wb\") as f: dataframe.to_parquet(f, **kwds)"}
{"text_id": "8601", "text": "docstring: def HotKeyText(InterfacePath: str) -> None: pass"}
{"text_id": "8602", "text": "docstring: def envelopeless_front_end(self, signal): return super().front_end(signal)"}
{"text_id": "8603", "text": "docstring: def fetch_file(url, filename): filepath = Path(filename) if filepath.is_file(): LOGGER.info(\"File exists, not overwriting\") return cmd = [ \"aria2c\", \"-q\", \"-d\", filepath.parent, \"-o\", filepath.name, url, ] run(cmd)"}
{"text_id": "8604", "text": "docstring: def buildIdTable(self): self.setIntType() returnt = \"DROP TABLE IF EXISTS tmp;\\n\\n\" returnt += \"CREATE TABLE tmp ENGINE=MYISAM SELECT %(field)s,count(*) as count FROM %(table)s GROUP BY %(field)s;\\n\\n\" % self.__dict__ returnt += \"\"\"CREATE TABLE IF NOT EXISTS %(field)s__id ( %(field)s__id %(intType)s PRIMARY KEY AUTO_INCREMENT, %(field)s VARCHAR (255), INDEX (%(field)s), %(field)s__count MEDIUMINT);\\n\\n\"\"\" % self.__dict__ returnt += \"\"\"INSERT INTO %(field)s__id (%(field)s,%(field)s__count) SELECT %(field)s,count FROM tmp LEFT JOIN %(field)s__id USING (%(field)s) WHERE %(field)s__id.%(field)s__id IS NULL ORDER BY count DESC;\\n\\n\"\"\" % self.__dict__ returnt += \"\"\"DROP TABLE tmp;\\n\\n\"\"\" self.idCode = \"%s__id\" % self.field return returnt"}
{"text_id": "8605", "text": "docstring: def import_font(filepath, url): if url == \"unifont\": url = \"http://unifoundry.com/pub/unifont/unifont-14.0.01/font-builds/unifont-14.0.01.ttf\" filename, extension = \"unifont.ttf\", \"ttf\" else: filename, extension = url.split(\"/\")[-1].rsplit(\".\", 1) if extension not in (\"ttf\", \"gz\", \"zip\"): return \"Unsupported font extension: %s\" % extension filename = \"%s.ttf\" % filename font_path = os.path.join(current.request.folder, \"static\", \"fonts\") if os.path.exists(os.path.join(font_path, filename)): current.log.warning(\"Using cached copy of %s\" % filename) return None cwd = os.getcwd() os.chdir(font_path) try: stream = fetch(url) except URLError as e: os.chdir(cwd) return str(e) try: if extension == \"gz\": import tarfile tf = tarfile.open(fileobj=stream) tf.extractall() elif extension == \"zip\": import zipfile zf = zipfile.ZipFile(stream) zf.extractall() else: f = open(filename, \"wb\") f.write(stream) f.close() finally: os.chdir(cwd) return None"}
{"text_id": "8606", "text": "docstring: def CopyFileBetweenVms(filename, src_vm, src_path, dest_vm, dest_path): with tempfile.NamedTemporaryFile() as tf: temp_path = tf.name src_vm.RemoteCopy( temp_path, os.path.join(src_path, filename), copy_to=False) dest_vm.RemoteCopy( temp_path, os.path.join(dest_path, filename), copy_to=True)"}
{"text_id": "8607", "text": "docstring: def _readRaw(self, maxage=0): raise ProgrammingError(self, 'Somebody please implement a proper ' '_readRaw or doRead method!')"}
{"text_id": "8608", "text": "docstring: def reset(self): if self.check_trial_done(): self.end() else: self.agent.reset() if self.outfile: self.outfile.close() if self.config.get('s3upload'): self.pipe.send({'upload':{'projectId':self.projectId ,'userId':self.userId,'file':self.filename,'path':self.path, 'bucket': self.config.get('bucket')}}) self.create_file() self.episode += 1"}
{"text_id": "8609", "text": "docstring: def _get_all_info_states(state: pyspiel.State, oracle: dict, state_data: dict, include_terminal_states: bool) -> None: print(f'Number of states: {len(state_data)}', end='\\r') if state.is_terminal() and not include_terminal_states: return info_tuple = ( state.information_state_string(0), state.information_state_string(1), ) if info_tuple in oracle: return oracle[info_tuple] = True cur_player = state.current_player() if cur_player < 0: cur_player = 0 if state.returns()[0] > 0 else 1 cur_is = state.information_state_string(cur_player) if cur_is not in state_data: state_data[cur_is] = state for action in state.legal_actions(): new_state = state.child(action) _get_all_info_states(new_state, oracle, state_data, include_terminal_states)"}
{"text_id": "8610", "text": "docstring: def save_image_single(img, filename): img = img.detach().cpu().numpy() img = img.transpose(1, 2, 0) plt.imsave(filename, img)"}
{"text_id": "8611", "text": "docstring: def process(self, enemies, surface): for e in enemies: for f in enemies: if e is not f: dist = e.distance_to(Vector2(f.x, f.y)) if dist < self.threshold: v = (Vector2(e.x, e.y)-Vector2(f.x, f.y)).normalised() e.move_rel(v.x * self.increment, v.y * self.increment, surface)"}
{"text_id": "8612", "text": "docstring: def read_file(myfile): try: with open(myfile, 'r') as fhandle: data = fhandle.read() return data except IOError: raise exceptions.FailedExecution( 'Unable to read file {0}'.format(myfile) )"}
{"text_id": "8613", "text": "docstring: def cmap_to_dict(cmap_ptr): map_len = cmap_ptr.contents.length _map = dict() if map_len != 0: for i in range(map_len): el = cmap_ptr.contents.elements[i] _map[el.key.decode(encoding=\"utf-8\")] = el.value.decode(encoding=\"utf-8\") return _map"}
{"text_id": "8614", "text": "docstring: def endElement(self, name): eltext = ''.join(self.curtext) if self.curtext else '' self.curtext = None self.curpath.pop() attrs = self.curattrs.pop() if name == 'title': self.title = eltext elif name == 'id' and self.curpath[-1] == 'page': self.id = eltext elif name == 'redirect': self.redirect = True elif name == 'namespace': key = attrs.getValue(\"key\") if debug['sax']: errprint(\"Saw namespace, key=%s, eltext=%s\" % (key, eltext)) article_namespaces[eltext] = key article_namespaces_lower[eltext.lower()] = key elif name == 'text': if debug['lots']: max_text_len = 150 endslice = min(max_text_len, len(eltext)) truncated = len(eltext) > max_text_len errprint( \"\"\"Calling process_article_text with title=%s, id=%s, redirect=%s; text=[%s%s]\"\"\" % (self.title, self.id, self.redirect, eltext[0:endslice], \"...\" if truncated else \"\")) self.output_handler.process_article_text(text=eltext, title=self.title, id=self.id, redirect=self.redirect) if self.status.item_processed(maxtime=Opts.max_time_per_stage): raise FinishParsing()"}
{"text_id": "8615", "text": "docstring: def __get_queue_stream_index(self) -> Tuple[int, int]: queue_index = 0 elapsed_time_queue = self.player.elapsed_time total_time = 0 track_time = 0 if self.items and len(self.items) > self._queue_stream_start_index: queue_index = ( self._queue_stream_start_index ) queue_track = None while len(self.items) > queue_index: queue_track = self.items[queue_index] if elapsed_time_queue > (queue_track.duration + total_time): total_time += queue_track.duration queue_index += 1 else: track_time = elapsed_time_queue - total_time break return queue_index, track_time"}
{"text_id": "8616", "text": "docstring: def filename(self) -> str: return self._doc_field_dict[\"filename\"]"}
{"text_id": "8617", "text": "docstring: def insert_values(self,tableName : str,columnNames : str, valuesToInsert: str): try: dataBase = sqlite3.connect(f'{self.databaseName}.DB') cursor = dataBase.cursor() query = 'INSERT INTO ' + tableName + f'({columnNames}) ' + f'VALUES ({valuesToInsert})' cursor.execute(query) dataBase.commit() dataBase.close() return 'Values Inserted!' except Exception as error: raise errorTypes.insertingError(error)"}
{"text_id": "8618", "text": "docstring: def next_url(url: str, current_cursor: int) -> str: return str(yarl.URL(url) % {\"cursor\": current_cursor + 1})"}
{"text_id": "8619", "text": "docstring: async def post_add_admin_role(request: requests.Request) -> responses.Response: payload = add_admin_role.AddAdminRole.parse_obj(await request.json()) add_request_to_collection(request, payload) return PydanticResponse(APIResponse[bool](result=True))"}
{"text_id": "8620", "text": "docstring: def B_star(f_exc,phi): return scipy.optimize.brentq(\\ lambda B: theta_tilde(f_exc,B,phi,1.0,1.0,lambda Q,llambda: 1.0),\\ 0,10.)"}
{"text_id": "8621", "text": "docstring: def new_group(ranks=None, timeout=default_pg_timeout, backend=None): _check_default_pg() global _pg_group_ranks default_backend, default_store = _pg_map[_default_pg] global_rank = _default_pg.rank() global_world_size = _default_pg.size() if not backend: backend = default_backend if ranks is not None: ranks = sorted(ranks) group_world_size = len(ranks) if group_world_size > global_world_size: raise RuntimeError(\"the new group's world size should be less or \" \"equal to the world size set by \" \"init_process_group\") for rank in ranks: if rank < 0 or rank >= global_world_size: raise RuntimeError(\"The new group's rank should be within the \" \"the world_size set by init_process_group\") if global_rank in ranks: group_rank = ranks.index(global_rank) else: group_rank = None else: ranks = list(range(global_world_size)) group_world_size = global_world_size group_rank = global_rank backend = Backend(backend) pg = _new_process_group_helper(group_world_size, group_rank, ranks, backend, default_store, timeout=timeout) _pg_group_ranks[pg] = { global_rank: group_rank for group_rank, global_rank in enumerate(ranks) } return pg"}
{"text_id": "8622", "text": "docstring: def init(self, session, weight_files=None, exclude_var_list=None): if exclude_var_list is None: exclude_var_list = list() import pickle if weight_files is None: weight_files = ['./weights/handsegnet-rhd.pickle', './weights/posenet3d-rhd-stb-slr-finetuned.pickle'] for file_name in weight_files: assert os.path.exists(file_name), \"File not found.\" with open(file_name, 'rb') as fi: weight_dict = pickle.load(fi) weight_dict = {k: v for k, v in weight_dict.items() if not any([x in k for x in exclude_var_list])} if len(weight_dict) > 0: init_op, init_feed = tf.contrib.framework.assign_from_values(weight_dict) session.run(init_op, init_feed) print('Loaded %d variables from %s' % (len(weight_dict), file_name))"}
{"text_id": "8623", "text": "docstring: def delete_view(self, name): self.gxmap.delete_view(self.classview(name))"}
{"text_id": "8624", "text": "docstring: def testFromListRaisesException(self): meta_dict = MetaDict({'a': MetaDict({'b': 1, 'c': 3}), 'd': 2, 'e': 0}) serializer = GetSerializer(meta_dict) self.assertRaisesRegexp( Exception, 'The element list should have the same length as serializer', serializer.FromList, [], MockedMetaDict)"}
{"text_id": "8625", "text": "docstring: def _collect_fields(self): for f in itervalues(self.properties.to_dict()): yield f if hasattr(f, \"fields\"): for inner_f in itervalues(f.fields.to_dict()): yield inner_f if hasattr(f, \"_collect_fields\"): for inner_f in f._collect_fields(): yield inner_f"}
{"text_id": "8626", "text": "docstring: def clear(self): globs = globals() for k in self: k = _mat_db_global(k) if k in globs: del globs[k]"}
{"text_id": "8627", "text": "docstring: def dir_clone(self, dir_path): if not os.path.exists(dir_path): self.stdout.write(\"Path %s does not exist !!\" % dir_path) return None tempdir = self.create_session() if tempdir is not None: os.system(\"cp %s/*.yang %s\" % (dir_path, tempdir)) return tempdir"}
{"text_id": "8628", "text": "docstring: def compute_frames_to_record( this_frame: FrameType, ) -> List[Tuple[FrameType, int, FrameType]]: frames: List[Tuple[FrameType, int]] = [ ( cast(FrameType, sys._current_frames().get(cast(int, t.ident), None)), cast(int, t.ident), ) for t in threading.enumerate() if t != threading.main_thread() ] frames.insert( 0, ( sys._current_frames().get( cast(int, threading.main_thread().ident), None ), cast(int, threading.main_thread().ident), ), ) new_frames: List[Tuple[FrameType, int, FrameType]] = [] for (frame, tident) in frames: orig_frame = frame if not frame: continue fname = frame.f_code.co_filename if not fname: back = cast(FrameType, frame.f_back) fname = Filename(back.f_code.co_filename) while not Scalene.should_trace(fname): if frame: frame = cast(FrameType, frame.f_back) if frame: fname = frame.f_code.co_filename if frame: new_frames.append((frame, tident, orig_frame)) return new_frames"}
{"text_id": "8629", "text": "docstring: def check_session_id(): if 'uuid' not in session: session['uuid'] = str(uuid.uuid4())"}
{"text_id": "8630", "text": "docstring: def data_group_type(group_data: Union[dict, GroupType]) -> Tuple[dict, dict]: file_data = {} if isinstance(group_data, dict): file_content = group_data.pop('fileContent', None) if file_content is not None: file_data = { 'fileContent': file_content, 'fileName': group_data.get('fileName'), 'type': group_data.get('type'), } else: if group_data.data.get('type') in ['Document', 'Report']: file_data = group_data.file_data group_data = group_data.data return file_data, group_data"}
{"text_id": "8631", "text": "docstring: def tuple(tensors, name=None, control_inputs=None): with ops.op_scope(tensors, name, \"tuple\") as name: gating_ops = [t.op for t in tensors if t is not None] if control_inputs: for c in control_inputs: if isinstance(c, ops.Tensor): c = c.op elif not isinstance(c, ops.Operation): raise TypeError(\"Control input must be Operation or Tensor: %s\" % c) gating_ops.append(c) gating_ops = sorted(set(gating_ops), key=lambda op: op._id) if not gating_ops: raise ValueError(\"Must have at least one Tensor: %s\" % tensors) gate = group(*gating_ops) tpl = [] for t in tensors: if t is not None: tpl.append(with_dependencies([gate], t)) else: tpl.append(None) return tpl"}
{"text_id": "8632", "text": "docstring: def update_cost_limit(self, cost_limit): self.cost_limit = cost_limit for i in range(len(self.nodes)): if self.nodes[i][0] >= -self.cost_limit: self.nodes = self.nodes[i:] break"}
{"text_id": "8633", "text": "docstring: def _core_filename(self, inter_filename=None): time_tag = get_time_for_filename(self.production_datetime) return f\"{self.PROJECT}_{self.LEVEL}_{self.NAME}_\" \\ f\"{time_tag}_{str(self.runconfig.product_counter).zfill(3)}\""}
{"text_id": "8634", "text": "docstring: def confirm_entry_conversion(self, entry): if not self.caller.ndb.confirm_msg_convert: self.caller.ndb.confirm_msg_convert = entry self.msg(\"{rEntry selected for conversion. To convert, repeat command. Otherwise cancel.\") self.msg(\"{rSelected entry:{n %s\" % entry.db_message) return False if self.caller.ndb.confirm_msg_convert != entry: self.msg(\"{rEntries did not match.\") self.msg(\"{rSelected originally:{n %s\" % self.caller.ndb.confirm_msg_convert.db_message) self.msg(\"{rSelected this time:{n %s\" % entry.db_message) self.msg(\"Previous selection cleared. You can select it again, for reals this time, then confirm.\") self.caller.ndb.confirm_msg_convert = None return False return True"}
{"text_id": "8635", "text": "docstring: def do_send(gen, past=None): try: cross = gen.send(past) except StopIteration: cross = None return cross"}
{"text_id": "8636", "text": "docstring: def clearMemory(): pathsList = sys.path[:] for index, path in enumerate(pathsList[::-1]): if MODULENAME in path.lower(): sys.path.remove(path) for module in list(sys.modules): if MODULENAME in module: del sys.modules[module]"}
{"text_id": "8637", "text": "docstring: def initialise_or_create_database_at(db_file_with_abs_path: str) -> None: qcodes.config.core.db_location = db_file_with_abs_path initialise_database()"}
{"text_id": "8638", "text": "docstring: def _get_governor(lcpu): with contextlib.suppress(IOError): file_name = (\"/sys/devices/system/cpu/cpufreq/\" \"policy{}/scaling_governor\".format(lcpu)) return from_file(file_name) with contextlib.suppress(IOError): file_name = (\"/sys/devices/system/cpu/cpu{}/cpufreq/\" \"scaling_governor\".format(lcpu)) return from_file(file_name) return None"}
{"text_id": "8639", "text": "docstring: def removeLiquidity_Q(params, substep, state_history, prev_state, policy_input): asset_id = policy_input['asset_id'] total_liquidity = int(prev_state['UNI_S' + asset_id]) Q_reserve = int(prev_state['UNI_Q' + asset_id]) amount = int(policy_input['UNI_burn']) q_amount = int(amount * Q_reserve // total_liquidity) if params['ENABLE_SYMMETRIC_LIQ']: return ('UNI_Q' + asset_id, int(Q_reserve - q_amount)) else: return ('UNI_Q' + asset_id, Q_reserve)"}
{"text_id": "8640", "text": "docstring: def paste_server_selector(wsgi_app, global_config=None, **app_config): try: import gunicorn except ImportError: gunicorn = None if gunicorn is None: from paste.httpserver import server_runner cleaned_app_config = dict( [(key, app_config[key]) for key in app_config if key in [\"host\", \"port\", \"handler\", \"ssl_pem\", \"ssl_context\", \"server_version\", \"protocol_version\", \"start_loop\", \"daemon_threads\", \"socket_timeout\", \"use_threadpool\", \"threadpool_workers\", \"threadpool_options\", \"request_queue_size\"]]) return server_runner(wsgi_app, global_config, **cleaned_app_config) else: from gunicorn.app.pasterapp import PasterServerApplication return PasterServerApplication(wsgi_app, global_config, **app_config)"}
{"text_id": "8641", "text": "docstring: def selection_tool(self, selection_file, mode): ret_val = self._selection_tool(selection_file.encode(), mode) return ret_val"}
{"text_id": "8642", "text": "docstring: def compact(self): builder = SegmentBuilder() pos = builder.allocate(8) builder.copy_from_struct(pos, Struct, self) buf = builder.as_string() t = type(self) res = t.__new__(t) res._init_from_buffer(buf, 8, self._data_size, self._ptrs_size) return res"}
{"text_id": "8643", "text": "docstring: def _update_config(self, restart_on_failure: bool = True) -> bool: default_alertmanager_config = textwrap.dedent( \"\"\" route: group_by: ['alertname'] group_wait: 30s group_interval: 5m repeat_interval: 1h receiver: 'web.hook' receivers: - name: 'web.hook' webhook_configs: - url: 'http://127.0.0.1:5001/' inhibit_rules: - source_match: severity: 'critical' target_match: severity: 'warning' equal: ['alertname', 'dev', 'instance'] \"\"\" ) config: dict = yaml.safe_load(default_alertmanager_config) if pagerduty_key := self.model.config.get(\"pagerduty_key\"): config.update( { \"receivers\": [ { \"name\": \"default_pagerduty\", \"pagerduty_configs\": [ {\"send_resolved\": True, \"service_key\": pagerduty_key} ], } ] } ) config[\"route\"][\"receiver\"] = \"default_pagerduty\" else: config.update( { \"receivers\": [ { \"name\": \"web.hook\", \"webhook_configs\": [ { \"url\": \"http://127.0.0.1:5001/\", } ], } ] } ) config[\"route\"][\"receiver\"] = \"web.hook\" config_yaml = yaml.safe_dump(config) config_hash = utils.sha256(config_yaml) if config_hash != self._stored.config_hash: self.container.push(self._config_path, config_yaml) api = AlertmanagerAPIClient(self._fetch_private_address(), self._api_port) if api.reload(): self._stored.config_hash = config_hash success = True else: logger.warning(\"config reload via HTTP POST failed\") if restart_on_failure: if self._restart_service(): self._stored.config_hash = config_hash success = True else: success = False else: success = False else: success = True return success"}
{"text_id": "8644", "text": "docstring: def sequence() -> str: t = int(time.time()) * 1000 if t > XRegistryBase._sequence: XRegistryBase._sequence = t else: XRegistryBase._sequence += 1 return str(XRegistryBase._sequence)"}
{"text_id": "8645", "text": "docstring: def selectionChanged(self): pass"}
{"text_id": "8646", "text": "docstring: def _delete(self, uuid: str) -> json_api.system_users.SystemUser: api_endpoint = ApiEndpoints.system_users.delete request_obj = api_endpoint.load_request(uuid=uuid) return api_endpoint.perform_request(http=self.auth.http, request_obj=request_obj)"}
{"text_id": "8647", "text": "docstring: def exists_user_notifications(session, user_id): res = session.execute(text(\"\"\"SELECT EXISTS( SELECT 1 FROM public.notification WHERE user_id='{0}') AS user\"\"\" .format(user_id))).fetchone() return res.user"}
{"text_id": "8648", "text": "docstring: def create_rules_and_add_unknown_spending( self, category, description, date, value, userid ): self.rules[category].append(description) self.add_transaction(date, category, value, userid) self.save_user(userid)"}
{"text_id": "8649", "text": "docstring: def parse_balanced_text(textre, text, throw_away = 0): strbuf = [] prevstring = \"(at beginning)\" leftmatches = [] parenlevel = 0 for string in textre.findall(text): if debug['debugparens']: errprint(\"pbt: Saw %s, parenlevel=%s\" % (string, parenlevel)) if string.startswith('<ref'): if not string.endswith('>'): wikiwarning(\"Strange parsing, saw odd ref tag: %s\" % string) if string.endswith('/>'): continue string = '<ref>' if string in right_match_chars: if parenlevel == 0: wikiwarning(\"Nesting level would drop below 0; string = %s, prevstring = %s\" % (string, prevstring.replace('\\n','\\\\n'))) yield string else: strbuf.append(string) assert len(leftmatches) == parenlevel should_left = right_match_chars[string] should_pop_off = 1 the_left = leftmatches[-should_pop_off] if should_left != the_left: if should_left == '<ref>': wikiwarning(\"Saw unmatched </ref>\") in_ref = any([match for match in leftmatches if match == '<ref>']) if not in_ref: wikiwarning(\"Stray </ref>??; prevstring = %s\" % prevstring.replace('\\n','\\\\n')) should_pop_off = 0 else: while (len(leftmatches) - should_pop_off >= 0 and should_left != leftmatches[len(leftmatches)-should_pop_off]): should_pop_off += 1 if should_pop_off >= 0: wikiwarning(\"%s non-matching brackets inside of <ref>...</ref>: %s ; prevstring = %s\" % (should_pop_off - 1, ' '.join(left_match_chars[x] for x in leftmatches[len(leftmatches)-should_pop_off:]), prevstring.replace('\\n','\\\\n'))) else: wikiwarning(\"Inside of <ref> but still interpreted as stray </ref>??; prevstring = %s\" % prevstring.replace('\\n','\\\\n')) should_pop_off = 0 elif the_left == '<ref>': wikiwarning(\"Stray %s inside of <ref>...</ref>; prevstring = %s\" % (string, prevstring.replace('\\n','\\\\n'))) should_pop_off = 0 else: wikiwarning(\"Non-matching brackets: Saw %s, expected %s; prevstring = %s\" % (string, left_match_chars[the_left], prevstring.replace('\\n','\\\\n'))) if should_pop_off > 0: parenlevel -= should_pop_off if debug['debugparens']: errprint(\"pbt: Decreasing parenlevel by 1 to %s\" % parenlevel) leftmatches = leftmatches[:-should_pop_off] if parenlevel == 0: yield ''.join(strbuf) strbuf = [] else: if string in left_match_chars: if throw_away > 0: wikiwarning(\"Throwing away left bracket %s as a reparse strategy\" % string) throw_away -= 1 else: parenlevel += 1 if debug['debugparens']: errprint(\"pbt: Increasing parenlevel by 1 to %s\" % parenlevel) leftmatches.append(string) if parenlevel > 0: strbuf.append(string) else: yield string prevstring = string leftover = ''.join(strbuf) if leftover: wikiwarning(\"Unmatched left paren, brace or bracket: %s characters remaining\" % len(leftover)) wikiwarning(\"Remaining text: [%s]\" % bound_string_length(leftover)) wikiwarning(\"Reparsing:\") for string in parse_balanced_text(textre, leftover, throw_away = parenlevel): yield string"}
{"text_id": "8650", "text": "docstring: def with_argument(self, *args, **kwargs): arg_values = self.get_spec(self.CONST_ARGS, []) args = [str(arg) for arg in args] arg_values.extend(args) for k, v in kwargs.items(): if \" \" in k: raise ValueError(\"Argument key cannot contain space.\") arg_values.append(f\"--{str(k)}\") if v is None: continue arg_values.append(str(v)) self.set_spec(self.CONST_ARGS, arg_values) return self"}
{"text_id": "8651", "text": "docstring: def filter_persons_by_idx(file_names: List[str], keep_person_idxs: List[int]) -> List[int]: idx_per_person = [[] for _ in range(15)] if keep_person_idxs is not None: keep_person_idxs = [f'p{person_idx:02d}/' for person_idx in set(keep_person_idxs)] for idx, file_name in enumerate(file_names): if any(keep_person_idx in file_name for keep_person_idx in keep_person_idxs): person_idx = int(file_name.split('/')[-3][1:]) idx_per_person[person_idx].append(idx) else: for idx, file_name in enumerate(file_names): person_idx = int(file_name.split('/')[-3][1:]) idx_per_person[person_idx].append(idx) return list(itertools.chain(*idx_per_person))"}
{"text_id": "8652", "text": "docstring: def soy(date: datetime.date, offset: int = 0) -> datetime.date: return type(date)(date.year + offset, 1, 1)"}
{"text_id": "8653", "text": "docstring: def define_schemes(): return [ (\"dive(t=764): 1->2 gear too early\", \"v001\", 45.0, 30.40), (\"dive(t=903): 1st gear not reached\", \"v001\", 45.0, 36.4), (\"lowPower(t=724)\", \"v001\", 43.0, 69.6), (\"Py:LowP AccDB:DnShift(t=1571-8)\", \"v019\", 69, 63.4), (\"p_max from lower g(t=1571-8)\", \"v020\", 69, 63.4), (\"sameCar?\", \"v023\", 1, 0), (\"noPowerDives(t=1540-80)\", \"v024\", 26, 63.60), (\"noPowerDive(t=1574)\", \"v024\", 26, 66.4), (\"noPowerDive(t=1574)\", \"v025\", 69, 63.4), ( \"decelToStop(t=1591)\", \"v025\", 69, 72.0, \"AccDB not respecting n_min=0.9 x n_idle (Annex 2-3.k.3)\", ), (\"lowPowerDive\", \"v033\", 5, 72), (\"low -1 gears?\", \"v035\", 65, 63, \"Diffs in gears above 2\"), (\"noPowerDive(t=1574)\", \"v024\", 26, 66.4), (\"g5Ok?(1542)\", \"v075\", 55, 62.4, \"Vehicle has too many insufficient powers\"), (\"lowP, lowG betterPAvail(t=1574)\", \"v087\", 65, 63.40), (\"lowPower(t=1574)\", \"v088\", 65, 63.40), (\"lowPower\", \"v101\", 1, 0), (\"lowPower\", \"v111\", 1, 0), (\"extention\", \"v117\", 13, 72), (\"extention\", \"v118\", 13, 72), (\"extention\", \"v119\", 13, 72), (\"extention\", \"v120\", 13, 72), (\"extention\", \"v121\", 13, 72), (\"lowPower\", \"v124\", 1, 0), (\"lowPower\", \"v125\", 1, 0), ]"}
{"text_id": "8654", "text": "docstring: def generate_random_time(): time = round(random.uniform(0.1, 30.0), 2) scramble = \" \".join(CubeUtils.generate_scramble()) date = datetime.datetime.now() time = Time(time, scramble, date) return time"}
{"text_id": "8655", "text": "docstring: def force_set_element(): js_executor.execute_template('setValueTemplate', {'value': val}, self.element) return True"}
{"text_id": "8656", "text": "docstring: def error_popup(self, error): if error == '': return else: self.dialog = MDDialog( text = f\"Error: {error}\", ) self.dialog.open() app = MDApp.get_running_app() app.update_view_model.ERROR_MSG = ''"}
{"text_id": "8657", "text": "docstring: def wrap(self, text, width=None, **kwargs): width = self.width if width is None else width lines = [] for line in text.splitlines(): lines.extend( (_linewrap for _linewrap in SequenceTextWrapper( width=width, term=self, **kwargs).wrap(line)) if line.strip() else (u'',)) return lines"}
{"text_id": "8658", "text": "docstring: def load(self, filename: Optional[str] = None) -> 'Core': if filename is None: filename = self.path + self.files[self.selected] with open(filename, 'r') as fh: raw = fh.readlines() normal = self._normalize_loaded_text(raw) self.data.replace(normal) return Core(self.data, self.term)"}
{"text_id": "8659", "text": "docstring: def add_friendship_from_invite(self, friendinvite): a_to_b = Friend( friend_a_id=friendinvite.requesting_id, friend_b_id=friendinvite.requested_id, creation_date=datetime.datetime.utcnow() ) b_to_a = Friend( friend_a_id=friendinvite.requested_id, friend_b_id=friendinvite.requesting_id, creation_date=datetime.datetime.utcnow() ) try: self.session.bulk_save_objects([a_to_b, b_to_a]) self.session.delete(friendinvite) self.session.commit() except SQLAlchemyError as e: logger.log_error(f\"Database Error: {str(e)}\") self.session.rollback() raise exceptions.ServerError(\"Error adding friendship\") return (a_to_b, b_to_a)"}
{"text_id": "8660", "text": "docstring: def register_server(self, c, devices, messageID): if isinstance(devices, str): devices = [devices] found = [] for device in devices: device = device.upper() servers = self.deviceServers.setdefault(device, []) servers.append({'target': c.source, 'context': c.ID, 'messageID': messageID}) for (server, channel), (known_device, idnResult) in self.knownDevices.items(): if device != known_device: continue found.append((device, server, channel, True)) return found"}
{"text_id": "8661", "text": "docstring: def _trimRecords(self): self.highpassrecords.resize(self.nhighpassrecords, refcheck=False) self.lowpassrecords.resize(self.nlowpassrecords, refcheck=False) self.digitalsvalrecords.resize(self.ndigitalsvalrecords, refcheck=False) for recname in ('highpassrecords', 'lowpassrecords', 'digitalsvalrecords'): if len(self.__getattribute__(recname)) == 0: self.__delattr__(recname)"}
{"text_id": "8662", "text": "docstring: def single_attach_lg(rw_mol, frag_attach_idxs, lg_group, lg_mol): assert lg_group in SINGLE_ATTACH, \"Function meant only for single attach\" amap_idx = {atom.GetAtomMapNum(): atom.GetIdx() for atom in rw_mol.GetAtoms() if atom.GetAtomMapNum() != 0} bt = Chem.BondType.SINGLE if lg_group in SINGLE_BOND_ATTACH_GROUPS else Chem.BondType.DOUBLE for atom in lg_mol.GetAtoms(): if atom.GetAtomMapNum() >= 1000: lg_attach_idx = amap_idx[atom.GetAtomMapNum()] if len(frag_attach_idxs) > 1 or len(frag_attach_idxs) == 0: print(\"Cannot attach single_attach_lg to multiple or zero attachment points\") return rw_mol else: frag_attach_idx = frag_attach_idxs[0] rw_mol.AddBond(frag_attach_idx, lg_attach_idx, bt) return rw_mol"}
{"text_id": "8663", "text": "docstring: def parse_commands(content: str) -> (str, list[str], int): cmd_match = re.match(rf\"\\s*([a-zA-Z0-9_-]*)\\s*\", content) sub_cmd_match = re.match(r\"((?:[a-zA-Z0-9_-]*\\s*)*)\\s*\", content[cmd_match.end()::]) command = cmd_match.group(1) sub_commands = sub_cmd_match.group(1).split() offset = cmd_match.end() + sub_cmd_match.end() return command, sub_commands, offset"}
{"text_id": "8664", "text": "docstring: def check_finish(self): if not self.start_flag: assert len(self.buffer_data) == 0, \"the input buffer is not empty\""}
{"text_id": "8665", "text": "docstring: def main(): name = 'tmvoordle' name = name.lower() word_list_ini = load_dictionary.load('2of4brif.txt') trigrams_filtered = load_dictionary.load('least-likely_trigrams.txt') word_list = prep_words(name, word_list_ini) filtered_cv_map = cv_map_words(word_list) filter_1 = cv_map_filter(name, filtered_cv_map) filter_2 = trigram_filter(filter_1, trigrams_filtered) filter_3 = letter_pair_filter(filter_2) view_by_letter(name, filter_3)"}
{"text_id": "8666", "text": "docstring: def evaluate_model(model, X_test, Y_test, category_names): print(f'Best parameters: {model.best_params_}') y_pred = model.predict(X_test) for i, name in enumerate(Y_test.columns): report = classification_report(Y_test.iloc[:, i], pd.DataFrame(y_pred).iloc[:, i]) print(name) print(report)"}
{"text_id": "8667", "text": "docstring: def read_int(self): return self.read_long()"}
{"text_id": "8668", "text": "docstring: def calc_reweight_factor(self, labels_list): bg_class_ind = self.num_classes for ii, each_level_label in enumerate(labels_list): pos_inds = ((each_level_label >= 0) & (each_level_label < bg_class_ind)).nonzero( as_tuple=False).squeeze(1) self.cls_num_pos_samples_per_level[ii] += len(pos_inds) min_pos_samples = min(self.cls_num_pos_samples_per_level) max_pos_samples = max(self.cls_num_pos_samples_per_level) interval = 1. / (max_pos_samples - min_pos_samples + 1e-10) reweight_factor_per_level = [] for pos_samples in self.cls_num_pos_samples_per_level: factor = 2. - (pos_samples - min_pos_samples) * interval reweight_factor_per_level.append(factor) return reweight_factor_per_level"}
{"text_id": "8669", "text": "docstring: def run_frame(self, frame, node, annotated_locals=None): self.push_frame(frame) frame.states[frame.f_code.first_opcode] = frame_state.FrameState.init( node, self.ctx) frame_name = frame.f_code.co_name if frame_name not in self.local_ops or frame_name != \"<module>\": self.local_ops[frame_name] = [] self.annotated_locals[frame_name] = annotated_locals or {} else: assert annotated_locals is None can_return = False return_nodes = [] finally_tracker = _FinallyStateTracker() for block in frame.f_code.order: state = frame.states.get(block[0]) if not state: log.warning(\"Skipping block %d, nothing connects to it.\", block.id) continue self.frame.current_block = block op = None for op in block: state = self.run_instruction(op, state) new_why = finally_tracker.process(op, state, self.ctx) if new_why: state = state.set_why(new_why) if state.why: break if state.why: if finally_tracker.check_early_exit(state): for target in self.frame.targets[block.id]: del frame.states[target] can_return |= state.why in (\"return\", \"yield\") return_nodes.append(state.node) elif op.carry_on_to_next(): state = state.forward_cfg_node() frame.states[op.next] = state.merge_into(frame.states.get(op.next)) self._update_excluded_types(node) self.pop_frame(frame) if not return_nodes: assert not frame.return_variable.bindings frame.return_variable.AddBinding(self.ctx.convert.unsolvable, [], node) else: node = self.ctx.join_cfg_nodes(return_nodes) if not can_return: assert not frame.return_variable.bindings self._set_frame_return(node, frame, self.ctx.convert.no_return.to_variable(node)) return node, frame.return_variable"}
{"text_id": "8670", "text": "docstring: def x(self): acceleration = self.fdmexec.GetAuxiliary().GetPilotAccel(1) return convert_jsbsim_acceleration(acceleration)"}
{"text_id": "8671", "text": "docstring: def spark_dataframe_schema(df): schema_struct = dt.dtype(df.schema) return sch.schema(schema_struct.names, schema_struct.types)"}
{"text_id": "8672", "text": "docstring: def _forward_index_from_arff(self): scorer_name = self.scorer_name arff_file = self.trainarff vocab_file = self.trainarff_vocab with Timing('Loading and processing training data from %s using scorer %s...' % (arff_file, scorer_name), self.logging): (train_doc_term_freq, concepts) = loadarff(arff_file) pickle_output = '%s.pickle' % arff_file[:arff_file.rfind('.')] if not os.path.exists(pickle_output): def task(item, _pickle_output): with open(_pickle_output, 'wb') as outfile: pickle.dump(item, outfile, protocol=2) process = mp.Process(target=task, args=((train_doc_term_freq, concepts), pickle_output)) process.start() process.join() forward_index = self.get_scorer(scorer_name)(train_doc_term_freq) with Timing('Loading vocabulary from %s...' % vocab_file, self.logging): with open(vocab_file, 'rb') as infile: vocabulary = pickle.load(infile) mapping = {} for word, idx in vocabulary.iteritems(): mapping[idx] = word self.forward_index_ = forward_index self.num_concepts_, self.num_features_ = forward_index.shape self.concepts_ = concepts self.vocabulary_ = vocabulary self.mapping_ = mapping"}
{"text_id": "8673", "text": "docstring: def age_moment_vector_up_to(self, up_to_order, start_age_moments=None): n = self.nr_pools times = self.times if start_age_moments is None: start_age_moments = np.zeros((up_to_order, n)) max_order=start_age_moments.shape[0] if up_to_order>max_order: raise Error(\"\"\" To solve the moment system with order{0} start_age_moments up to (at least) the same order have to be provided. But the start_age_moments.shape was {1}\"\"\".format(up_to_order, start_age_moments.shape) ) if up_to_order<max_order: warning(\"\"\" Start_age_moments contained higher order values than needed. start_age_moments order was {0} while the requested order was {1}. This is no problem but possibly unintended. The higer order moments will be clipped \"\"\".format(max_order, up_to_order) ) start_age_moments=start_age_moments[0:order,:] if not (0 in self.start_values): ams, _ = self._solve_age_moment_system(up_to_order, start_age_moments) return ams.reshape((-1, up_to_order+1, n)) else: raise(ValueError('At least one pool is empty at the beginning.'))"}
{"text_id": "8674", "text": "docstring: def hybridParetoKnapsack(size, items, values, iterCounter, useRatioSort=False): paretoItems = [wPoint1(item) for item in items] solver = knapsackParetoSolver(paretoItems, values, range(len(values)), wPoint1(size), paretoPoint1(0, 0), wPoint1(0), iterCounter) solver.printInfo = printPct solver.forceUsePareto = False solver.useRatioSort = useRatioSort bestValue, bestSize, bestItems, bestValues, bestIndexes = solver.solve() return bestValue, bestSize.getDimension(0), bestItems, bestValues"}
{"text_id": "8675", "text": "docstring: def motion_notify_event(self, widget, event): self.last_win_x, self.last_win_y = x, y button = 0 data_x, data_y = self.get_data_xy(x, y) self.last_data_x, self.last_data_y = data_x, data_y return self.make_ui_callback('motion', button, data_x, data_y)"}
{"text_id": "8676", "text": "docstring: def aggregated_list( self, request: Union[compute.AggregatedListForwardingRulesRequest, dict] = None, *, project: str = None, retry: OptionalRetry = gapic_v1.method.DEFAULT, timeout: float = None, metadata: Sequence[Tuple[str, str]] = (), ) -> pagers.AggregatedListPager: has_flattened_params = any([project]) if request is not None and has_flattened_params: raise ValueError( \"If the `request` argument is set, then none of \" \"the individual field arguments should be set.\" ) if not isinstance(request, compute.AggregatedListForwardingRulesRequest): request = compute.AggregatedListForwardingRulesRequest(request) if project is not None: request.project = project rpc = self._transport._wrapped_methods[self._transport.aggregated_list] response = rpc(request, retry=retry, timeout=timeout, metadata=metadata,) response = pagers.AggregatedListPager( method=rpc, request=request, response=response, metadata=metadata, ) return response"}
{"text_id": "8677", "text": "docstring: def delete(self, name): if name in ['ca','ra','admin']: raise Exception('Sorry this name is reserved') if not (re.match('^[\\w\\-_\\(\\)]+$', name) is not None): raise Exception('Invalid profile name') try: self._storage.delete_profile(name) except Exception as err: raise Exception(err) try: del self._profiles_list[name] except KeyError as err: pass return True"}
{"text_id": "8678", "text": "docstring: def schedule_host_svc_downtime(self, host, minutes=30, start=None): cmd = \"SCHEDULE_HOST_SVC_DOWNTIME\" dt_cmd_str = self._fmt_dt_str(cmd, host, minutes, start=start) self._write_command(dt_cmd_str)"}
{"text_id": "8679", "text": "docstring: def register_view(): if current_user.username != \"admin\": return redirect(url_for(\"cabinet.cabinet_view\")) register_form = RegisterForm() register_form.tariff_select.choices = [tariff.value[\"tariff_name\"] for tariff in list(Tariffs)] if request.method == \"POST\": data = request.form if register_form.validate_on_submit() or (data and current_app.testing): new_user = User( name=data.get(\"name\"), phone=data.get(\"phone\"), email=data.get(\"email\"), username=data.get(\"username\"), password=data.get(\"password\"), tariff=data.get(\"tariff_select\"), address=data.get(\"address\"), state=State.activated_state.value, ) try: new_user.save_to_db() flash(messages[\"success_register\"], \"info\") return redirect(url_for(\"admin.admin_view\")) except ValueError as e: current_app.logger.info(\"Error while saving new user to the database - {0}\".format(e)) flash(messages[\"failure\"], \"warning\") return redirect(url_for(\"admin.register_view\")) return render_template(\"auth/register.html\", title=\"Register new user\", form=register_form)"}
{"text_id": "8680", "text": "docstring: def requires_2sa(self): return self.data.get(\"dsInfo\", {}).get(\"hsaVersion\", 0) >= 1 and ( self.data.get(\"hsaChallengeRequired\", False) or not self.is_trusted_session )"}
{"text_id": "8681", "text": "docstring: def modify_web_p_async(self, request): http_request = request.to_http_info(self.api_client.configuration) return self.__make_request_async(http_request, 'GET', 'file')"}
{"text_id": "8682", "text": "docstring: def with_source( self, url: str, branch: str = None, commit: str = None, secret_ocid: str = None ): self.set_spec(self.CONST_GIT_URL, url) self.set_spec(self.CONST_BRANCH, branch) self.set_spec(self.CONST_COMMIT, commit) self.set_spec(self.CONST_GIT_SSH_SECRET_ID, secret_ocid) return self"}
{"text_id": "8683", "text": "docstring: def produce(self, *, inputs: Inputs, timeout: float = None, iterations: int = None) -> CallResult[Outputs]: if not self._is_fit: raise PrimitiveNotFittedError(\"Primitive not fitted.\") result_df = pd.DataFrame({}) image_cols = inputs.metadata.get_columns_with_semantic_type('http://schema.org/ImageObject') for idx, col in enumerate(image_cols): base_path = inputs.metadata.query((metadata_base.ALL_ELEMENTS, col))['location_base_uris'][0].replace('file:///', '/') image_paths = np.array([os.path.join(base_path, filename) for filename in inputs.iloc[:,col]]) images_array = \\ np.array([image_array_from_path(fpath, target_size=self.ImageNet.target_size) for fpath in image_paths]) logger.debug(f'preprocessing {images_array.shape[0]} images') if images_array.ndim != 4: raise Exception('invalid input shape for images_array, expects a 4d array') X_test = self.ImageNet.preprocess(images_array) test_dataset = ImageNetGen(X_test, batch_size = self.hyperparams['batch_size']) preds = self.ImageNet.classify(test_dataset) preds = self.encoder.inverse_transform(np.argmax(preds, axis=1)) result_df[self.output_columns[idx]] = preds result_df = container.DataFrame(result_df, generate_metadata=True) for i in range(result_df.shape[1]): result_df.metadata = result_df.metadata.add_semantic_type((metadata_base.ALL_ELEMENTS, i), ('https://metadata.datadrivendiscovery.org/types/PredictedTarget')) return CallResult(result_df, has_finished=True)"}
{"text_id": "8684", "text": "docstring: def findEntry(self, term: str)-> Optional['Entry']: for packages in list(self.packageNamed.values()): if term in packages.entryNamed: return packages.entryNamed[term] for packages in list(self.packageNamed.values()): for entry in list(packages.entryNamed.values()): if term in entry.inflections: return entry for packages in list(self.packageNamed.values()): for entry in list(packages.entryNamed.values()): if term in entry.synonyms: return entry return None"}
{"text_id": "8685", "text": "docstring: def autodock_vina(receptor_pdbqt, ligand_pdbqt, output_file, box_center, box_size, exhaustiveness=10, n_poses=10, scoring_function='vina', cpu_cores=0, seed=42): v = Vina(sf_name=scoring_function, cpu=cpu_cores, seed=seed) v.set_receptor(receptor_pdbqt) v.set_ligand_from_file(ligand_pdbqt) v.compute_vina_maps(center=box_center, box_size=box_size) v.dock(exhaustiveness=exhaustiveness, n_poses=n_poses) v.write_poses(f\"{output_file.split('.')[0]}.pdbqt\", n_poses=n_poses, overwrite=True) pdbqt_to_sdf(pdbqt_file=f\"{output_file.split('.')[0]}.pdbqt\", output=f\"{output_file.split('.')[0]}.sdf\")"}
{"text_id": "8686", "text": "docstring: def new_range(self, ip_range): if not ip_range in self.ip_ranges: self.ip_ranges.add(ip_range) doc = self.rs.id_to_object(ip_range) doc.add_tag('sniffer') doc.save() print_success(\"New ip range: {}\".format(ip_range))"}
{"text_id": "8687", "text": "docstring: def fork_filter_mapped_init(samplename, fragment, VERBOSE=0, n_pairs=-1, PCR=1, summary=True): if VERBOSE: print 'Forking to the cluster: sample '+samplename+', fragment '+fragment JOBSCRIPT = JOBDIR+'store/filter_mapped_reads.py' cluster_time = '23:59:59' vmem = '8G' qsub_list = ['qsub','-cwd', '-b', 'y', '-S', '/bin/bash', '-o', JOBLOGOUT, '-e', JOBLOGERR, '-N', 'fmi '+samplename+' '+fragment, '-l', 'h_rt='+cluster_time, '-l', 'h_vmem='+vmem, JOBSCRIPT, '--samples', samplename, '--fragments', fragment, '--verbose', VERBOSE, '--maxreads', n_pairs, '--PCR', PCR, ] if not summary: call_list.append('--no-summary') qsub_list = map(str, qsub_list) if VERBOSE: print ' '.join(qsub_list) return sp.check_output(qsub_list)"}
{"text_id": "8688", "text": "docstring: def unlockedIdle(self): while self.active and \\ self.results[0] == [] and \\ self.rqCore.machine.isNimbySafeToRunJobs(): try: self._openEvents() self.results = select.select(self.fileObjList, [], [], 5) except Exception: pass if not self.rqCore.machine.isNimbySafeToRunJobs(): log.warning(\"memory threshold has been exceeded, locking nimby\") self.active = True if self.active: self._closeEvents() self.lockNimby() self.thread = threading.Timer(rqd.rqconstants.CHECK_INTERVAL_LOCKED, self.lockedInUse) self.thread.start()"}
{"text_id": "8689", "text": "docstring: def on_put(self, req, resp, _id): try: new_learning_object = req_to_dict(req) user = req.context.get('user') modify_one( db_client=self.db_client, old_learning_object_id=_id, new_learning_object=new_learning_object, user=user ) resp.body = dumps({'data': {'id': _id}}) except LearningObjectNotFoundError as e: resp.status = falcon.HTTP_NOT_FOUND resp.body = dumps( {'message': json.dumps(e.args[0], ensure_ascii=False)}) except LearningObjectSchemaError as e: resp.status = falcon.HTTP_BAD_REQUEST resp.body = dumps( {'message': json.dumps(e.args[0], ensure_ascii=False)}) except LearningObjectMetadataSchemaError as e: resp.status = falcon.HTTP_BAD_REQUEST resp.body = dumps( {'message': json.dumps(e.args[0], ensure_ascii=False)}) except UserInactiveError as e: resp.status = falcon.HTTP_UNAUTHORIZED resp.body = dumps( {'message': json.dumps(e.args[0], ensure_ascii=False)}) except UserPermissionError as e: resp.status = falcon.HTTP_UNAUTHORIZED resp.body = dumps( {'message': json.dumps(e.args[0], ensure_ascii=False)})"}
{"text_id": "8690", "text": "docstring: def handle_init_session(self, channel: Channel, session: FBDPSession) -> None: session.charset = cast(MIME, session.data_format).params.get('charset', 'ascii') session.errors = cast(MIME, session.data_format).params.get('errors', 'strict')"}
{"text_id": "8691", "text": "docstring: def run(self) -> None: constants.SHOULD_PREVENT_PIPELINE_EXECUTION = True pb2_pipeline_json = self.entrypoint_args[PIPELINE_JSON_OPTION] pb2_pipeline = Pb2Pipeline() json_format.Parse(pb2_pipeline_json, pb2_pipeline) main_module_source = self.entrypoint_args[MAIN_MODULE_SOURCE_OPTION] step_source = self.entrypoint_args[STEP_SOURCE_OPTION] input_artifact_sources = json.loads( self.entrypoint_args[INPUT_ARTIFACT_SOURCES_OPTION] ) materializer_sources = json.loads( self.entrypoint_args[MATERIALIZER_SOURCES_OPTION] ) pipeline_name = pb2_pipeline.pipeline_info.id step_module, step_name = step_source.rsplit(\".\", 1) self.setup(pipeline_name=pipeline_name, step_name=step_name) integration_registry.activate_integrations() importlib.import_module(main_module_source) constants.USER_MAIN_MODULE = main_module_source step_class = source_utils.load_source_path_class(step_source) if not issubclass(step_class, BaseStep): raise TypeError( f\"The step source `{step_source}` passed to the \" f\"entrypoint is not pointing to a `{BaseStep}` subclass.\" ) step = step_class() original_step_module = ( \"__main__\" if step_module == main_module_source else step_module ) step.__module__ = original_step_module self._create_executor_class( step=step, input_artifact_sources=input_artifact_sources, materializer_sources=materializer_sources, ) run_name = self.get_run_name(pipeline_name=pipeline_name) orchestrator = Repository().active_stack.orchestrator execution_info = orchestrator.run_step( step=step, run_name=run_name, pb2_pipeline=pb2_pipeline ) pipeline_node = orchestrator._get_node_with_step_name( step_name=step_name, pb2_pipeline=pb2_pipeline ) self.post_run( pipeline_name=pipeline_name, step_name=step_name, pipeline_node=pipeline_node, execution_info=execution_info, )"}
{"text_id": "8692", "text": "docstring: def convert_uri_to_buffer(self, **kwargs): if urllib.parse.urlparse(self.uri).scheme in {'http', 'https', 'data'}: req = urllib.request.Request(self.uri, headers={'User-Agent': 'Mozilla/5.0'}) with urllib.request.urlopen(req) as fp: self.buffer = fp.read() elif os.path.exists(self.uri): with open(self.uri, 'rb') as fp: self.buffer = fp.read() else: raise FileNotFoundError(f'{self.uri} is not a URL or a valid local path')"}
{"text_id": "8693", "text": "docstring: def _WaitTillInit(): try: global_step = sess.run(py_utils.GetGlobalStep()) except tf.errors.FailedPreconditionError as e: tf.logging.info('Probably the expected race on global_step: %s', e) raise msg = 'step:%6d' % global_step self._SetStatusMessage(msg) if global_step < self._start_up_delay_steps: msg = 'global step (%d) has not reached start up delay steps (%d)' % ( global_step, self._start_up_delay_steps) tf.logging.info('%s', msg) raise tf.errors.FailedPreconditionError( node_def=None, op=None, message=msg) return global_step"}
{"text_id": "8694", "text": "docstring: def run(self): self.output.close() self.circuit = QuantumCircuit.from_qasm_file(self.qasmDir) self.collapsed = True return execute(self.circuit, backend= self.backend)"}
{"text_id": "8695", "text": "docstring: def delete_inactive_external_versions(limit=200, days=30 * 3): days_ago = datetime.now() - timedelta(days=days) queryset = Version.external.filter( active=False, modified__lte=days_ago, )[:limit] for version in queryset: try: last_build = version.last_build if last_build: status = BUILD_STATUS_PENDING if last_build.finished: status = BUILD_STATUS_SUCCESS if last_build.success else BUILD_STATUS_FAILURE send_build_status( build_pk=last_build.pk, commit=last_build.commit, status=status, link_to_build=True, ) except Exception: log.exception( \"Failed to send status: project=%s version=%s\", version.project.slug, version.slug, ) else: log.info( \"Removing external version. project=%s version=%s\", version.project.slug, version.slug, ) version.delete()"}
{"text_id": "8696", "text": "docstring: def accuracy(self, params, data, labels): predictions = self.argmax(self.inference(params, data)) return tf.contrib.metrics.accuracy(predictions, tf.cast(labels, tf.int32))"}
{"text_id": "8697", "text": "docstring: def cublasZtrmm(handle, side, uplo, trans, diag, m, n, alpha, A, lda, B, ldb, C, ldc): status = _libcublas.cublasZtrmm_v2(handle, _CUBLAS_SIDE_MODE[side], _CUBLAS_FILL_MODE[uplo], _CUBLAS_OP[trans], _CUBLAS_DIAG[diag], m, n, ctypes.byref(cuda.cuDoubleComplex(alpha.real, alpha.imag)), int(A), lda, int(B), ldb, int(C), ldc) cublasCheckStatus(status)"}
{"text_id": "8698", "text": "docstring: def mean_cellscore_fraction_list(cell_scores): return [mean_cellscore_fraction(cell_scores, i+1) for i in range(cell_scores.shape[1])]"}
{"text_id": "8699", "text": "docstring: def _get_axlist(ax): if ax is None: axlist = [pl.gca()] elif isinstance(ax, pl.Axes): axlist = [ax] elif isinstance(ax, pl.Figure): axlist = ax.axes elif isinstance(ax, list): axlist = ax else: errormsg = f'Could not recognize object {type(ax)}: must be None, Axes, Figure, or list of axes' raise ValueError(errormsg) return axlist"}
{"text_id": "8700", "text": "docstring: def handle_accept(self): pair = self.accept() if not pair: return (sock, address) = pair if address in self._banned: if time.time() - self._banned[address] < 3600: sock.close() return del self._banned[address] if not self._listen: sock.close() return connection.Connection(node = self, sock = sock, address = address)"}
{"text_id": "8701", "text": "docstring: def check_av_status(CurrentIngest): avStatus = False event = 'format identification' CurrentIngest.caller = 'pymmFunctions.is_av()' AV = pymmFunctions.is_av(CurrentIngest.currentTargetObject.inputPath) if not AV: outcome = \"WARNING: {} is not recognized as an a/v object.\".format( CurrentIngest.currentTargetObject.inputPath ) status = \"WARNING\" print(outcome) else: outcome = \"{} is a(n) {} object, way to go.\".format( CurrentIngest.currentTargetObject.inputPath, AV ) status = \"OK\" avStatus = True loggers.log_event( CurrentIngest, event, outcome, status ) return avStatus, AV"}
{"text_id": "8702", "text": "docstring: def CSP_timescale_participation_indices(nr, JacK, evals, A, B): nv = A.shape[0] TPI = np.zeros((nv,2*nr)) for i in range(nv): TPI[i] = CSP_participation_to_one_timescale(i, nr, JacK, evals, A, B) return TPI"}
{"text_id": "8703", "text": "docstring: def block_send(self, _gaswei:int,_pricewei:int,_valeth:int=0,_debugtx: bool = False,_receipList: bool = False) -> str: _fn = self._underlying_method() try: _t = _fn.buildTransaction({ 'from': self._operate, 'gas': _gaswei, 'gasPrice': _pricewei }) _t['nonce'] = self._web3_eth.getTransactionCount(self._operate) if _valeth > 0: _t['value'] = _valeth if _debugtx: print(f\"======== Signing \u2705 by {self._operate}\") print(f\"======== Transaction \u2705 check\") print(_t) if 'data' in _t: signed = self._web3_eth.account.sign_transaction(_t) txHash = self._web3_eth.sendRawTransaction(signed.rawTransaction) tx_receipt = None if _receipList is True: print(f\"======== awaiting Confirmation \ud83d\udeb8\ufe0f {self.sign}\") tx_receipt = self._web3_eth.waitForTransactionReceipt(txHash) if _debugtx: print(\"======== TX Result \u2705\") print(tx_receipt) print(f\"======== TX blockHash \u2705\") if tx_receipt is not None: print(f\"{Bolors.OK}{tx_receipt.blockHash.hex()}{Bolors.RESET}\") else: print(f\"{Bolors.WARNING}{txHash.hex()}{Bolors.RESET} - broadcast hash\") if _receipList is False: time.sleep(self._wait) except ContractLogicError as er: print(f\"{Bolors.FAIL}Error {er} {Bolors.RESET}: token\") except ValueError as err: if \"message\" in err.args[0]: message = err.args[0][\"message\"] print(f\"{Bolors.FAIL}Error Revert {Bolors.RESET}, token: {message}\") else: print(f\"{Bolors.FAIL}Error Revert {Bolors.RESET}, token. Reason: Unknown\")"}
{"text_id": "8704", "text": "docstring: def create_dataloader(args, explicit_mode): data_dir = args.data_dir dataset = args.dataset train_ratio = args.train_ratio size = (args.input_height, args.input_width) augment_parameters = args.augment_parameters if dataset == 'kitti': files_path = EIGEN_PATH.format(explicit_mode) dataset = KittiDepthLoader(data_dir, files_path, explicit_mode, train_ratio, size, augment_parameters) elif dataset == 'cityscapes': files_path = CITYSCAPES_PATH.format(explicit_mode) dataset = CityScapesLoader(data_dir, files_path, explicit_mode, train_ratio, size, augment_parameters) else: raise ValueError(\"No other dataset has been implemented yet.\") n_img = len(dataset) print('Loaded a dataset with {} images'.format(n_img)) if explicit_mode == 'train': return DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True) return DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1, pin_memory=True)"}
{"text_id": "8705", "text": "docstring: def check_pg_stat_replication(self): for i in walrepl.polling(max_try=20, interval=0.5): res = self.get_pg_stat_replication() if len(res) == 0: continue elif res[0].state == 'streaming' and res[0].sync_state == 'sync': tinctest.logger.info('pg_stat_replication is updated with the information') return True else: continue return False"}
{"text_id": "8706", "text": "docstring: def ISetMin(InterfaceItemPath, MinValue): pass"}
{"text_id": "8707", "text": "docstring: def _parse(self, fpath): if self.is_hmmer: _GS_line_parser = self._parse_GS_record_hmmer else: _GS_line_parser = self._parse_GS_record with open(fpath, 'rU') as handle: line = handle.next() if not line.startswith('# STOCKHOLM 1.0'): raise ParserError('Missing Stockholm file header') for line in handle: line = line.strip() if line[0:4] == '#=GF': self._parse_GF_record(line) elif line[0:4] == '#=GS': _GS_line_parser(line) elif line[0:4] == '#=GR': self._parse_GR_record(line) elif line[0:4] == '#=GC': self._parse_GC_record(line) elif line[0:2] == '//': break elif line: self._parse_sequence(line) _sequences = [] for seqname in self._ordered_seq_ids: seq = self._seq_data[seqname]['seq'] _sequences.append(seq) del self._seq_data[seqname]['seq'] n_sequences = len(_sequences) self.logger.debug('Creating Alignment with {0} sequences'.format(n_sequences)) msa = Alignment(_sequences, self._ordered_seq_ids, seq_annotations=self._seq_data, aln_annotations=self._aln_data) return msa"}
{"text_id": "8708", "text": "docstring: def _largest_common_subgraph(self, candidates, constraints, to_be_mapped=None): if to_be_mapped is None: to_be_mapped = {frozenset(self.subgraph.nodes)} current_size = len( next(iter(to_be_mapped), []) ) found_iso = False if current_size <= len(self.graph): for nodes in sorted(to_be_mapped, key=sorted): next_sgn = min(nodes, key=lambda n: min(candidates[n], key=len)) isomorphs = self._map_nodes( next_sgn, candidates, constraints, to_be_mapped=nodes ) try: item = next(isomorphs) except StopIteration: pass else: yield item yield from isomorphs found_iso = True if found_iso or current_size == 1: return left_to_be_mapped = set() for nodes in to_be_mapped: for sgn in nodes: new_nodes = self._remove_node(sgn, nodes, constraints) left_to_be_mapped.add(new_nodes) yield from self._largest_common_subgraph( candidates, constraints, to_be_mapped=left_to_be_mapped )"}
{"text_id": "8709", "text": "docstring: def multinomial_nll(true_counts, logits): logits_perm = tf.transpose(logits, (0, 2, 1)) true_counts_perm = tf.transpose(true_counts, (0, 2, 1)) counts_per_example = tf.reduce_sum(true_counts_perm, axis=-1) dist = tf.contrib.distributions.Multinomial(total_count=counts_per_example, logits=logits_perm) seqlen = tf.to_float(tf.shape(true_counts)[0]) return -tf.reduce_sum(dist.log_prob(true_counts_perm)) / seqlen"}
{"text_id": "8710", "text": "docstring: def AddContentsFunction(t): native: 't := JContents(a_t).(*i_template.Template)'"}
{"text_id": "8711", "text": "docstring: def reset_parameters(self): self.ffn.reset_parameters() self.self_attention.reset_parameters() if hasattr(self, 'source_attention'): self.source_attention.reset_parameters()"}
{"text_id": "8712", "text": "docstring: def heatmap(latitudes, longitudes, values, resolution=None, maptype=MAPTYPE): img, pixels = background_and_pixels(latitudes, longitudes, MAX_SIZE, maptype) z = grid_density_gaussian_filter( zip(pixels['x_pixel'], pixels['y_pixel'], values), MAX_SIZE * SCALE, resolution=resolution if resolution else MAX_SIZE * SCALE, ) width = SCALE * MAX_SIZE plt.figure(figsize=(10, 10)) plt.imshow(np.array(img)) plt.imshow(z, origin='lower', extent=[0, width, 0, width], alpha=0.9) plt.scatter(pixels['x_pixel'], pixels['y_pixel'], s=1) plt.gca().invert_yaxis() plt.axis([0, width, width, 0]) plt.axis('off') plt.tight_layout() plt.show()"}
{"text_id": "8713", "text": "docstring: def run_step(self): controls = dict() for index,vehicle in enumerate(self.state['vehicles']): if (vehicle.type == 'robot') and (not vehicle.active) and (self.t >= self.av_activate): vehicle.active = True controls[index] = vehicle.controller.calculate(vehicle) for index,vehicle in enumerate(self.state['vehicles']): vehicle.state['index'] = index vehicle.state['step'] = self.state['step'] vehicle.state['time'] = self.state['time'] vehicle.control = controls[index] vehicle.acc = vehicle.control vehicle.vel += vehicle.acc*self.dt vehicle.pos += vehicle.vel*self.dt queue = sorted(self.vehicles, key=lambda vehicle: vehicle.pos.x) queue = [vehicle.state['index'] for vehicle in queue] self.state['queue'] = queue if not (self.learning_mode or self.hv_heterogeneity): self.check_crash(vehicle=None, raise_error=True) if not (self.learning_mode): self.check_crowding(vehicle=None, raise_warning=True, pct=0.5) self.state['step'] += 1 self.state['time'] += self.dt self.archive_state()"}
{"text_id": "8714", "text": "docstring: def resnet34_v1b(pretrained=False, root=os.path.expanduser('~/.torch/models'), **kwargs): model = ResNetV1b(BasicBlockV1b, [3, 4, 6, 3], **kwargs) if pretrained: from model.model_store import get_model_file model.load_state_dict(torch.load(get_model_file('resnet%d_v%db' % (34, 1), root=root))) from data.imagenet import ImageNetAttr attrib = ImageNetAttr() model.synset = attrib.synset model.classes = attrib.classes model.classes_long = attrib.classes_long return model"}
{"text_id": "8715", "text": "docstring: def clean(times, vals, start_ts, num_times): toR = [0 for i in range(num_times)] for t, v in zip(times, vals): ind = (int(t) - start_ts)//3600 if ind < num_times: toR[ind] = v return interp(toR, vals[0])"}
{"text_id": "8716", "text": "docstring: def new_crash_id(self): return self._new_crash_id"}
{"text_id": "8717", "text": "docstring: def prf_photometry(self, cadences=None, parallel=True, **kwargs): from .prf import PRFPhotometry log.warning('Warning: PRF-fitting photometry is experimental ' 'in this version of lightkurve.') prfphot = PRFPhotometry(model=self.get_model(**kwargs)) prfphot.run(self.flux + self.flux_bkg, cadences=cadences, parallel=parallel, pos_corr1=self.pos_corr1, pos_corr2=self.pos_corr2) return prfphot"}
{"text_id": "8718", "text": "docstring: def config(data_folder=settings.data_folder, logs_folder=settings.logs_folder, imgs_folder=settings.imgs_folder, cache_folder=settings.cache_folder, use_cache=settings.use_cache, log_file=settings.log_file, log_console=settings.log_console, log_level=settings.log_level, log_name=settings.log_name, log_filename=settings.log_filename, useful_tags_node=settings.useful_tags_node, useful_tags_path=settings.useful_tags_path, osm_xml_node_attrs=settings.osm_xml_node_attrs, osm_xml_node_tags=settings.osm_xml_node_tags, osm_xml_way_attrs=settings.osm_xml_way_attrs, osm_xml_way_tags=settings.osm_xml_way_tags, default_access=settings.default_access, default_crs=settings.default_crs, default_user_agent=settings.default_user_agent, default_referer=settings.default_referer, default_accept_language=settings.default_accept_language, nominatim_endpoint=settings.nominatim_endpoint, nominatim_key=settings.nominatim_key, overpass_endpoint=settings.overpass_endpoint, all_oneway=settings.all_oneway): settings.use_cache = use_cache settings.cache_folder = cache_folder settings.data_folder = data_folder settings.imgs_folder = imgs_folder settings.logs_folder = logs_folder settings.log_console = log_console settings.log_file = log_file settings.log_level = log_level settings.log_name = log_name settings.log_filename = log_filename settings.useful_tags_node = useful_tags_node settings.useful_tags_path = useful_tags_path settings.useful_tags_node = list(set(useful_tags_node + osm_xml_node_attrs + osm_xml_node_tags)) settings.useful_tags_path = list(set(useful_tags_path + osm_xml_way_attrs + osm_xml_way_tags)) settings.osm_xml_node_attrs = osm_xml_node_attrs settings.osm_xml_node_tags = osm_xml_node_tags settings.osm_xml_way_attrs = osm_xml_way_attrs settings.osm_xml_way_tags = osm_xml_way_tags settings.default_access = default_access settings.default_crs = default_crs settings.default_user_agent = default_user_agent settings.default_referer = default_referer settings.default_accept_language = default_accept_language settings.nominatim_endpoint = nominatim_endpoint settings.nominatim_key = nominatim_key settings.overpass_endpoint = overpass_endpoint settings.all_oneway = all_oneway if settings.log_file or settings.log_console: log('Configured osmnx')"}
{"text_id": "8719", "text": "docstring: def OnLeftDown(self, event): click_posn = event.GetPosition() if self.shift_down: self.is_box_select = True (self.sbox_w, self.sbox_h) = (0, 0) (self.sbox_1_x, self.sbox_1_y) = click_posn else: self.is_box_select = False self.SetCursor(wx.Cursor(wx.CURSOR_HAND)) (self.last_drag_x, self.last_drag_y) = click_posn event.Skip()"}
{"text_id": "8720", "text": "docstring: def dict_to_tuples(d): retval = [] for k, vals in d.iteritems(): retval.append([(k, v) for v in vals]) return retval"}
{"text_id": "8721", "text": "docstring: def exchange_attribute(attribute: str, exchange: str = CRYPTO_EXCHANGE) -> dict: try: check_exchange_existence(exchange=exchange) return ExchangeAttribute(attribute=attribute, exchange=exchange) except Exception as exception: logger.error('Oops! An error Occurred \u26a0\ufe0f') raise exception"}
{"text_id": "8722", "text": "docstring: def channel_scale(self, c, channel, scale=None): return self.selectedDevice(c).channel_scale(channel, scale)"}
{"text_id": "8723", "text": "docstring: def text_to_table(text, delimiter=',', head_delimiter=' = ', row_delimiter='\\n', deserializer=None, head_deserializer=None): head = {} header = None values = [tuple()] * 1000000 values_idx = 0 if isinstance(text, str): lines = text.split(row_delimiter) else: lines = text table_found = False for line in lines: if table_found: values[values_idx] = tuple(deserializer(v.strip()) for v in line.split(delimiter)) values_idx += 1 if values_idx >= len(values): values += [[]] * 1000000 elif head_delimiter in line: name, value = line.split(head_delimiter, 1) head[name.strip()] = head_deserializer(value.strip()) elif line.count(delimiter) > 1: try: vals = tuple(deserializer(v.strip()) for v in line.split(delimiter)) if all(isinstance(v, str) for v in vals): if all(len(v) == 0 for v in vals): raise RuntimeError('Ignore this row. It is empty') raise ValueError('Save the column header names.') values[values_idx] = vals values_idx += 1 except RuntimeError: continue except (ValueError, TypeError, Exception): header = [str(v.strip()) for v in line.split(delimiter)] table_found = True values = values[:values_idx] return head, header, values"}
{"text_id": "8724", "text": "docstring: def updateAliasesFromPipeline(self, pipeline): pip_aliases = pipeline.aliases.keys() mashup_aliases = [a.name for a in self.currentMashup.alias_list] new_aliases = [] if len(pip_aliases) == len(mashup_aliases): old_a = None new_a = None for a in self.currentMashup.alias_list: if a.name not in pip_aliases: old_a = a.do_copy(new_ids=True, id_scope=self.mshptrail.id_scope, id_remap={}) new_aliases.append(old_a) else: new_aliases.append(a) for a in pip_aliases: if a not in mashup_aliases: new_a = (a, pipeline.aliases[a]) if old_a is not None and new_a is not None: (a, info) = new_a parameter = pipeline.db_get_object(info[0],info[1]) old_a.name = a old_a.component.vttype = parameter.vtType old_a.component.vtid = parameter.real_id old_a.component.vtparent_type = info[2] old_a.component.vt_parent_id = info[3] old_a.component.mid = info[4] old_a.component.type = parameter.type old_a.component.val = parameter.strValue old_a.component.vtpos = parameter.pos elif len(pip_aliases) < len(mashup_aliases): pos = 0 for a in self.currentMashup.alias_list: if a.name in pip_aliases: alias = a.do_copy(new_ids=True, id_scope=self.mshptrail.id_scope, id_remap={}) alias.component.pos = pos new_aliases.append(alias) pos += 1 else: pos = len(mashup_aliases) new_aliases = [a for a in self.currentMashup.alias_list] for a in pip_aliases: if a not in mashup_aliases: info = pipeline.aliases[a] parameter = pipeline.db_get_object(info[0],info[1]) cid = self.id_scope.getNewId('mashup_component') aid = self.id_scope.getNewId('mashup_alias') component = Component(cid, parameter.vtType, parameter.real_id, info[2], info[3], info[4], parameter.type, parameter.strValue, parameter.pos, pos, \"\") alias = Alias(aid, a, component) new_aliases.append(alias) pos += 1 return self.createMashupVersion(new_aliases, quiet=False)"}
{"text_id": "8725", "text": "docstring: def open(self, path): if not isinstance(path, str): raise TypeError(\"expecting a string type argument *path*\") if not len(path) > 0: raise ValueError(\"argument *path* must not be empty\") log.info(\"searching for reader with path '{0}'\".format(path)) with self.lock: self.dev = nfc.dev.connect(path) if self.dev is None: log.error(\"no reader found at '{0}'\".format(path)) else: log.info(\"using {0}\".format(self.dev)) return bool(self.dev)"}
{"text_id": "8726", "text": "docstring: def handle_line(self, line): normalised = _preprocess_line(line) if normalised == \"\" or normalised == \" \": return None, False command_id, command, args = _parse_line(normalised) if command is None: return None, False is_error, cleaned_response, end_session = \\ self.run_command(command, args) if is_error: response_code = \"?\" else: response_code = \"=\" if command_id is not None: response_prefix = response_code + command_id else: response_prefix = response_code if cleaned_response == \"\": response_sep = \"\" else: response_sep = \" \" response = \"%s%s%s\\n\\n\" % ( response_prefix, response_sep, cleaned_response) return response, end_session"}
{"text_id": "8727", "text": "docstring: def removeGapOnly(arr, relativePositions, rmfile, log): out = open(rmfile, \"a\") if out.closed: print('file is closed') if len(arr) != 0: sums = sum(arr == \"-\") absolutePositions = set(np.where(sums == len(arr[:, 0]))[0]) rmpos = [] for n in absolutePositions: rmpos.append(relativePositions[n]) for n in rmpos: relativePositions.remove(n) rmpos = set(rmpos) arr = arr[:, sums != len(arr[:, 0])] if len(rmpos) != 0: rmpos_str = [str(x) for x in rmpos] log.info(\"Removing gap only sites %s\" % ( \", \".join(rmpos_str))) out.write(\"remove_gap_only\\t%s\\n\" % (\",\".join(rmpos_str))) else: rmpos = set() out.close() return (arr, rmpos, relativePositions)"}
{"text_id": "8728", "text": "docstring: def visit_Operation(self, expression, *operands): operation = self.operations.get(type(expression), None) if operation is not None and \\ all( isinstance(o, Constant) for o in operands) : value = operation(*(x.value for x in operands)) if isinstance(expression, BitVec): return BitVecConstant(expression.size, value, taint=expression.taint) else: isinstance(expression, Bool) return BoolConstant(value, taint=expression.taint) else: if any( operands[i] is not expression.operands[i] for i in xrange(len(operands))): expression = type(expression)(*operands, taint=expression.taint) return expression"}
{"text_id": "8729", "text": "docstring: def delete_user_group( self, uuid: str ) -> bool: parameters = dict() parameters[\"uuid\"] = GraphQLParam(uuid, \"UUID\", True) response = self._mutation( name=\"deleteOrgUserGroup\", params=parameters, fields=None ) return response"}
{"text_id": "8730", "text": "docstring: def main1(): t1 = time.time() uid_counter = read_scouting_stats() read_event_counts() print(\"---Bronze---\") uid_stats(uid_counter[1]) print(\"---Silver---\") uid_stats(uid_counter[2]) print(\"---Gold---\") uid_stats(uid_counter[3]) print(\"---Platinum---\") uid_stats(uid_counter[4]) print(\"--Diamond---\") uid_stats(uid_counter[5]) print(\"---Master---\") uid_stats(uid_counter[6]) print(\"---Grandmaster---\") uid_stats(uid_counter[7]) deltatime = time.time()-t1 print(\"Run time: \", \"{:2d}\".format(int(deltatime//60)), \"minutes and\", \"{:05.2f}\".format(deltatime%60), \"seconds\")"}
{"text_id": "8731", "text": "docstring: def _load_brief(self): url = self.PTN_GSE_BRIEF % {'id': self.id} handle = Download(url) http_fp = handle.read() self._parse_brief(http_fp) http_fp.close()"}
{"text_id": "8732", "text": "docstring: def kickedFrom(self, channel, kicker, message): self._runhandler( \"kickedFrom\", channel, kicker, self.factory.to_unicode(message) )"}
{"text_id": "8733", "text": "docstring: def main( dump: Iterable[mwxml.Page], features_output_h, stats_output_h, args) -> None: stats = { 'performance': { 'start_time': None, 'end_time': None, 'revisions_analyzed': 0, 'pages_analyzed': 0, }, } stats['performance']['start_time'] = datetime.datetime.utcnow() writer = csv.writer(features_output_h) pages_generator = extract_pages( dump, language=args.language, stats=stats, only_last_revision=args.only_last_revision, ) if args.change_bytes: args.ensure_sorted = True if args.change_bytes: writer.writerow(csv_output_fields_with_change) else: writer.writerow(csv_output_fields) for mw_page in pages_generator: page_revision_list = [] for revision in mw_page.revisions: if revision.user is None: user_type = 'None' user_username = 'None' user_id = -2 else: if revision.user.id is not None: user_type = 'registered' user_username = revision.user.text user_id = revision.user.id else: user_type = 'ip' user_username = revision.user.text user_id = -1 revision_parent_id = revision.parent_id if revision.parent_id is None: revision_parent_id = -1 if revision.minor: revision_minor = 1 else: revision_minor = 0 revout = [mw_page.id, mw_page.title, revision.id, revision.parent_id, revision.timestamp, user_type, user_username, user_id, revision_minor, revision.nbytes ] if args.ensure_sorted or args.change_bytes: page_revision_list.append(revout) else: writer.writerow(revout) if page_revision_list: page_revision_list.sort(key=lambda revout: arrow.get(revout[4])) prev_nbites = None for revout in page_revision_list: if args.change_bytes: nbytes = revout[9] if prev_nbites is None: change = nbytes else: change = nbytes - prev_nbites writer.writerow(revout + [change]) prev_nbites = nbytes else: writer.writerow(revout) stats['performance']['end_time'] = datetime.datetime.utcnow() with stats_output_h: dumper.render_template( stats_template, stats_output_h, stats=stats, )"}
{"text_id": "8734", "text": "docstring: def init_subdoc(app): if app.config.multidocs_is_master: app.info(bold(\"Compiling the master document\")) app.connect('env-updated', merge_environment) app.connect('html-collect-pages', merge_js_index) if app.config.multidocs_subdoc_list: def load_indexer(docnames): app.builder.info(bold('skipping loading of indexes... '), nonl=1) app.builder.load_indexer = load_indexer else: app.info(bold(\"Compiling a sub-document\")) app.connect('html-page-context', fix_path_html) if not app.config.multidoc_first_pass: app.connect('env-updated', fetch_citation) def link_static_files(): app.builder.info(bold('linking _static directory.')) static_dir = os.path.join(app.builder.outdir, '_static') master_static_dir = os.path.join('..', '_static') if os.path.lexists(static_dir): try: shutil.rmtree(static_dir) except OSError: os.unlink(static_dir) os.symlink(master_static_dir, static_dir) app.builder.copy_static_files = link_static_files if app.config.multidoc_first_pass == 1: app.config.intersphinx_mapping = {}"}
{"text_id": "8735", "text": "docstring: def put_data(self, src, overwrite=True, check_timestamp=True): if isinstance(src, basestring): src_timestamp = os.path.getmtime(src) self._metadata[\"__timestamp__\"] = src_timestamp self._metadata[\"__filename__\"] = src_filename = src src = open(src, \"r\") close_src = True else: src_filename = None src_timestamp = None close_src = False update = True if os.path.exists(self._path): if not overwrite: raise StorageObjectAlreadyExists(self._name) if check_timestamp: try: with open(\"{}.__metadata__\".format(self._path), \"r\") as f: metadata = json.load(f) dst_timestamp = metadata.get(\"__timestamp__\") dst_filename = metadata.get(\"__filename__\") update = src_filename is None or dst_filename is None or src_timestamp is None or dst_timestamp is None\\ or src_filename != dst_filename or src_timestamp != dst_timestamp except: pass if update: _ensure_path_exists(os.path.dirname(self._path)) with open(\"{}.__metadata__\".format(self._path), \"w\") as f: json.dump(self._metadata, f) with open(self._path, \"w\") as dest: buf = src.read(self._BUF_SIZE) while len(buf) > 0: dest.write(buf) buf = src.read(self._BUF_SIZE) if close_src: src.close()"}
{"text_id": "8736", "text": "docstring: def prepare_app(username: str, password: str) -> Tuple[flask.Flask, flask_basicauth.BasicAuth]: app = flask.Flask(defaults.APP_NAME) app.config['JSON_AS_ASCII'] = False app.json_encoder = rnd_utilities.CustomJsonEncoder app.config['BASIC_AUTH_USERNAME'] = username app.config['BASIC_AUTH_PASSWORD'] = password app.config['BASIC_AUTH_FORCE'] = True basic_auth = flask_basicauth.BasicAuth(app) return app, basic_auth"}
{"text_id": "8737", "text": "docstring: def update( self, subStyle ): if subStyle is None: return properties = self.propertyNames() for p in properties: if hasattr( subStyle, p ): v1 = getattr( self, p ) v2 = getattr( subStyle, p ) if v2 is not None: if isinstance( v1, SubStyle ): v1.update( v2 ) else: setattr( self, p, v2 )"}
{"text_id": "8738", "text": "docstring: def duty_cycle(self, duty: float) -> None: if self.state() == PinState.NOT_STARTED: raise RuntimeError(f\"Attempt to use pin ({self.pin_number}) that is not started\") if duty < 0 or duty > 1: raise ValueError(\"duty_cycle must be in range 0 to 1\") self.pca9685.set_duty_cycle(self.pin_number, duty) self._state = duty"}
{"text_id": "8739", "text": "docstring: def record_content_submission( dynamodb_table: Table, content_id: str, content_type: ContentType, content_ref: str, content_ref_type: ContentRefType, additional_fields: t.Set = set(), ): submit_time = datetime.datetime.now() ContentObject( content_id=content_id, content_type=content_type, content_ref=content_ref, content_ref_type=content_ref_type, additional_fields=additional_fields, submission_times=[submit_time], created_at=submit_time, updated_at=submit_time, ).write_to_table(dynamodb_table)"}
{"text_id": "8740", "text": "docstring: def peering_type(self) -> str: return pulumi.get(self, \"peering_type\")"}
{"text_id": "8741", "text": "docstring: def closed_task_states(self): names = self['providers']['snow']['closed_task_states'] states = self['providers']['snow']['states'] return [states[name] for name in names]"}
{"text_id": "8742", "text": "docstring: def _generate_single_view (self, id): if id in self._virtualizers: log.warning(\"Requested Virtualizer with ID: %s is already exist! \" \"Virtualizer creation skipped...\" % id) else: log.debug( \"Generating Single BiSBiS Virtualizer with id: %s\" % id) self._virtualizers[id] = SingleBiSBiSVirtualizer(self.dov, id) return self._virtualizers[id]"}
{"text_id": "8743", "text": "docstring: def to_dict(self) -> Dict: _dict = {} if hasattr(self, 'id') and self.id is not None: _dict['id'] = self.id if hasattr(self, 'dep_component_id') and self.dep_component_id is not None: _dict['dep_component_id'] = self.dep_component_id if hasattr(self, 'api_url') and self.api_url is not None: _dict['api_url'] = self.api_url if hasattr(self, 'display_name') and self.display_name is not None: _dict['display_name'] = self.display_name if hasattr(self, 'cluster_id') and self.cluster_id is not None: _dict['cluster_id'] = self.cluster_id if hasattr(self, 'cluster_name') and self.cluster_name is not None: _dict['cluster_name'] = self.cluster_name if hasattr(self, 'grpcwp_url') and self.grpcwp_url is not None: _dict['grpcwp_url'] = self.grpcwp_url if hasattr(self, 'location') and self.location is not None: _dict['location'] = self.location if hasattr(self, 'operations_url') and self.operations_url is not None: _dict['operations_url'] = self.operations_url if hasattr(self, 'orderer_type') and self.orderer_type is not None: _dict['orderer_type'] = self.orderer_type if hasattr(self, 'config_override') and self.config_override is not None: _dict['config_override'] = self.config_override if hasattr(self, 'consenter_proposal_fin') and self.consenter_proposal_fin is not None: _dict['consenter_proposal_fin'] = self.consenter_proposal_fin if hasattr(self, 'node_ou') and self.node_ou is not None: _dict['node_ou'] = self.node_ou.to_dict() if hasattr(self, 'msp') and self.msp is not None: _dict['msp'] = self.msp.to_dict() if hasattr(self, 'msp_id') and self.msp_id is not None: _dict['msp_id'] = self.msp_id if hasattr(self, 'resources') and self.resources is not None: _dict['resources'] = self.resources.to_dict() if hasattr(self, 'scheme_version') and self.scheme_version is not None: _dict['scheme_version'] = self.scheme_version if hasattr(self, 'storage') and self.storage is not None: _dict['storage'] = self.storage.to_dict() if hasattr(self, 'system_channel_id') and self.system_channel_id is not None: _dict['system_channel_id'] = self.system_channel_id if hasattr(self, 'tags') and self.tags is not None: _dict['tags'] = self.tags if hasattr(self, 'timestamp') and self.timestamp is not None: _dict['timestamp'] = self.timestamp if hasattr(self, 'type') and self.type is not None: _dict['type'] = self.type if hasattr(self, 'version') and self.version is not None: _dict['version'] = self.version if hasattr(self, 'zone') and self.zone is not None: _dict['zone'] = self.zone return _dict"}
{"text_id": "8744", "text": "docstring: def do_ipy(self, arg: str) -> None: from .pyscript_bridge import PyscriptBridge bridge = PyscriptBridge(self) if self.locals_in_py: def load_ipy(self, app): banner = 'Entering an embedded IPython shell type quit() or <Ctrl>-d to exit ...' exit_msg = 'Leaving IPython, back to {}'.format(sys.argv[0]) embed(banner1=banner, exit_msg=exit_msg) load_ipy(self, bridge) else: def load_ipy(app): banner = 'Entering an embedded IPython shell type quit() or <Ctrl>-d to exit ...' exit_msg = 'Leaving IPython, back to {}'.format(sys.argv[0]) embed(banner1=banner, exit_msg=exit_msg) load_ipy(bridge)"}
{"text_id": "8745", "text": "docstring: def local_master_repos(self, restore, extra_args): api = api_instance(TEMPLATE_ORG_NAME) template_repo_urls = [ api.insert_auth(url).replace(LOCAL_DOMAIN, BASE_DOMAIN) for url in api.get_repo_urls(assignment_names) ] git_commands = [ \"git clone {}\".format(url) for url in template_repo_urls ] result = run_in_docker( \" && \".join(git_commands), extra_args=extra_args ) assert result.returncode == 0 return assignment_names"}
{"text_id": "8746", "text": "docstring: def trainRom(self, samplerType, kwargs): self.raiseADebug(\"Start to train roms\") for romInfo in self.romsDictionary.values(): cvMetrics = romInfo['Instance'].convergence(self.tempTargetEvaluation) if cvMetrics is not None: converged = self.isRomConverged(cvMetrics) romInfo['Converged'] = converged if converged: romInfo['Instance'].reset() romInfo['Instance'].train(self.tempTargetEvaluation) self.raiseADebug(\"ROM \", romInfo['Instance'].name, \" is converged!\") else: self.raiseAMessage(\"Minimum initial training size is met, but the training size is not enough to be used to perform cross validation\") self.oldTrainingSize = len(self.tempTargetEvaluation)"}
{"text_id": "8747", "text": "docstring: def now(tz: tzinfo | None = None) -> str: return str(datetime.now(tz=tz).timestamp())"}
{"text_id": "8748", "text": "docstring: def row_formatted(df, formats={}, width=None): out = df.apply(lambda x: x.map(formats.get(x.name,'{}').format), axis=1) if width: out.index = out.index.str.slice(0, width) return out"}
{"text_id": "8749", "text": "docstring: def create_snmp_read_community(self, headers=None, payload=None, active_validation=True, **request_parameters): check_type(headers, dict) check_type(payload, list) if headers is not None: if 'Content-Type' in headers: check_type(headers.get('Content-Type'), basestring, may_be_none=False) if 'X-Auth-Token' in headers: check_type(headers.get('X-Auth-Token'), basestring, may_be_none=False) params = { } params.update(request_parameters) params = dict_from_items_with_values(params) path_params = { } _payload = payload or [] if active_validation: self._request_validator('jsd_7aa3da9d4e098ef2_v1_2_10')\\ .validate(_payload) with_custom_headers = False _headers = self._session.headers or {} if headers: _headers.update(dict_of_str(headers)) with_custom_headers = True e_url = ('/dna/intent/api/v1/global-credential/snmpv2-read-' + 'community') endpoint_full_url = apply_path_params(e_url, path_params) if with_custom_headers: json_data = self._session.post(endpoint_full_url, params=params, json=_payload, headers=_headers) else: json_data = self._session.post(endpoint_full_url, params=params, json=_payload) return self._object_factory('bpm_7aa3da9d4e098ef2_v1_2_10', json_data)"}
{"text_id": "8750", "text": "docstring: def smvJoinMultipleByKey(self, keys, joinType = 'inner'): jdf = self._jPythonHelper.smvJoinMultipleByKey(self._jdf, smv_copy_array(self._sc, *keys), joinType) return SmvMultiJoin(self._sql_ctx, jdf)"}
{"text_id": "8751", "text": "docstring: def save(self, output_directory): dump(self.classifier, path.join(output_directory, \"classifier.joblib\")) dump(self.binarizer, path.join(output_directory, \"binarizer.joblib\"))"}
{"text_id": "8752", "text": "docstring: def save(output, filename): with open(filename, 'w') as f: f.write(output) os.chmod(filename, 0o755)"}
{"text_id": "8753", "text": "docstring: def _BuildDataSource(self, task_id=None): return self._BuildDataSourceWithMetadata(task_id=task_id)['data']"}
{"text_id": "8754", "text": "docstring: def optimize_T_gaussian_width(tst_data, T0, gwidth0, func_z, max_iter=400, T_step_size=0.05, gwidth_step_size=0.01, batch_proportion=1.0, tol_fun=1e-3, reg=1e-5): X, Y = tst_data.xy() nx, d = X.shape J = T0.shape[0] T = theano.shared(T0, name='T') Xth = tensor.dmatrix('X') Yth = tensor.dmatrix('Y') it = theano.shared(1, name='iter') gamma_sq_init = gwidth0**0.5 gamma_sq_th = theano.shared(gamma_sq_init, name='gamma') regth = theano.shared(reg, name='reg') diag_regth = regth*tensor.eye(J) Z = func_z(Xth, Yth, T, tensor.sqr(gamma_sq_th)) W = old_div(Z.sum(axis=0),nx) Z0 = Z - W Sig = old_div(Z0.T.dot(Z0),nx) s = nlinalg.matrix_inverse(Sig + diag_regth).dot(W).dot(W)*nx gra_T, gra_gamma_sq = tensor.grad(s, [T, gamma_sq_th]) step_pow = 0.5 max_gam_sq_step = 1.0 func = theano.function(inputs=[Xth, Yth], outputs=s, updates=[ (T, T+T_step_size*gra_T/it**step_pow/tensor.sum(gra_T**2)**0.5 ), (it, it+1), (gamma_sq_th, gamma_sq_th+gwidth_step_size*tensor.sgn(gra_gamma_sq) \\ *tensor.minimum(tensor.abs_(gra_gamma_sq), max_gam_sq_step) \\ /it**step_pow) ] ) S = np.zeros(max_iter) J = T0.shape[0] Ts = np.zeros((max_iter, J, d)) gams = np.zeros(max_iter) for t in range(max_iter): ind = np.random.choice(nx, min(int(batch_proportion*nx), nx), replace=False) try: S[t] = func(X[ind, :], Y[ind, :]) except: print('Exception occurred during gradient descent. Stop optimization.') print('Return the value from previous iter. ') import traceback as tb tb.print_exc() t = t -1 break Ts[t] = T.get_value() gams[t] = gamma_sq_th.get_value()**2 if t >= 2 and abs(S[t]-S[t-1]) <= tol_fun: break S = S[:t+1] Ts = Ts[:t+1] gams = gams[:t+1] info = {'Ts': Ts, 'T0':T0, 'gwidths': gams, 'obj_values': S, 'gwidth0': gwidth0} if t >= 0: opt_T = Ts[-1] opt_gwidth = gams[-1] if util.is_real_num(gams[-1]) else gwidth0 if np.linalg.norm(opt_T) <= 1e-5: opt_T = T0 opt_gwidth = gwidth0 else: opt_T = T0 opt_gwidth = gwidth0 return (opt_T, opt_gwidth, info )"}
{"text_id": "8755", "text": "docstring: def plot_ci_bootstrap(xs, ys, resid, nboot=500, ax=None): if ax is None: ax = plt.gca() bootindex = sp.random.randint for _ in range(nboot): resamp_resid = resid[bootindex(0, len(resid) - 1, len(resid))] pc = np.polyfit(xs, ys + resamp_resid, 1) ax.plot(xs, np.polyval(pc, xs), \"b-\", linewidth=2, alpha=3.0 / float(nboot)) return ax"}
{"text_id": "8756", "text": "docstring: def fortran_str(obj, quotes=True): if isinstance(obj, str): return str_str(obj, quotes) if isinstance(obj, bool): return bool_str(obj) if '__iter__' in dir(obj): try: return arr_str(array(obj, dtype=np.double)) except: return ' '.join([fortran_str(o, quotes) for o in obj]) else: return str(obj)"}
{"text_id": "8757", "text": "docstring: def line_to_datetime(line): date_str = line.split(' ')[0] time_str = line.split(' ')[1] month, day, year = map(int, date_str.split('/')) hour, minute, second = map(int, time_str.split(':')) dt = datetime(year, month, day, hour, minute, second) return dt"}
{"text_id": "8758", "text": "docstring: def GenerateTweet(self): return self._MarkovChain.generateTweet()"}
{"text_id": "8759", "text": "docstring: def luserMe(self, info):"}
{"text_id": "8760", "text": "docstring: def start(self, instmt): instmt.set_prev_l2_depth(L2Depth(20)) instmt.set_l2_depth(L2Depth(20)) instmt.set_instmt_snapshot_table_name(self.get_instmt_snapshot_table_name(instmt.get_exchange_name(), instmt.get_instmt_name())) self.init_instmt_snapshot_table(instmt) return [self.api_socket.connect(self.api_socket.get_link(), on_message_handler=partial(self.on_message_handler, instmt), on_open_handler=partial(self.on_open_handler, instmt), on_close_handler=partial(self.on_close_handler, instmt))]"}
{"text_id": "8761", "text": "docstring: def delete_iterable_list_cache( cls, data_location: DataLocation, *settings_path: str, ) -> typing.Callable[[str], typing.Callable[[Context, typing.List[typing.Any]], None]]: def inner(value: typing.Any): def wrapper(ctx, data: list): if data_location == DataLocation.GUILD: d = ctx.bot.guild_settings[ctx.guild.id] elif data_location == DataLocation.USER: d = ctx.bot.user_settings[ctx.author.id] for i in settings_path[:-1]: d = d.setdefault(i, dict()) settings_list = d.setdefault(settings_path[-1], list()) if value not in settings_list: return else: settings_list.remove(value) return wrapper return inner"}
{"text_id": "8762", "text": "docstring: def mosaic(in_files:list,out_path:str) -> str: ext = os.path.splitext(out_path)[1] intermediate_path = out_path.replace(ext,\".vrt\") interim_path = out_path.replace(ext,f\".TEMP{ext}\") north= 8895604.157 west= -20015109.354 south = -6671703.118 east= 20015109.354 command = [\"gdalbuildvrt\",\"-te\",str(west),str(south),str(east),str(north),'-q',intermediate_path] #command = [\"gdalbuildvrt\",intermediate_path] command += in_files subprocess.call(command) subprocess.call([\"gdal_translate\",\"-co\", \"TILED=YES\",'-co',\"COPY_SRC_OVERVIEWS=YES\",'-co', \"COMPRESS=DEFLATE\",'-q', intermediate_path,out_path]) os.remove(intermediate_path) try: os.remove(intermediate_path+\".ovr\") except: pass with open(out_path,'rb') as rf: with open(interim_path,'wb') as wf: shutil.copyfileobj(rf,wf) ds = gdal.Open(interim_path,1) for i in range(ds.RasterCount): ds.GetRasterBand(i + 1).DeleteNoDataValue() ds = None subprocess.call([\"gdaladdo\",interim_path, \"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\", \"512\", \"1024\"]) ds = gdal.Open(interim_path,1) for i in range(ds.RasterCount): ds.GetRasterBand(i + 1).SetNoDataValue(-3000) ds = None subprocess.call([\"gdal_translate\",\"-co\", \"TILED=YES\",'-co',\"COPY_SRC_OVERVIEWS=YES\",'-co', \"COMPRESS=DEFLATE\",'-q', interim_path,out_path]) os.remove(interim_path) return out_path"}
{"text_id": "8763", "text": "docstring: def scanClusterCreated(cluster, created=None, token=None, args={}): return resource.scan(**{**{ 'type': 'object', 'index': 'cluster', 'indexValue': cluster, 'sort': 'created', 'sortValue': created, 'keyValue': token, }, **args})"}
{"text_id": "8764", "text": "docstring: def startCompound( self ): pass"}
{"text_id": "8765", "text": "docstring: def predict_dst(self, control=None, x0=None): control = self.U if control is None else control xt = self.X1[:, 0] if x0 is None else x0 res = [xt] for t in range(control.shape[1] - 1): _ct = control[:, t].reshape(self.shift + 1, -1) _xt = xt.reshape(self.shift + 1, -1) ups_t = np.einsum('sc,sm->scm', _ct, _xt).flatten() xt_1 = self.A @ xt + self.B @ ups_t xt = xt_1 res.append(xt_1) return np.array(res).T"}
{"text_id": "8766", "text": "docstring: def from_dict(cls, _dict: Dict) -> 'TransitGatewayReference': args = {} if 'crn' in _dict: args['crn'] = _dict.get('crn') else: raise ValueError('Required property \\'crn\\' not present in TransitGatewayReference JSON') if 'id' in _dict: args['id'] = _dict.get('id') else: raise ValueError('Required property \\'id\\' not present in TransitGatewayReference JSON') if 'name' in _dict: args['name'] = _dict.get('name') else: raise ValueError('Required property \\'name\\' not present in TransitGatewayReference JSON') return cls(**args)"}
{"text_id": "8767", "text": "docstring: def index_by_id(self, record_uuid): self.bulk_index([record_uuid])"}
{"text_id": "8768", "text": "docstring: def mark_bads_batch(subject_list, subjects_dir=None): for subj in subject_list: print(\"For subject %s\" % (subj)) if not subjects_dir: subjects_dir = os.environ['SUBJECTS_DIR'] dirname = subjects_dir + '/' + subj sub_file_list = os.listdir(dirname) for raw_fname in sub_file_list: if raw_fname.endswith('_bcc-raw.fif'): continue if raw_fname.endswith('-raw.fif'): print(\"Raw calculations for file %s\" % (dirname + '/' + raw_fname)) raw = mne.io.Raw(dirname + '/' + raw_fname, preload=True) raw.plot(block=True) print('The bad channels marked are %s ' % (raw.info['bads'])) save_fname = dirname + '/' + raw.filenames[0].split('/')[-1].split('-raw.fif')[0] + '_bcc-raw.fif' raw.save(save_fname) return"}
{"text_id": "8769", "text": "docstring: def _assert_user_and_group_in_same_backend( self, user_entity_id, user_driver, group_entity_id, group_driver): if user_driver is not group_driver: user_driver.get_user(user_entity_id) group_driver.get_group(group_entity_id) raise exception.CrossBackendNotAllowed(group_id=group_entity_id, user_id=user_entity_id)"}
{"text_id": "8770", "text": "docstring: def enable_metadata_reload(self): value = self.config.get(self.KEY_ENABLE_METADATA_RELOAD, False) return bool(value)"}
{"text_id": "8771", "text": "docstring: def cma_pre_lbfgs(x_init, fun=None, fun_grad=None, grad_lookup=None, options={}): x1 = cmaes(x_init, fun, options=options[\"cmaes\"]) lbfgs(x1, fun_grad=fun_grad, grad_lookup=grad_lookup, options=options[\"lbfgs\"])"}
{"text_id": "8772", "text": "docstring: def unsubscribe(self, feed_key=None, group_key=None, shared_user=None): if shared_user is not None and feed_key is not None: self._client.unsubscribe(\"{0}/f/{1}\".format(shared_user, feed_key)) elif group_key is not None: self._client.unsubscribe(\"{0}/g/{1}\".format(self._user, feed_key)) elif feed_key is not None: self._client.unsubscribe(\"{0}/f/{1}\".format(self._user, feed_key)) else: raise AdafruitIO_MQTTError(\"Must provide a feed_key or group_key.\")"}
{"text_id": "8773", "text": "docstring: def batch_arrays( arrays_x: Union[np.ndarray, List[np.ndarray]], y: np.ndarray, batch_size: int = 32, shuffle: bool = True, uniform_batch_size: bool = False, ) -> Tuple[np.ndarray, np.ndarray]: if isinstance(arrays_x, list): arrays_x = np.array(arrays_x, dtype=object) if shuffle: arrays_x, y = shuffle_multiple(arrays_x, y, numpy_indexing=False) fixed_shape = arrays_x[0].shape[1:] lengths = [x.shape[0] for x in arrays_x] unique_len = np.unique(lengths) x_dtype = arrays_x[0].dtype y_dtype = y.dtype xlist = [] ylist = [] for length in unique_len: idx = np.nonzero(lengths == length)[0] for b in range(0, len(idx), batch_size): batch_idx = idx[b : b + batch_size] size = batch_size if uniform_batch_size else len(batch_idx) _x = np.zeros((size, length) + fixed_shape, dtype=x_dtype) _y = np.zeros(size, dtype=y_dtype) _y[:size] = y[batch_idx] for i, j in enumerate(batch_idx): _x[i, ...] = arrays_x[j] xlist.append(_x) ylist.append(_y) x_batch = np.array(xlist, dtype=object) y_batch = np.array(ylist, dtype=y_dtype if uniform_batch_size else object) return x_batch, y_batch"}
{"text_id": "8774", "text": "docstring: def generate_root_cert( path: Path, country: str, state: str, locality: str, org_name: str, common_name: str ) -> typing.Tuple[\"cryptography.x509.Certificate\", rsa.RSAPrivateKey]: root_key = generate_rsa_key(path) subject = x509.Name( [ x509.NameAttribute(NameOID.COUNTRY_NAME, country), x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, state), x509.NameAttribute(NameOID.LOCALITY_NAME, locality), x509.NameAttribute(NameOID.ORGANIZATION_NAME, org_name), x509.NameAttribute(NameOID.LOCALITY_NAME, common_name), ] ) root_cert_builder = ( x509.CertificateBuilder() .subject_name(subject) .issuer_name(subject) .public_key(root_key.public_key()) .serial_number(x509.random_serial_number()) .not_valid_before(datetime.utcnow()) .not_valid_after(datetime.utcnow() + timedelta(days=365 * 4)) .add_extension(x509.BasicConstraints(ca=True, path_length=None), critical=True) ) root_cert = root_cert_builder.sign(root_key, hashes.SHA256()) pem = root_cert.public_bytes(serialization.Encoding.PEM) with open(path.parent / \"root.crt\", \"wb\") as pem_out: pem_out.write(pem) return root_cert, root_key"}
{"text_id": "8775", "text": "docstring: def duration(time): time = int(time) hours = time // 3600 mins = time % 3600 // 60 secs = time % 60 if len(str(hours)) == 1 and hours != 0: hours = f\"0{hours}\" if len(str(mins)) == 1: mins = f\"0{mins}\" if len(str(secs)) == 1: secs = f\"0{secs}\" if hours == 0: return f\"{mins}:{secs}\" else: return f\"{hours}:{mins}:{secs}\""}
{"text_id": "8776", "text": "docstring: def LineEndRectExtend(*args, **kwargs): return _stc.StyledTextCtrl_LineEndRectExtend(*args, **kwargs)"}
{"text_id": "8777", "text": "docstring: def remove_handler_for_watch(self, event_handler, watch): with self._lock: self._handlers[watch].remove(event_handler)"}
{"text_id": "8778", "text": "docstring: def main(): parser = argparse.ArgumentParser() common.config.add_argument(parser) parser.add_argument( '--poll-interval', type=float, default=2, help=\"The interval between polls\" ) args = parser.parse_args() api = args.config.create_context() account_id = args.config.active_account response = api.account.summary(account_id) last_transaction_id = response.get(\"lastTransactionID\", 200) while True: time.sleep(args.poll_interval) response = api.transaction.since( account_id, id=last_transaction_id ) for transaction in response.get(\"transactions\", 200): print(transaction.title()) last_transaction_id = response.get(\"lastTransactionID\", 200)"}
{"text_id": "8779", "text": "docstring: def _chained_exceptions(exc_info): yield exc_info exc_type, exc, exc_traceback = exc_info context = set() context.add(exc) while True: if exc.__suppress_context__: exc = exc.__cause__ else: exc = exc.__context__ if exc in context: break context.add(exc) if exc is None: break yield type(exc), exc, exc.__traceback__"}
{"text_id": "8780", "text": "docstring: def one_shot_iterator(dataset_factory, output_types, output_shapes, container=None, shared_name=None, name=None): result = _op_def_lib.apply_op(\"OneShotIterator\", dataset_factory=dataset_factory, output_types=output_types, output_shapes=output_shapes, container=container, shared_name=shared_name, name=name) return result"}
{"text_id": "8781", "text": "docstring: def flush(timeout, identifier): log(\"Plugin %s flushing timeout %s and identifier %s\" % PLUGIN_NAME, timeout, identifier)"}
{"text_id": "8782", "text": "docstring: def fetch_quantitative_questions(request): return JsonResponse({ str(question.id): { 'prompts': { code: translate(question.prompt, code) for code, _ in settings.LANGUAGES }, 'left-anchors': { code: translate(question.left_anchor, code) for code, _ in settings.LANGUAGES }, 'right-anchors': { code: translate(question.right_anchor, code) for code, _ in settings.LANGUAGES }, 'min-score': question.min_score, 'max-score': question.max_score, 'input-type': question.input_type, } for question in QuantitativeQuestion.objects.filter(active=True) })"}
{"text_id": "8783", "text": "docstring: def convert_der_sig(signature, as_hex=True): if not signature: return \"\" if USE_FASTECDSA: r, s = DEREncoder.decode_signature(bytes(signature)) else: sg, junk = ecdsa.der.remove_sequence(signature) if junk != b'': raise EncodingError(\"Junk found in encoding sequence %s\" % junk) r, sg = ecdsa.der.remove_integer(sg) s, sg = ecdsa.der.remove_integer(sg) sig = '%064x%064x' % (r, s) if as_hex: return sig else: return bytes.fromhex(sig)"}
{"text_id": "8784", "text": "docstring: def process_variable_state( self, var_ident_str: str, variable_state: Any, memory_trace: Set = None ) -> MutableMapping: if memory_trace is None: memory_trace = set() var_id = id(variable_state) if var_id in memory_trace: variable_type: str = self.REFERENCE_TYPE_STRING else: variable_type: str = self._search_type_string(variable_state) memory_trace.add(var_id) state_mapping: MutableMapping = { self.TYPE_HEADER: variable_type, self.PYTHON_ID_HEADER: var_id, self.COLOR_HEADER: self._color_mapping[var_ident_str], } state_mapping[self.REPR_HEADER] = self.custom_repr( variable_state, state_mapping[self.TYPE_HEADER], memory_trace ) if state_mapping[self.TYPE_HEADER] in self._GRAPH_OBJECT_TYPES: variable_state: Union[Node, Edge] state_mapping[self.GRAPH_ID_HEADER] = get_cytoscape_id( self._graph, variable_state, self.GRAPH_ID_HEADER ) state_mapping[self.GRAPH_PROPERTY_HEADER] = deepcopy( self._graph.nodes.get(variable_state, {}) ) return state_mapping"}
{"text_id": "8785", "text": "docstring: def rad_spc_alt(cls, im_gi, spc_fil): gxapi_cy.WrapFFT2._rad_spc_alt(GXContext._get_tls_geo(), im_gi, spc_fil.encode())"}
{"text_id": "8786", "text": "docstring: def create_port_precommit(self, context): return"}
{"text_id": "8787", "text": "docstring: def template_render(filename, template_path, render_type, context): print('..creating template for {0}'.format(filename)) env = Environment(loader=FileSystemLoader('{0}/{1}'.format(template_path, render_type))) env.filters['md5_hash'] = md5_hash env.filters['des_hash'] = des_hash env.filters['sha512_hash'] = sha512_hash template = env.get_template(filename) rendered_template = template.render(context) return rendered_template"}
{"text_id": "8788", "text": "docstring: def _writeXML(self,output,outputDictionary): if self.dynamic: outputInstance = xmlUtils.DynamicXmlElement('MetricPostProcessor', pivotParam=self.pivotParameter) else: outputInstance = xmlUtils.StaticXmlElement('MetricPostProcessor') if self.dynamic: for key, values in outputDictionary.items(): assert(\"|\" in key) metricName, nodeName = key.split('|') for ts, pivotVal in enumerate(self.pivotValues): if values.shape[0] == 1: outputInstance.addScalar(nodeName, metricName,values[0], pivotVal=pivotVal) else: outputInstance.addScalar(nodeName, metricName,values[ts], pivotVal=pivotVal) else: for key, values in outputDictionary.items(): assert(\"|\" in key) metricName, nodeName = key.split('|') if len(list(values)) == 1: outputInstance.addScalar(nodeName, metricName, values[0]) else: self.raiseAnError(IOError, \"Multiple values are returned from metric '\", metricName, \"', this is currently not allowed\") return outputInstance"}
{"text_id": "8789", "text": "docstring: def cb_cov_ell_cnt(self, _event): cnt = self.cov_ell_cnt.get() for sv in range(len(self._sv)): self._sv[sv].cov_ell_cnt = cnt for sgv in range(len(self._sgv)): self._sgv[sgv].cov_ell_cnt = cnt self.lbl_cov_ell_cnt_val.config(text=cnt) self.draw()"}
{"text_id": "8790", "text": "docstring: def pos_marker(self): if hasattr(self, \"segment\"): return getattr( self.segment.pos_marker, \"source_pos_marker\", self.segment.pos_marker ) elif hasattr(self, \"pos\"): return self.pos else: return None"}
{"text_id": "8791", "text": "docstring: def Transform(self, emitter, registers, input_address, elements, output_address): emitter.EmitNewline() emitter.EmitComment('MinMax::Transform') register_count = (elements + 15) / 16 load = [registers.QuadRegister() for unused_i in range(register_count)] emitter.EmitVLoadAE(8, elements, load, input_address, None) emitter.EmitPldOffset(input_address, emitter.ImmediateConstant(16)) for register in load: emitter.EmitVMax('u8', register, register, self.min) for register in load: emitter.EmitVMin('u8', register, register, self.max) emitter.EmitNewline() emitter.EmitVStoreAE(8, elements, load, output_address, None) emitter.EmitPld(output_address) registers.FreeRegisters(load)"}
{"text_id": "8792", "text": "docstring: def visualize(self, ax): ax.set_xlim(0, self.width) ax.set_ylim(0, self.height) for p in self.patches: polygon = p[\"polygon\"] center = np.mean(polygon.get_xy(),0) patch = ax.add_patch(polygon) ax.text(center[0],center[1],p[\"name\"])"}
{"text_id": "8793", "text": "docstring: def _convert_config(cfg: OmegaConf): if 'cls' in cfg and \"_target_\" not in cfg: cfg._target_ = cfg.pop(\"cls\") if 'params' in cfg: params = cfg.pop('params') for param_key, param_val in params.items(): cfg[param_key] = param_val try: for _, sub_cfg in cfg.items(): if isinstance(sub_cfg, DictConfig): _convert_config(sub_cfg) except omegaconf_errors.OmegaConfBaseException as e: logging.warning(f\"Skipping config conversion for cfg:\\n{cfg}\\n due to OmegaConf error encountered :\\n{e}.\")"}
{"text_id": "8794", "text": "docstring: def S_to_R(S, w, t): dw = w[1] - w[0] fac = np.ones(len(w)) fac[1: len(w) - 1: 2] = 4 fac[2: len(w) - 2: 2] = 2 fac = fac * dw / 3 R = np.zeros(len(t)) for i in range(len(t)): R[i] = 2 * np.dot(fac, S * np.cos(w * t[i])) return R"}
{"text_id": "8795", "text": "docstring: def __validate_ensure_param_space_bounds(self, param_space, key, bounds): if type(param_space[key]) is not list: raise TypeError(\"Please provide a numerical list of length two for [{0:s}]\".format( key)) if len(param_space[key]) is not 2: raise TypeError(\"Please provide a numerical list of length two for [{0:s}]\".format( key)) if not all(isinstance(eelement, numbers.Real) for eelement in param_space[key]): raise TypeError(\"Please provide a numerical list of length two for [{0:s}]\".format( key)) if param_space[key][0] > param_space[key][1]: aux = param_space[key][0] param_space[key][0] = param_space[key][1] param_space[key][1] = aux if param_space[key][0] < bounds[0]: if param_space[key][1] < bounds[0]: param_space = self.__validate_set_param_space_with_warning( param_space, key, [bounds[0], bounds[0]]) else: param_space = self.__validate_set_param_space_with_warning( param_space, key, [bounds[0], param_space[key][1]]) if param_space[key][0] > bounds[1]: param_space = self.__validate_set_param_space_with_warning( param_space, key, [bounds[1], bounds[1]]) if param_space[key][0] < bounds[0]: param_space = self.__validate_set_param_space_with_warning( param_space, key, [bounds[0], bounds[0]]) if param_space[key][1] > bounds[1]: param_space = self.__validate_set_param_space_with_warning( param_space, key, [param_space[key][0], bounds[1]]) return param_space"}
{"text_id": "8796", "text": "docstring: def to_config(self, cfg_file=None, section_name=None, section_comment=None, section_level=0, append=False, quiet=False, exclude_defaults=False, include_descr=True): if cfg_file is not None and os.path.isfile(cfg_file) and not append and not quiet: warnings.warn('Selected configuration file already exists and will be overwritten!') config_output = [] if numpy.all([ isinstance(d, ParSet) or d is None for d in self.data.values() ]): for k in self.keys(): if self.data[k] is None: continue section_comment = self.descr[k] if include_descr else None config_output += ParSet.config_lines(self.data[k], section_name=k, section_comment=section_comment, section_level=section_level, exclude_defaults=exclude_defaults, include_descr=include_descr) else: if section_name is None and self.cfg_section is None: raise ValueError('No top-level section name available for configuration!') _section_name = self.cfg_section if section_name is None else section_name _section_comment = self.cfg_comment if section_comment is None else section_comment config_output += ParSet.config_lines(self, section_name=_section_name, section_comment=_section_comment, section_level=section_level, exclude_defaults=exclude_defaults, include_descr=include_descr) if cfg_file is None: return config_output with open(cfg_file, 'a' if append else 'w') as f: f.write('\\n'.join(config_output))"}
{"text_id": "8797", "text": "docstring: def train(self) -> None: self.logger.info(self.config.device) trainLogger = logging.getLogger(TRAIN_LOGGER_NAME) trainLogger.info(\"epoch,loss\") validationLogger = logging.getLogger(VALIDATION_LOGGER_NAME) validationLogger.info(\"epoch,f1,accuracy\") bestF1 = float('-inf') for epoch in range(1, self.config.epochs + 1): epochStartTime = time.time() trainLoss = self.trainOneEpoch() trainLogger.info(\"%d,%f\", epoch, trainLoss) run_time = time.time() - epochStartTime self.logger.info('[%d/%d], loss: %f, time:%f', epoch, self.config.epochs, trainLoss, run_time) if epoch > 1 and self.config.modelSaveEpoch > 0 and epoch % self.config.modelSaveEpoch == 0: torch.save(self.model.state_dict(), self.config.outDir / Path('epoch_{}.pth'.format(epoch))) self.logger.info('Epoch %d: model saved', epoch) if self.config.validationEpoch > 0 and epoch % self.config.validationEpoch == 0: f1, acc = self.validateOneEpoch() self.logger.info(\"Epoch %d -- f1: %f, acc: %s\", epoch, f1, acc) if f1 > bestF1: bestF1 = f1 torch.save({'epoch': epoch, 'model_state_dict': self.model.state_dict(), }, self.config.outDir / 'best_f1.pth') self.logger.info('%d: Updated best model', epoch) validationLogger.info(\"%d,%f,%f\", epoch, f1, acc)"}
{"text_id": "8798", "text": "docstring: def write_grade(self, assignment, account, grade): results_path = self._get_submission_config(assignment, account).get('Storer', 'ResultsDest') if results_path is None: return if not os.path.exists(results_path): os.makedirs(results_path) with open(os.path.join(results_path, 'grade.vmr'), 'wt') as f: f.write(grade)"}
{"text_id": "8799", "text": "docstring: def cb_start(self, update, context): chat_id = update.message.chat_id if chat_id in self.clientChatIds: update.message.reply_text('You are already subscribed.') return print(f\"Subscribing chat_id '{chat_id}'\") self.clientChatIds.add(chat_id) update.message.reply_text(\"You have successfully subscribed.\") self.saveToFile(self.configFile)"}
{"text_id": "8800", "text": "docstring: def torso_visible(keypoints): return ((keypoints[KEYPOINT_DICT['left_hip'], 2] > MIN_CROP_KEYPOINT_SCORE or keypoints[KEYPOINT_DICT['right_hip'], 2] > MIN_CROP_KEYPOINT_SCORE) and (keypoints[KEYPOINT_DICT['left_shoulder'], 2] > MIN_CROP_KEYPOINT_SCORE or keypoints[KEYPOINT_DICT['right_shoulder'], 2] > MIN_CROP_KEYPOINT_SCORE))"}
{"text_id": "8801", "text": "docstring: def random(m, n, density=0.01, format='coo', dtype=None, random_state=None, data_rvs=None): if density < 0 or density > 1: raise ValueError(\"density expected to be 0 <= density <= 1\") dtype = np.dtype(dtype) if dtype.char not in 'fdg': raise NotImplementedError(\"type %s not supported\" % dtype) mn = m * n tp = np.intc if mn > np.iinfo(tp).max: tp = np.int64 if mn > np.iinfo(tp).max: msg = \"\"\"\\ Trying to generate a random sparse matrix such as the product of dimensions is greater than %d - this is not supported on this machine \"\"\" raise ValueError(msg % np.iinfo(tp).max) k = int(density * m * n) if random_state is None: random_state = np.random elif isinstance(random_state, (int, np.integer)): random_state = np.random.RandomState(random_state) if data_rvs is None: data_rvs = random_state.rand if mn < 3*k: ind = random_state.permutation(mn)[:k] else: ind = np.empty(k, dtype=tp) selected = set() for i in xrange(k): j = random_state.randint(mn) while j in selected: j = random_state.randint(mn) selected.add(j) ind[i] = j j = np.floor(ind * 1. / m).astype(tp) i = (ind - j * m).astype(tp) vals = data_rvs(k).astype(dtype) return coo_matrix((vals, (i, j)), shape=(m, n)).asformat(format)"}
{"text_id": "8802", "text": "docstring: def save(self): if self.ticker_storage: start_delays = dict((interval, ticker.task.next_call_time()) for interval, ticker in self.ticker_pool.tickers.items()) for store_key, (args, kwargs) in self.ticker_storage.items(): interval = store_key[1] kwargs[\"_start_delay\"] = start_delays.get(interval, None) ServerConfig.objects.conf(key=self.save_name, value=dbserialize(self.ticker_storage)) else: ServerConfig.objects.conf(key=self.save_name, delete=True)"}
{"text_id": "8803", "text": "docstring: def mode(self,col:Union[int,str,tuple,None]=None,get:[0,1,2]=1): from .stats.mode import mode return mode(self,col,get,Matrix,dataframe)"}
{"text_id": "8804", "text": "docstring: def eval_grad_f(self, Y): Y = UTPM.init_jacobian(Y) retval = self.eval_f(Y) return UTPM.extract_jacobian(retval)"}
{"text_id": "8805", "text": "docstring: async def create( self, scope: str, policy_assignment_name: str, parameters: \"models.PolicyAssignment\", **kwargs ) -> \"models.PolicyAssignment\": cls = kwargs.pop('cls', None) error_map = { 401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError } error_map.update(kwargs.pop('error_map', {})) api_version = \"2019-09-01\" content_type = kwargs.pop(\"content_type\", \"application/json\") accept = \"application/json\" url = self.create.metadata['url'] path_format_arguments = { 'scope': self._serialize.url(\"scope\", scope, 'str', skip_quote=True), 'policyAssignmentName': self._serialize.url(\"policy_assignment_name\", policy_assignment_name, 'str'), } url = self._client.format_url(url, **path_format_arguments) query_parameters = {} query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str') header_parameters = {} header_parameters['Content-Type'] = self._serialize.header(\"content_type\", content_type, 'str') header_parameters['Accept'] = self._serialize.header(\"accept\", accept, 'str') body_content_kwargs = {} body_content = self._serialize.body(parameters, 'PolicyAssignment') body_content_kwargs['content'] = body_content request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs) pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs) response = pipeline_response.http_response if response.status_code not in [201]: map_error(status_code=response.status_code, response=response, error_map=error_map) raise HttpResponseError(response=response, error_format=ARMErrorFormat) deserialized = self._deserialize('PolicyAssignment', pipeline_response) if cls: return cls(pipeline_response, deserialized, {}) return deserialized"}
{"text_id": "8806", "text": "docstring: def calculate_firing_rate(kernels, stimulus, t, r0=0.0, L0=0.0, G=1.0): rate = np.zeros([t.size, len(kernels)]) for i, t in enumerate(t): rate[i, ...] = [np.sum(kernel * stimulus[i, ...]) for kernel in kernels] rate += r0 rate[rate < L0] = 0 return G*rate"}
{"text_id": "8807", "text": "docstring: def update_translations(request): FieldTranslation.delete_orphan_translations() num_translations = FieldTranslation.update_translations() return render_to_response('modeltranslation/admin/update_translations_ok.html',{\"num_translations\":num_translations}, RequestContext(request))"}
{"text_id": "8808", "text": "docstring: def orders(self) -> List[Order]: return store.orders.get_orders(self.exchange, self.symbol)"}
{"text_id": "8809", "text": "docstring: def load_timeseries(timeseries_file, ts='roi'): if (ts == 'roi') or (ts == 'voxel'): timeseries = np.load(timeseries_file)['roi'] return timeseries else: print('You have not selected a valid timeseries type.' + 'options are ts=\\'roi\\' or ts=\\'voxel\\'.') pass"}
{"text_id": "8810", "text": "docstring: def make_frequentist_estimation(n_lam_array: Sequence[float], n_trans_array: Sequence[float]) \\ -> Tuple[Sequence[float], Optional[Sequence[ScipyDistribution]]]: point_estimates = [float(n_lam) / (n_lam + n_trans) for n_lam, n_trans in zip(n_lam_array, n_trans_array)] distrs = None return point_estimates, distrs"}
{"text_id": "8811", "text": "docstring: def load_graph(fpath): output_vars_map = [] output_vars_list = [] if isinstance(fpath, str): buf = open(fpath, \"rb\").read() else: buf = fpath.read() cg = _imperative_rt.load_graph(buf, output_vars_map, output_vars_list) return CompGraphLoadResult(cg, dict(output_vars_map), output_vars_list)"}
{"text_id": "8812", "text": "docstring: def ctcpMakeQuery(self, user, messages): self.msg(user, ctcpStringify(messages))"}
{"text_id": "8813", "text": "docstring: def cors_dict__(origin=None): origin = '*' if origin is None else origin cors_string = 'Origin, Accept , Content-Type, X-Requested-With, X-CSRF-Token' CORS_HEADERS = {'Access-Control-Allow-Methods': 'POST, OPTIONS, GET', 'Access-Control-Allow-Headers': cors_string, 'Access-Control-Allow-Credentials': 'true', 'Access-Control-Allow-Origin': origin} return CORS_HEADERS"}
{"text_id": "8814", "text": "docstring: def zincrby(self, *args): if self._cluster: return self.execute(b'ZINCRBY', *args, shard_key=args[0]) return self.execute(b'ZINCRBY', *args)"}
{"text_id": "8815", "text": "docstring: def Slice(onnx_node, ng_inputs): x = ng_inputs[0] starts = onnx_node.get_attribute_value('starts') ends = onnx_node.get_attribute_value('ends') if not (starts and ends and len(starts) == len(ends)): raise ValueError('Slice node (%s): attributes `starts` and `ends` must be set ' 'and of equal length.', onnx_node.name) axes = onnx_node.get_attribute_value('axes', list(range(len(starts)))) slices_count = max(len(axes), *starts) if slices_count > len(x.axes): raise ValueError('Slice node (%s): specifies %d slices, there are only %d input axes.', onnx_node.name, slices_count, len(x.axes)) slices = [slice(starts[axes.index(axis_number)], ends[axes.index(axis_number)]) if (axis_number in axes) else slice(None) for axis_number in range(len(x.axes))] return cast_to_pos_axes(ng.tensor_slice(x, slices))"}
{"text_id": "8816", "text": "docstring: def categorize_folders(root: FilePath, labels: pd.DataFrame, *, suffix: Optional[str] = None) -> None: root = Path(root) sample_names = [str(item) for item in labels.iloc[:, 0]] target_names = [str(item) for item in labels.iloc[:, 1]] for klass in set(target_names): cat_dir = root / klass if cat_dir.is_dir(): return cat_dir.mkdir(parents=True, exist_ok=True) for sample_name, target_name in zip(sample_names, target_names): src = root / sample_name if suffix is not None: src = src.with_suffix(suffix) shutil.move(src, root / target_name)"}
{"text_id": "8817", "text": "docstring: def _AddRateToSummary(tag, rate, step, sw): sw.add_summary( summary_pb2.Summary(value=[summary_pb2.Summary.Value( tag=tag, simple_value=rate)]), step)"}
{"text_id": "8818", "text": "docstring: def copy_all_char_to_wtf(basefolder, classdict, file): src = os.path.join(masterfolder, file) for chars in classdict.values(): for char in chars: dst = os.path.join(basefolder, char) shutil.copy(src, dst)"}
{"text_id": "8819", "text": "docstring: def is_locked(self): if isinstance(self.locked_at, datetime): _tue = timedelta(seconds=settings.LOCKING['time_until_expiration']) if (datetime.today() - self.locked_at) < _tue: return True else: return False return False"}
{"text_id": "8820", "text": "docstring: def check_health_callback(self, kwargs): try: if self.check_classifier_health(): self.check_if_trained(None) self.timer_handle_list.append( self.run_in(self.check_health_callback, self.run_in_delay) ) except requests.exceptions.HTTPError as exception: self.log( \"Error trying to turn on entity. Will try again in 1s. Error: {}\".format( exception ), level=\"WARNING\", ) self.timer_handle_list.append(self.run_in(self.check_health_callback, 1))"}
{"text_id": "8821", "text": "docstring: def pull_device_data(): devices = get_attached_devices() data = { 'qr': [], 'obj_pit': [], 'subj_pit': [] } if not devices: return data device_file_paths = [] device_file_path = utils.create_file_path('data/tablets') pull_device_files(device_file_path, '/storage/emulated/0/Download') for device_dir in os.listdir(device_file_path): if device_dir in TABLET_SERIAL_NUMBERS.keys(): device_file_paths.append(device_dir) for device in device_file_paths: download_directory = os.path.join(device_file_path, device) for file in os.listdir(download_directory): for dataset, pattern in FILENAME_REGEXES.items(): if re.fullmatch(pattern, file): with open(os.path.join(download_directory, file)) as data_file: if dataset == 'qr': file_contents = data_file.read().rstrip('\\n') else: file_contents = json.load(data_file) data[dataset].append(file_contents) break data['qr'] = qr_code_uploader.upload_qr_codes(data['qr']) for dataset in ['obj_pit', 'subj_pit']: current_data = local_database_communicator.read_dataset(dataset) modified_data = [] for datapoint in data[dataset]: if datapoint in current_data: continue local_database_communicator.update_dataset( f'raw.{dataset}', datapoint, {'team_number': datapoint['team_number']} ) modified_data.append({'team_number': datapoint['team_number']}) utils.log_info(f'{len(modified_data)} items uploaded to {dataset}') data[dataset] = modified_data return data"}
{"text_id": "8822", "text": "docstring: def deserialize(self, str): if python3: codecs.lookup_error(\"rosmsg\").msg_type = self._type try: if self.header is None: self.header = std_msgs.msg.Header() if self.goal_id is None: self.goal_id = actionlib_msgs.msg.GoalID() if self.goal is None: self.goal = fundamentals.msg.MissionStatusGoal() end = 0 _x = self start = end end += 12 (_x.header.seq, _x.header.stamp.secs, _x.header.stamp.nsecs,) = _get_struct_3I().unpack(str[start:end]) start = end end += 4 (length,) = _struct_I.unpack(str[start:end]) start = end end += length if python3: self.header.frame_id = str[start:end].decode('utf-8', 'rosmsg') else: self.header.frame_id = str[start:end] _x = self start = end end += 8 (_x.goal_id.stamp.secs, _x.goal_id.stamp.nsecs,) = _get_struct_2I().unpack(str[start:end]) start = end end += 4 (length,) = _struct_I.unpack(str[start:end]) start = end end += length if python3: self.goal_id.id = str[start:end].decode('utf-8', 'rosmsg') else: self.goal_id.id = str[start:end] start = end end += 4 (self.goal.value,) = _get_struct_i().unpack(str[start:end]) return self except struct.error as e: raise genpy.DeserializationError(e)"}
{"text_id": "8823", "text": "docstring: def create_strategy_variant_request(variant_id, strategy, legs): strategy_variant_request = { 'variantId': variant_id, 'strategy': strategy, 'legs': legs } return strategy_variant_request"}
{"text_id": "8824", "text": "docstring: def insert_many(table: str, args: list, batch_size=100): cache_stable = _get_table_cache(table) if cache_stable is None: raise TDError(f\"Table '{table}' does not exist \uff0cplease use 'insert_many_with_stable' function instead.\") return _insert_many(table, cache_stable, args, batch_size=batch_size)"}
{"text_id": "8825", "text": "docstring: def evaluate(defaults: typing.List, test_data: pd.Series) -> float: return max(test_data[defaults])"}
{"text_id": "8826", "text": "docstring: def register_reader(format, cls=None): fmt, is_compound = _normalize_format(format) def decorator(reader): format_class = _formats.setdefault(fmt, {}).setdefault(cls, {}) if 'reader' in format_class: raise DuplicateRegistrationError('reader', fmt, cls) if cls is None: def wrapped_reader(fp, mode='U', mutate_fh=False, **kwargs): if not _is_iter_list(fp): fp = [fp] with open_files(fp, mode) as fhs: generator = reader(*fhs, **kwargs) if mutate_fh or (not is_compound and _is_string_or_bytes(fp[0])): while True: yield next(generator) else: orig_positions = [fh.tell() for fh in fhs] read_positions = orig_positions try: while True: orig_positions = [fh.tell() for fh in fhs] for fh, pos in zip(fhs, read_positions): fh.seek(pos) next_result = next(generator) read_positions = [fh.tell() for fh in fhs] for fh, pos in zip(fhs, orig_positions): fh.seek(pos) yield next_result finally: for fh, pos in zip(fhs, orig_positions): fh.seek(pos) else: def wrapped_reader(fp, mode='U', mutate_fh=False, **kwargs): if not _is_iter_list(fp): fp = [fp] with open_files(fp, mode) as fhs: orig_positions = [fh.tell() for fh in fhs] result = reader(*fhs, **kwargs) if not mutate_fh: for fh, pos in zip(fhs, orig_positions): fh.seek(pos) return result wrapped_reader.__doc__ = reader.__doc__ wrapped_reader.__name__ = reader.__name__ format_class['reader'] = wrapped_reader format_class['reader_args'] = _factor_format(format) return wrapped_reader return decorator"}
{"text_id": "8827", "text": "docstring: def weighted_sum_images(images, weights): assert len(weights) == len(images) nonzero_indices = np.nonzero(weights)[0] if len(nonzero_indices) < 2: raise Exception('At least 2 non-zero weights are required') first, second = nonzero_indices[:2] res = cv2.addWeighted(images[first], weights[first], images[second], weights[second], 0) if len(nonzero_indices) == 2: return res for i in nonzero_indices[2:]: res = cv2.addWeighted(res, 1., images[i], weights[i], 0) return res"}
{"text_id": "8828", "text": "docstring: def checkAllAssertionsForDirectory( relTestcaseDir: str, extension, pattern: str = '', expectedIssuesFileMap={}, expectedMetricsFileMap={}): test_files = getTestFiles( relTestcaseDir, relative=True, extension=extension, pattern=pattern, ) l = [] for test_file in test_files: basename = os.path.basename(test_file) if basename in expectedIssuesFileMap: issue_map = expectedIssuesFileMap[basename] else: full_filename = getTestFile(test_file) issue_map = extractExpectedIssuesMapFromFile(full_filename) if basename in expectedMetricsFileMap: metrics_map = expectedMetricsFileMap[basename] else: full_filename = getTestFile(test_file) metrics_map = extractExpectedMetricsMapFromFile(full_filename) l.append((test_file, issue_map, metrics_map)) return l"}
{"text_id": "8829", "text": "docstring: def spec(self): attr = self._attr spec = '((\"doc\"->>\\'{attr}\\')::{cast})' return spec.format(attr=attr.slug, cast=attr.db_cast)"}
{"text_id": "8830", "text": "docstring: def quantize(self): self._load_model_data() self._collect_target_varnames() self._set_activation_persistable() if self._algo in [\"KL\", \"hist\"]: _logger.info(\"Preparation stage ...\") batch_id = 0 for data in self._data_loader(): self._executor.run(program=self._program, feed=data, fetch_list=self._fetch_list, return_numpy=False, scope=self._scope) self._collect_activation_abs_min_max() if batch_id % 5 == 0: _logger.info(\"Run batch: \" + str(batch_id)) batch_id += 1 if self._batch_nums and batch_id >= self._batch_nums: break _logger.info(\"Finish preparation stage, all batch:\" + str(batch_id)) self._init_sampling_act_histogram() _logger.info(\"Sampling stage ...\") batch_id = 0 for data in self._data_loader(): self._executor.run(program=self._program, feed=data, fetch_list=self._fetch_list, return_numpy=False, scope=self._scope) self._sampling() if batch_id % 5 == 0: _logger.info(\"Run batch: \" + str(batch_id)) batch_id += 1 if self._batch_nums and batch_id >= self._batch_nums: break _logger.info(\"Finish sampling stage, all batch: \" + str(batch_id)) self._reset_activation_persistable() if self._algo == 'avg': for var_name in self._quantized_act_var_name: self._quantized_threshold[var_name] = \\ np.array(self._quantized_var_avg[var_name]).mean() if self._algo in [\"KL\", \"hist\"]: self._calculate_kl_hist_threshold() if self._algo in [\"KL\", \"abs_max\", \"hist\", \"avg\", \"mse\"]: self._update_program() else: self._save_input_threhold() self._save_output_threshold() if any(op_type in self._quantizable_op_type for op_type in self._dynamic_quantize_op_type): self._collect_dynamic_quantize_op_threshold( self._dynamic_quantize_op_type) return self._program"}
{"text_id": "8831", "text": "docstring: def multisine_time(f1, f2, N, ms_type='full'): f0 = (f2-f1) / N linesMin = np.ceil(f1/f0).astype(int) linesMax = np.floor(f2/f0).astype(int) lines = np.arange(linesMin, linesMax, dtype=int) if lines[0] == 0: lines = lines[1:] phases = 2*np.pi*np.random.rand(len(lines)) def fex(t): try: return (np.sum(np.cos(2*np.pi*f0*np.outer(t, lines) + phases), axis=1) / np.sqrt(len(lines))) except MemoryError: f = 0 for phase, line in zip(phases, lines): f += np.cos(2*np.pi*f0*t*line + phase) return f / np.sqrt(len(lines)) return fex, lines"}
{"text_id": "8832", "text": "docstring: async def create_movie( movie_request: MovieCreate, api_key: FastApiKey = Depends(validate_public_key), db: Session = Depends(get_db) ) -> dict: db_existing_key = ApiKeyRepo.fetch_by_public(db, api_key[7:]) if db_existing_key: db_movie = MovieRepo.fetch_by_title_and_subtitle(db, title=movie_request.title, subtitle=movie_request.subtitle) if db_movie: raise HTTPException(status_code=400, detail=\"Movie already exists!\") else: raise HTTPException( status_code=HTTP_403_FORBIDDEN, detail=\"Could not validate credentials\" ) return await MovieRepo.create(db, movie=movie_request)"}
{"text_id": "8833", "text": "docstring: def error(self, caller, etype, message, tag='ERROR', verbosity='silent', color=None): verbval = max(self.getDesiredVerbosity(caller),self.checkVerbosity(self.verbosity)) self.message(caller,message,tag,verbosity,color=color) if not self.suppressErrs: self.printWarnings() if verbval<3: sys.tracebacklimit=0 raise etype(message)"}
{"text_id": "8834", "text": "docstring: def strip_comment_line_with_symbol(line, start): parts = line.split(start) counts = [len(re.findall(r'(?:^|[^\"\\\\]|(?:\\\\\\\\|\\\\\")+)(\")', part)) for part in parts] total = 0 for nr, count in enumerate(counts): total += count if total % 2 == 0: return start.join(parts[:nr + 1]).rstrip() else: return line.rstrip()"}
{"text_id": "8835", "text": "docstring: def update_or_create(self, model, data, fields=None, nocache=False, manager='objects', database=None, serializer_cls=None, *args, **kwargs): try: instance, many = self.get_instance(data) except self.model.DoesNotExist: instance, many = None, False return self.perform_changes(instance=instance, data=data, many=many, allow_add_remove=many)"}
{"text_id": "8836", "text": "docstring: def _get_gen_sizes(self): gens = [] with open(self.global_log) as fin: for line in fin: if 'evaluations so far.' in line: gens.append(int(line.split()[-4]) - sum(gens)) if 'Global Log ' in line: gens = [] return gens"}
{"text_id": "8837", "text": "docstring: def peek_level(self, level): for layer in self.m_stack: if layer and layer[0][0] == level: return [node for _, node in layer]"}
{"text_id": "8838", "text": "docstring: def visualize(model): if model.method == 'LDA': return reducer = umap.UMAP() print('Calculating UMAP projection ...') vec_umap = reducer.fit_transform(model.vec[model.method]) print('Calculating UMAP projection. Done!') plot_proj(vec_umap, model.cluster_model.labels_) dr = '/contextual_topic_identification/docs/images/{}/{}'.format(model.method, model.id) if not os.path.exists(dr): os.makedirs(dr) plt.savefig(dr + '/2D_vis')"}
{"text_id": "8839", "text": "docstring: def mock_home(monkeypatch, tmp_path): home_path = tmp_path.joinpath(\"home\") config_path = home_path.joinpath(\".config\", \"parboil\", \"templates\") config_path.mkdir(parents=True) monkeypatch.setenv(\"HOME\", str(home_path)) monkeypatch.setenv(\"USERPROFILE\", str(home_path)) monkeypatch.setattr(Path, \"home\", lambda: home_path)"}
{"text_id": "8840", "text": "docstring: def assert_keys_equal(result_keys: List[str], target_keys: List[str]) -> bool: return set(result_keys) == set(target_keys)"}
{"text_id": "8841", "text": "docstring: def attname_to_anum(self, relname, attname): if attname == 'oid': return 'ObjectIdAttributeNumber' if relname == 'gp_distribution_policy': relname = 'gp_policy' elif relname == 'pg_stat_last_operation': relname = 'pg_statlastop' elif relname == 'pg_stat_last_shoperation': relname = 'pg_statlastshop' return 'Anum_{relname}_{attname}'.format( relname=relname, attname=attname)"}
{"text_id": "8842", "text": "docstring: def _validation_step(self, batch): pass"}
{"text_id": "8843", "text": "docstring: def increment_filename(self, filename, path='', insert=''): path = path.strip('/') basename, dot, ext = filename.partition('.') suffix = dot + ext for i in itertools.count(): if i: insert_i = '{}{}'.format(insert, i) else: insert_i = '' name = u'{basename}{insert}{suffix}'.format(basename=basename, insert=insert_i, suffix=suffix) if not self.exists(u'{}/{}'.format(path, name)): break return name"}
{"text_id": "8844", "text": "docstring: def compute_bounds(feed: \"Feed\", stop_ids: Optional[list[str]] = None) -> np.array: from .stops import geometrize_stops return geometrize_stops(feed, stop_ids=stop_ids).total_bounds"}
{"text_id": "8845", "text": "docstring: def score_plenND(hmm, state_sequences): plens = [] for seqid in range(len(state_sequences)): pth = [x[0] for x in groupby(state_sequences[seqid])] plen = len(pth) plens.append(plen) return np.array(plens)"}
{"text_id": "8846", "text": "docstring: def import_cmd(ctx, filename, bbox, bboxes): path = Path(filename).resolve() conn = db.Db(**ctx.obj['cfg']['database']) tin_schema = db.Schema(ctx.obj['cfg']['tin_schema']) log = ctx.obj['log'] try: schema.create_relations(conn, tin_schema, ctx.obj['cfg']['epsg']) if path.is_file(): f = path.suffix.lower().split('.')[1] if f == 'obj': fmt = 'objmem' else: raise click.exceptions.BadParameter(f\"Unsupported format {f}\") formatter = formats.factory.create(fmt, conn=conn, schema=tin_schema) formatter.insert(path, ctx.obj['cfg']['epsg'], bbox=bbox) formatter.create_index() elif path.is_dir(): suffixes = set([Path(f).suffix.lower() for f in listdir(filename) if (path / f).is_file()]) files = [path / f for f in listdir(filename) if (path / f).is_file()] if len(files) == 0: click.exceptions.FileError(f\"Did not find any file in {path}.\") if len(suffixes) > 1: click.exceptions.FileError(f\"Found the {suffixes} filetypes \" f\"in {path}. It is not clear which\" f\" to import. Please use only \" f\"a single format in a directory.\") suffix = suffixes.pop() f = suffix.lower().split('.')[1] if f == 'obj': fmt = 'objdb' else: raise click.exceptions.BadParameter(f\"Unsupported format {f}\") formatter = formats.factory.create(fmt, conn=conn, schema=tin_schema) geojson = json.load(bboxes) if bboxes else None total = len(files) for i,file_path in enumerate(files): tile = file_path.stem if geojson: polygon = [utils.get_polygon(f) for f in geojson['features'] if f['properties']['tile'] == tile] if len(polygon) == 0: log.warning(f\"Couldn't match any GeoJSON feature to \" f\"tile {tile}\") bbox = None elif len(polygon) > 1: log.warning(f\"More than 1 matching feature to tile\" f\" {tile}. Only using the first.\") bbox = utils.bbox(polygon[0]) else: bbox = utils.bbox(polygon[0]) formatter.insert(file_path, ctx.obj['cfg']['epsg'], bbox=bbox) log.info(f\"[{i+1}/{total}] Imported {tile}\") formatter.create_index() else: raise FileNotFoundError(path) except Exception as e: raise finally: conn.close()"}
{"text_id": "8847", "text": "docstring: def unpublish(self, language): if not self.publisher_is_draft: raise PublicIsUnmodifiable('The public instance cannot be unpublished. Use draft.') title = self.title_set.get(language=language) public_title = title.publisher_public title.published = False title.publisher_state = constants.PUBLISHER_STATE_DIRTY title.save() if hasattr(self, 'title_cache'): self.title_cache[language] = title public_title.published = False public_title.save() public_article = self.publisher_public public_placeholders = public_article.get_placeholders() for pl in public_placeholders: pl.cmsplugin_set.filter(language=language).delete() public_article.save() self.save() from cms.signals import post_unpublish post_unpublish.send(sender=Article, instance=self, language=language) return True"}
{"text_id": "8848", "text": "docstring: def gather_pixels(self): print(\"Querying MAST to obtain a list of target pixel files...\") from .mast import get_tpf_urls urls = get_tpf_urls(self.campaign, channel=self.channel) print(\"Found {} target pixel files.\".format(len(urls))) with click.progressbar(urls, label=\"Reading target pixel files\", show_pos=True) as bar: for url in bar: if self.data_store is not None: path = url.replace(\"http://archive.stsci.edu/missions/k2/target_pixel_files\", self.data_store) else: path = url self.add_tpf(path)"}
{"text_id": "8849", "text": "docstring: def make_rebuilding_default_sim_env( interface_generating_function: Optional[Callable[[], GymTrainedInterface]] ) -> RebuildingEnv: interface = interface_generating_function() return RebuildingEnv.from_custom_sim_env( make_default_sim_env(interface), interface_generating_function=interface_generating_function, )"}
{"text_id": "8850", "text": "docstring: def Save(value, filename_prefix, **kwargs): last = value steps = GetGlobalStep() for k in sorted(kwargs): with tf.control_dependencies([last]): last = tf.py_func(_Save, [steps, filename_prefix, k, kwargs[k]], []) with tf.control_dependencies([last]): return tf.identity(value)"}
{"text_id": "8851", "text": "docstring: def _handle_notifications(self, model): if model.detected_appliance is None: return body = { \"_id\": model.detected_appliance.appliance_id, \"name\": model.detected_appliance.name, \"type\": model.detected_appliance.category, \"active\": model.detected_appliance.signature[0, 0], \"reactive\": model.detected_appliance.signature[0, 1], \"status\": \"true\" } inst_id = model.installation_id logging.info(\"Sending notification data:\") self._orch_token = utils.get_jwt('nilm', self._orch_jwt_psk) if not self._orch_debug_mode: url = self._notifications_url + inst_id + '/' + \\ self._notifications_suffix try: resp = requests.post(url, data=json.dumps(body), headers={'Authorization': 'jwt %s' % (self._orch_token)}) except Exception as e: logging.warning(\"Sending of notification failed!\") logging.warning(\"Exception type: %s\" % (str(type(e)))) logging.warning(e) if not resp.ok: logging.error( \"Sending of notification data for %s failed: (%d, %s)\" % (model.installation_id, resp.status_code, resp.text) ) logging.error(\"Request body:\") logging.error(\"%s\" % (json.dumps(body))) else: logging.info(\"Appliance detection notification sent: %s\" % (json.dumps(body))) else: logging.info(\"Notification: %s\", json.dumps(body))"}
{"text_id": "8852", "text": "docstring: def LTBitslice(X): X[0] = rotateLeft(X[0], 13) X[2] = rotateLeft(X[2], 3) X[1] = xor(X[1], X[0], X[2]) X[3] = xor(X[3], X[2], shiftLeft(X[0], 3)) X[1] = rotateLeft(X[1], 1) X[3] = rotateLeft(X[3], 7) X[0] = xor(X[0], X[1], X[3]) X[2] = xor(X[2], X[3], shiftLeft(X[1], 7)) X[0] = rotateLeft(X[0], 5) X[2] = rotateLeft(X[2], 22) return X"}
{"text_id": "8853", "text": "docstring: def forward(self, X, training=False, *args, **kwargs): return X"}
{"text_id": "8854", "text": "docstring: def update_health_monitor(self, context, old_health_monitor, health_monitor, pool, service): try: self.lbdriver.update_health_monitor(old_health_monitor, health_monitor, pool, service) self.cache.put(service, self.agent_host) except NeutronException as exc: LOG.error(\"update_health_monitor: NeutronException: %s\" % exc.msg) except Exception as exc: LOG.error(\"update_health_monitor: Exception: %s\" % exc.message)"}
{"text_id": "8855", "text": "docstring: def save_as_images(self, filename_without_ext, records, w=3, h=3): for (page_no, image) in enumerate(self.render_all(records, w, h), start=1): filename = '{0}{1:04d}.png'.format(filename_without_ext, page_no) self.dryrun_wrapper( 'save {}'.format(filename), lambda: image.save(filename, 'PNG')) self.console('- save all png images succeeded') return True"}
{"text_id": "8856", "text": "docstring: def generate_traps_db(mib_sources, output_dir, output_file, output_format, no_descr, debug, mib_files): from pysmi.codegen import JsonCodeGen from pysmi.compiler import MibCompiler from pysmi.parser import SmiV1CompatParser from pysmi.reader import getReadersFromUrls from pysmi.searcher import AnyFileSearcher from pysmi.writer import FileWriter if debug: set_debug() from pysmi import debug debug.setLogger(debug.Debug('all')) mib_sources = [mib_sources] if mib_sources else [MIB_SOURCE_URL] if output_file: allowed_extensions = ALLOWED_EXTENSIONS_BY_FORMAT[output_format] if not any(output_file.endswith(x) for x in allowed_extensions): abort( \"Output file {} does not end with an allowed extension '{}'\".format( output_file, \", \".join(allowed_extensions) ) ) if output_dir and output_file: abort(\"Do not set both --output-dir and --output-file at the same time.\") elif not output_file and not output_dir: abort(\"Need to set one of --output-dir or --output-file\") with TempDir('ddev_mibs') as compiled_mibs_sources: compiled_mibs_sources = os.path.abspath(compiled_mibs_sources) echo_info(\"Writing intermediate compiled MIBs to {}\".format(compiled_mibs_sources)) mibs_sources_dir = os.path.join(compiled_mibs_sources, 'mibs_sources') if not os.path.isdir(mibs_sources_dir): os.mkdir(mibs_sources_dir) mib_sources = ( sorted(set([os.path.abspath(os.path.dirname(x)) for x in mib_files if os.path.sep in x])) + mib_sources ) mib_files = [os.path.basename(x) for x in mib_files] searchers = [AnyFileSearcher(compiled_mibs_sources).setOptions(exts=['.json'])] code_generator = JsonCodeGen() file_writer = FileWriter(compiled_mibs_sources).setOptions(suffix='.json') mib_compiler = MibCompiler(SmiV1CompatParser(tempdir=''), code_generator, file_writer) mib_compiler.addSources(*getReadersFromUrls(*mib_sources, **dict(fuzzyMatching=True))) mib_compiler.addSearchers(*searchers) compiled_mibs, compiled_dependencies_mibs = compile_and_report_status(mib_files, mib_compiler) for mib_file_name in compiled_dependencies_mibs: os.replace( os.path.join(compiled_mibs_sources, mib_file_name + '.json'), os.path.join(mibs_sources_dir, mib_file_name + '.json'), ) compiled_mibs = [os.path.join(compiled_mibs_sources, x + '.json') for x in compiled_mibs] trap_db_per_mib = generate_trap_db(compiled_mibs, mibs_sources_dir, no_descr) use_json = output_format == \"json\" if output_file: write_compact_trap_db(trap_db_per_mib, output_file, use_json=use_json) echo_success(\"Wrote trap data to {}\".format(os.path.abspath(output_file))) else: write_trap_db_per_mib(trap_db_per_mib, output_dir, use_json=use_json) echo_success(\"Wrote trap data to {}\".format(os.path.abspath(output_dir)))"}
{"text_id": "8857", "text": "docstring: def _ensure_clocks_present(self): frequencies = { 'sync': self.SYNC_CLOCK_FREQUENCY, 'usb': self.USB_CLOCK_FREQUENCY, 'fast': self.FAST_CLOCK_FREQUENCY, 'ss': self.SS_CLOCK_FREQUENCY } self.assertIsNotNone(frequencies[self.domain], f\"no frequency provied for `{self.domain}`-domain clock!\")"}
{"text_id": "8858", "text": "docstring: def update_rose_sxs(rose_dict, units): subplot_spec = dict(type=\"polar\", t=0.02) subplot_args = { \"rows\": 1, \"cols\": 2, \"horizontal_spacing\": 0.01, \"specs\": [[subplot_spec, subplot_spec]], \"subplot_titles\": [\"\", \"\"], } polar_props = dict( bgcolor=\"#fff\", angularaxis=dict( tickmode=\"array\", tickvals=[0, 45, 90, 135, 180, 225, 270, 315], ticktext=[\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"], tickfont=dict(color=\"#444\", size=14), showticksuffix=\"last\", showline=False, color=\"#888\", gridcolor=\"#efefef\", rotation=90, direction=\"clockwise\", ), radialaxis=dict( color=\"#888\", gridcolor=\"#efefef\", tickangle=0, range=[0, 5], tick0=1, showticklabels=False, ticksuffix=\"%\", showticksuffix=\"last\", showline=False, tickfont=dict(color=\"#444\"), ), ) station_name = luts.map_data.loc[rose_dict[\"sid\"]][\"real_name\"] rose_layout = { \"title\": dict( text=\"Historical wind comparison, \" + station_name, font=dict(family=\"Open Sans\", size=18), x=0.5, ), \"margin\": dict(l=0, t=100, r=0, b=20), \"font\": dict(family=\"Open Sans\", size=14), \"legend\": {\"orientation\": \"h\", \"x\": -0.05, \"y\": 1}, \"height\": 650, \"paper_bgcolor\": luts.background_color, \"plot_bgcolor\": luts.background_color, \"polar1\": {**polar_props, **{\"hole\": 0.1}}, \"polar2\": {**polar_props, **{\"hole\": 0.1}}, } if \"trace_dict\" in rose_dict: empty_trace = go.Barpolar(rose_dict[\"trace_dict\"]) return make_empty_sxs_rose(empty_trace, subplot_args, rose_layout, rose_dict) subplot_args[\"subplot_titles\"] = rose_dict[\"target_decades\"] fig = make_subplots(**subplot_args) data_list = [pd.DataFrame(df_dict) for df_dict in rose_dict[\"data_list\"]] max_axes = pd.DataFrame() for df, show_legend, i in zip(data_list, [True, False], [1, 2]): traces = [] max_axes = max_axes.append( get_rose_traces(df, traces, units, show_legend), ignore_index=True ) _ = [fig.add_trace(trace, row=1, col=i) for trace in traces] rmax = max_axes[\"frequency\"].max() + 1 polar_props[\"radialaxis\"][\"range\"][1] = rmax polar_props[\"radialaxis\"][\"dtick\"] = math.floor(rmax / 2.5) polar_props[\"radialaxis\"][\"showticklabels\"] = True for i in fig[\"layout\"][\"annotations\"]: i[\"y\"] = i[\"y\"] + 0.05 i[\"font\"] = dict(size=14, color=\"#444\") i[\"text\"] = \"<b>\" + i[\"text\"] + \"</b>\" station_calms = pd.DataFrame(rose_dict[\"calms_dict\"]) rose_layout[\"polar1\"][\"hole\"] = station_calms.iloc[0][\"percent\"] rose_layout[\"polar2\"][\"hole\"] = station_calms.iloc[1][\"percent\"] fig[\"layout\"][\"annotations\"] = fig[\"layout\"][ \"annotations\" ] + get_rose_calm_sxs_annotations(fig[\"layout\"][\"annotations\"], station_calms) fig.update_layout(**rose_layout) return fig"}
{"text_id": "8859", "text": "docstring: def service_account_name(self): return self._service_account_name"}
{"text_id": "8860", "text": "docstring: def update(self): data = ecobee.NETWORK data.update() for sensor in data.ecobee.get_remote_sensors(self.index): for item in sensor['capability']: if ( item['type'] == self.type and self.type == 'temperature' and self.sensor_name == sensor['name']): self._state = float(item['value']) / 10 elif ( item['type'] == self.type and self.type == 'humidity' and self.sensor_name == sensor['name']): self._state = item['value'] elif ( item['type'] == self.type and self.type == 'occupancy' and self.sensor_name == sensor['name']): self._state = item['value']"}
{"text_id": "8861", "text": "docstring: def generateReport(net,lstVars): print(\"************* net.shape: \", net.shape) nFeatures = net.shape[3] print(\"************* membership of oil field ********************\") cntGps = np.zeros([nXs,nYs,nZs],dtype=np.int) print(\"lstOilFields: \\n\", lstOilFields) vec=[] for iRow in range(nRows): train = data[:, iRow].reshape(np.array([nFeatures, 1, 1])) absbm, absbm_id = find_absbm3(train, net) print(iRow, np.array(absbm_id)+1, lstOilFields[iRow]) vec.append([iRow,absbm_id, absbm.T]) iiX = absbm_id[0] iiY = absbm_id[1] iiZ = absbm_id[2] cntGps[iiX,iiY,iiZ]+=1 for x in range(net.shape[0]): for y in range(net.shape[1]): for z in range(net.shape[2]): train = net[x, y, z, :].reshape(np.array([nFeatures, 1, 1])) iRow+=1 absbm_id = str([x,y,z]).replace(',','') absbm = train vec.append([iRow,absbm_id, absbm.T]) print(\"************* Features of oil fields ********************\") print(\"Features: \\n\", lstVars) unique, counts = np.unique(([str(vec[i][1]) for i in range(len(vec))]), return_counts=True) cntList = list(zip(unique,counts)) cnt = [cntList[i][1] -1 for i in range(len(cntList))] return cntGps"}
{"text_id": "8862", "text": "docstring: def mde_bcra3f2( rho2: float, n: float, J: float, K: int, power: float = 0.80, alpha: float = 0.10, two_tailed: bool = True, p: float = 0.50, g2: int = 0, r21: int = 0, r22: int = 0, print_pretty: bool = True, ) -> Dict: df = ceil(K * (J - 2) - g2) sse = sqrt( rho2 * (1 - r22) / (p * (1 - p) * J * K) + (1 - rho2) * (1 - r21) / (p * (1 - p) * J * K * n) ) mde = _mde(power, alpha, sse, df, two_tailed) if print_pretty: confidence_intervals = [round(i, 3) for i in mde[f'{int((1 - round(alpha, 2)) * 100)}% Confidence Interval']] str_print = (\"Minimum Detectable Effect Size\" + \"\\n\" + \"-\" * 39 + \"\\n\" + f\" {round(mde['minimum_detectable_effect'], 3)} {int((1 - round(alpha, 2)) * 100)}% CI {confidence_intervals}\" + \"\\n\" + \"-\" * 39 + \"\\n\" + f\"Degrees of Freedom: {df}\" + \"\\n\" + f\"Standardized Standard Error: {round(sse, 3)}\" + \"\\n\" + f\"Type I Error Rate: {round(alpha, 2)}\" + \"\\n\" + f\"Type II Error Rate: {round(1 - power, 2)}\" + \"\\n\" + f\"Two-Tailed Test: {two_tailed}\" ) print(str_print) return mde"}
{"text_id": "8863", "text": "docstring: def remove_component_from_entity(self, component_type, entity): self._components[component_type].discard(entity) if not self._components[component_type]: del self._components[component_type] del self._entities[entity][component_type] if not self._entities[entity]: del self._entities[entity]"}
{"text_id": "8864", "text": "docstring: def draw_lattice(cyclic_flats): lattice=nx.DiGraph() flat_pool=set([]) for rank in sorted(cyclic_flats.keys()): for flat2 in cyclic_flats[rank].copy(): lattice.add_node(frozenset(flat2)) to_be_removed=[] for flat1 in flat_pool: if flat1.issubset(flat2): lattice.add_edge(frozenset(flat1),frozenset(flat2)) to_be_removed.append(frozenset(flat1)) flat_pool.difference_update(to_be_removed) flat_pool.update({frozenset(flat) for flat in cyclic_flats[rank]}) pos={} for rank,flats in cyclic_flats.items(): for x_pos,flat in enumerate(flats): pos[frozenset(flat)]=[x_pos,rank] labels={node:i for i,node in enumerate(lattice.nodes)} fig,ax=plt.subplots(figsize=[3,3]) plt.axis('on') nx.draw_networkx(lattice,pos=pos,labels=labels,label=list(labels.keys()),ax=ax) ax.set_ylabel('rank') ax.yaxis.set_major_locator(MaxNLocator(integer=True)) ax.tick_params(left=False, bottom=False, labelleft=True, labelbottom=False) plt.box(on=False) plt.text(ax.get_xlim()[0],ax.get_ylim()[1],''.join(['{0}: {1}\\n'.format(i,', '.join(node)) for node,i in labels.items()])) plt.show() return fig,ax"}
{"text_id": "8865", "text": "docstring: def _num_tokens_of(rule): total = len(rule.get(\"tokens\")) for _ in (\"prev_classes\", \"prev_tokens\", \"next_tokens\", \"next_classes\"): val = rule.get(_) if val: total += len(val) return total"}
{"text_id": "8866", "text": "docstring: def parseMemberTags(self, page): self.itemList = list() linkList = page.findAll('a') for link in linkList: if link.has_key('href'): result = self.__re_illust.findall(link['href']) if len(result) > 0: image_id = int(result[0]) self.itemList.append(PixivTagsItem(int(image_id), 0, 0)) self.checkLastPage(page, fromMember=True) self.availableImages = SharedParser.parseCountBadge(page) return self.itemList"}
{"text_id": "8867", "text": "docstring: def createTransformation(self, domain): return GeodeticCoordinateTransformation(domain, reference=self)"}
{"text_id": "8868", "text": "docstring: def sum(self, tsr, axes, out): if isinstance(axes, (tuple, list)): logger.warn(\"GPUTensor only supports single axis for sum. \" \"You specified: %s\", str(axes)) else: tsr._tensor.sum(axis=axes, target=out._tensor) return out"}
{"text_id": "8869", "text": "docstring: def evaluate_gecko_kwargs(parsed_args): return { 'num_classes': parsed_args.classes, 'num_shots': parsed_args.shots, 'eval_inner_batch_size': parsed_args.eval_batch, 'eval_inner_iters': parsed_args.eval_iters, 'replacement': parsed_args.replacement, 'weight_decay_rate': parsed_args.weight_decay, 'num_samples': parsed_args.eval_samples, 'transductive': parsed_args.transductive, 'save_fine_tuned_checkpoints': parsed_args.save_fine_tuned_checkpoints, 'save_fine_tuned_checkpoints_dir': parsed_args.save_fine_tuned_checkpoints_dir, 'reptile_fn': _args_gecko(parsed_args) }"}
{"text_id": "8870", "text": "docstring: def vel(track, t_vel, r): time = 0 for msg in track: if msg.type == 'note_on' or msg.type=='note_off': time += msg.time if msg.type == 'note_on': if msg.velocity != 0: r_mod = t_vel*r msg.velocity = rd.randint(max(t_vel - r_mod, 0), min(t_vel + r_mod, 127)) return track"}
{"text_id": "8871", "text": "docstring: def rename_columns(self, columns): if isinstance(columns, list): assert len(columns) == len( self.columns.values ), \"columns length do not match the dataset\" columns = dict(zip(self.columns.values, columns)) return self.rename(columns=columns)"}
{"text_id": "8872", "text": "docstring: def _log_probability(self, sample, df, loc, sigma): dims = len(loc) return ( scipy.special.gammaln((df + dims) / 2) - (df + dims) / 2 * ( 1 + (1 / df) * (sample - loc) @ np.linalg.inv(sigma) @ (sample - loc).T ) - ( scipy.special.gammaln(df / 2) + .5 * ( dims * (np.log(df) + np.log(np.pi)) + np.log(np.linalg.norm(sigma)) ) ) )"}
{"text_id": "8873", "text": "docstring: def calc_cv_error(loss_files, dataset_path, wildcards, outputs, df_filter_string='drop_voxels_with_any_negative_amplitudes,drop_voxels_near_border'): device = torch.device(\"cpu\") if df_filter_string: df_filter = sfp_model.construct_df_filter(df_filter_string) else: df_filter = None ds = sfp_model.FirstLevelDataset(dataset_path, device=device, df_filter=df_filter) dl = torchdata.DataLoader(ds, len(ds)) features, targets = next(iter(dl)) preds = torch.empty(targets.shape[:2], dtype=targets.dtype) for path in loss_files: m, l, _, _ = load_single_model(path.replace('_loss.csv', ''), False) test_subset = l.test_subset.unique() test_subset = [int(i) for i in test_subset[0].split(',')] pred = m(features[:, test_subset, :]) preds[:, test_subset] = pred torch.save({'predictions': preds, 'targets': targets}, outputs[1]) data = dict(wildcards) data.pop('model_type') data['loss_func'] = [] data['cv_loss'] = [] for loss_func in ['weighted_normed_loss', 'crosscorrelation', 'normed_loss', 'explained_variance_score', 'cosine_distance', 'cosine_distance_scaled']: cv_loss = _calc_loss(preds, targets, loss_func, True) data['loss_func'].append(loss_func) data['cv_loss'].append(cv_loss) data['dataset_df_path'] = dataset_path data['fit_model_type'] = l.fit_model_type.unique()[0] if 'true_model_type' in l.columns: data['true_model_type'] = l.true_model_type.unique()[0] cv_loss_csv = pd.DataFrame(data) cv_loss_csv.to_csv(outputs[0], index=False)"}
{"text_id": "8874", "text": "docstring: def report(self): author = self.profile return { \"Screen name\": author.screenName, \"Followers\": author.followersCount, \"Tweet URL\": self.getTweetURL(), \"Tweet ID\": self.guid, \"Tweeted at\": str(self.createdAt), \"Is reply\": \"Y\" if self.isReply() else \"N\", \"Is RT\": \"Y\" if self.isRT() else \"N\", \"Message\": self.message, \"Favs\": self.favoriteCount, \"RTs\": self.retweetCount, }"}
{"text_id": "8875", "text": "docstring: def validate_invoking_event(event): if 'invokingEvent' in event: invoking_event = json.loads(event['invokingEvent']) else: raise Exception('Error, invokingEvent not found in event, aborting.') if 'resultToken' not in event: raise Exception('Error, resultToken not found in event, aborting.') if 'configurationItem' not in invoking_event: raise Exception(\"Error, configurationItem not found in event['invokingEvent'], aborting.\") if 'resourceType' not in invoking_event['configurationItem']: raise Exception(\"Error, resourceType not found in event['invokingEvent']['configurationItem'], aborting.\") if 'configuration' not in invoking_event['configurationItem']: raise Exception(\"Error, configuration not found in event['invokingEvent']['configurationItem'], aborting.\") if 'userName' not in invoking_event['configurationItem']['configuration']: raise Exception(\"Error, userName not found in event['invokingEvent']['configurationItem']['configuration'], aborting.\") if 'resourceId' not in invoking_event['configurationItem']: raise Exception(\"Error, resourceId not found in event['invokingEvent']['configurationItem'], aborting.\") if 'configurationItemCaptureTime' not in invoking_event['configurationItem']: raise Exception(\"Error, configurationItemCaptureTime not found in event['invokingEvent']['configurationItem'], aborting.\") return invoking_event"}
{"text_id": "8876", "text": "docstring: def for_model(self, fn): return ray.get(self.workers[0].for_model.remote(fn))"}
{"text_id": "8877", "text": "docstring: def _resize(self, new_size=None): self.size = 0 if new_size is None: new_size = len(self.buckets) * 2 elif new_size is 0: new_size = len(self.buckets) / 2 temp_list = list() for bucket in self.buckets: for key, value in bucket.items(): temp_list.append((key, value)) self.buckets = [LinkedList() for i in range(new_size)] for item in temp_list: self.set(item[0],item[1])"}
{"text_id": "8878", "text": "docstring: async def async_setup_entry( hass: HomeAssistant, config_entry: ConfigEntry, async_add_entities: AddEntitiesCallback, ) -> None: control_unit: ControlUnit = hass.data[DOMAIN][config_entry.entry_id] @callback def async_add_select(args: Any) -> None: entities: list[HaHomematicGenericEntity] = [] for hm_entity in args: entities.append(HaHomematicSelect(control_unit, hm_entity)) if entities: async_add_entities(entities) config_entry.async_on_unload( async_dispatcher_connect( hass, control_unit.async_signal_new_hm_entity( config_entry.entry_id, HmPlatform.SELECT ), async_add_select, ) ) async_add_select( control_unit.async_get_new_hm_entities_by_platform(HmPlatform.SELECT) )"}
{"text_id": "8879", "text": "docstring: def _add_technologies(self): technologies = self.annotations.technologies if technologies: next_id = max([int(a.id[1:]) for a in technologies.annotations]) + 1 for term in self.ontology.technologies: searchterm = r'\\b%s\\b' % term matches = list(re.finditer(searchterm, self.annotations.text, flags=re.I)) for match in matches: json_obj = { \"id\": \"t%d\" % next_id, \"@type\": 'http://vocab.lappsgrid.org/Technology', \"start\": match.start(), \"end\": match.end() } next_id += 1 anno = Annotation(json_obj) anno.text = term technologies.add(anno)"}
{"text_id": "8880", "text": "docstring: def _process_ci_elts(self, elt, func): if isinstance(elt, mathml_ci): func(elt) else: for e in elt.xml_element_children(): self._process_ci_elts(e, func)"}
{"text_id": "8881", "text": "docstring: def valid_location(currBoard, location): if (location > 8 or location < 0): return False elif (currBoard[location] != \" \"): return False else: return True"}
{"text_id": "8882", "text": "docstring: def _udev_rule(vid, pid=None, *args): rule = \"\" if pid: rule = 'SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"%s\", ATTRS{idProduct}==\"%s\", TAG+=\"uaccess\", RUN{builtin}+=\"uaccess\"' % (vid, pid) else: rule = 'SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"%s\", TAG+=\"uaccess\", RUN{builtin}+=\"uaccess\"' % vid if args: rule = ', '.join([rule, *args]) return rule"}
{"text_id": "8883", "text": "docstring: def _executeScriptInternal(test, commands): parsedCommands = libcxx.test.format.parseScript(test, preamble=commands) litConfig = lit.LitConfig.LitConfig( progname='lit', path=[], quiet=False, useValgrind=False, valgrindLeakCheck=False, valgrindArgs=[], noExecute=False, debug=False, isWindows=platform.system() == 'Windows', params={}) _, tmpBase = libcxx.test.format._getTempPaths(test) execDir = os.path.dirname(test.getExecPath()) for d in (execDir, os.path.dirname(tmpBase)): if not os.path.exists(d): os.makedirs(d) res = lit.TestRunner.executeScriptInternal(test, litConfig, tmpBase, parsedCommands, execDir) if isinstance(res, lit.Test.Result): res = ('', res.output, 127, None) (out, err, exitCode, timeoutInfo) = res conflatedErrorOutput = re.search(\"(# command stderr:.+$)\", out, flags=re.DOTALL) if conflatedErrorOutput: conflatedErrorOutput = conflatedErrorOutput.group(0) out = out[:-len(conflatedErrorOutput)] err += conflatedErrorOutput return (out, err, exitCode, timeoutInfo)"}
{"text_id": "8884", "text": "docstring: def byteImage(self): image = Image.new(\"RGB\", (self.width, self.height)) image.putdata( [ tuple(self.pixels[i][j]) for i in range(self.height) for j in range(self.width) ] ) return self._pilImage_to_bytes(image)"}
{"text_id": "8885", "text": "docstring: def atom(x): try: return int(x) except Exception: try: return float(x) except Exception: return x"}
{"text_id": "8886", "text": "docstring: def _compute_posterior( self, fit_parameters: bool, profiler: GPMXNetSimpleProfiler): if self._debug_log is not None: self._debug_log.set_state(self.state) no_pending_state = self.state if self.state.pending_evaluations: no_pending_state = TuningJobState( hp_ranges=self.state.hp_ranges, candidate_evaluations=self.state.candidate_evaluations, failed_candidates=self.state.failed_candidates, pending_evaluations=[]) self._posterior_for_state(no_pending_state, fit_parameters, profiler) if self.state.pending_evaluations: pending_configs = [ x.candidate for x in self.state.pending_evaluations] new_pending = self._draw_fantasy_values(pending_configs) with_pending_state = TuningJobState( hp_ranges=self.state.hp_ranges, candidate_evaluations=self.state.candidate_evaluations, failed_candidates=self.state.failed_candidates, pending_evaluations=new_pending) self._posterior_for_state( with_pending_state, fit_parameters=False, profiler=None) self.fantasy_samples = new_pending"}
{"text_id": "8887", "text": "docstring: def process_request(self, request, controller): try: request.environ['CSRF_TOKEN'] = \\ request.cookies[mg_globals.app_config['csrf_cookie_name']] except KeyError: request.environ['CSRF_TOKEN'] = self._make_token(request) if (getattr(controller, 'csrf_enabled', True) and request.method not in self.SAFE_HTTP_METHODS and ('gmg.verify_csrf' in request.environ or 'paste.testing' not in request.environ) ): return self.verify_tokens(request)"}
{"text_id": "8888", "text": "docstring: def _pipepager(generator, cmd, color): import subprocess env = dict(os.environ) cmd_detail = cmd.rsplit('/', 1)[-1].split() if color is None and cmd_detail[0] == 'less': less_flags = os.environ.get('LESS', '') + ' '.join(cmd_detail[1:]) if not less_flags: env['LESS'] = '-R' color = True elif 'r' in less_flags or 'R' in less_flags: color = True c = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, env=env) encoding = get_best_encoding(c.stdin) try: for text in generator: if not color: text = strip_ansi(text) c.stdin.write(text.encode(encoding, 'replace')) except (IOError, KeyboardInterrupt): pass else: c.stdin.close() That means when the user hits ^C, the parent process (click) terminates, but less is still alive, paging the output and messing up the terminal. If the user wants to make the pager exit on ^C, they should set `LESS='-K'`. It's not our decision to make. while True: try: c.wait() except KeyboardInterrupt: pass else: break"}
{"text_id": "8889", "text": "docstring: def addRead(self, key): self.requests.append({'read': key}) return self"}
{"text_id": "8890", "text": "docstring: def _GetReportingClient(): http_client = http.Http() orig_request = http_client.request def RequestWithAPIKey(*args, **kwargs): url_parts = urlparse.urlsplit(args[0]) query_params = urlparse.parse_qs(url_parts.query) query_params['key'] = CRASH_API_KEY modified_url_parts = list(url_parts) modified_url_parts[3] = urllib.urlencode(query_params, doseq=True) modified_args = list(args) modified_args[0] = urlparse.urlunsplit(modified_url_parts) return orig_request(*modified_args, **kwargs) http_client.request = RequestWithAPIKey client_class = core_apis.GetClientClass(util.API_NAME, util.API_VERSION) return client_class(get_credentials=False, http=http_client)"}
{"text_id": "8891", "text": "docstring: def classBalancemap(mask): maskShape = mask.shape weight = np.ones(maskShape) for i in range(10): mask = ndimage.binary_dilation(mask).astype(mask.dtype) weight[mask==1] = 100 mask1 = copy.deepcopy(mask) for i in range(10): mask1 = ndimage.binary_dilation(mask1).astype(mask1.dtype) mask = mask1 - mask weight[mask==1] = 50 mask2 = copy.deepcopy(mask1) for i in range(10): mask2 = ndimage.binary_dilation(mask2).astype(mask2.dtype) mask = mask2 - mask1 weight[mask==1] = 20 return weight"}
{"text_id": "8892", "text": "docstring: def delete_distributed_device_with_http_info(self, name, **kwargs): \"\"\"Deletes a single DistributedDevice This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.delete_distributed_device_with_http_info(name, async_req=True) >>> result = thread.get() :param async_req bool :param str name: The name of a specific instance of the resource (required) :return: None If the method is called asynchronously, returns the request thread. \"\"\" all_params = ['name'] all_params.append('async_req') all_params.append('_return_http_data_only') all_params.append('_preload_content') all_params.append('_request_timeout') params = locals() for key, val in six.iteritems(params['kwargs']): if key not in all_params: raise TypeError( \"Got an unexpected keyword argument '%s'\" \" to method delete_distributed_device\" % key ) params[key] = val del params['kwargs'] if ('name' not in params or params['name'] is None): raise ValueError(\"Missing the required parameter `name` when calling `delete_distributed_device`\") collection_formats = {} path_params = {} if 'name' in params: path_params['name'] = params['name'] query_params = [] header_params = {} form_params = [] local_var_files = {} body_params = None auth_settings = ['basicAuth', 'jwtAuth'] return self.api_client.call_api( '/distributed_storage/distributed_devices/{name}', 'DELETE', path_params, query_params, header_params, body=body_params, post_params=form_params, files=local_var_files, response_type=None, auth_settings=auth_settings, async_req=params.get('async_req'), _return_http_data_only=params.get('_return_http_data_only'), _preload_content=params.get('_preload_content', True), _request_timeout=params.get('_request_timeout'), collection_formats=collection_formats)"}
{"text_id": "8893", "text": "docstring: def validate(self): if not isinstance(self.exp_id, basestring): raise utils.ValidationError( 'Expected exp_id to be a string, received %s' % type( self.exp_id)) if not isinstance(self.exp_version, int): raise utils.ValidationError( 'Expected exp_version to be an int, received %s' % type( self.exp_version)) if not isinstance(self.unresolved_issues, list): raise utils.ValidationError( 'Expected unresolved_issues to be a list, received %s' % ( type(self.unresolved_issues))) for issue in self.unresolved_issues: issue.validate()"}
{"text_id": "8894", "text": "docstring: def crossEntropyLoss(self, Y, A, epsilon=1e-15): m = Y.shape[1] loss = -1 * (Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon)) cost = 1 / m * np.sum(loss) return np.squeeze(cost)"}
{"text_id": "8895", "text": "docstring: def evaluate(self,labels): if len(labels)!=self.numberOfVariables : nVar=self.numberOfVariables nGiven=len(labels) raise RuntimeError('number of given labels (%d) does not match gm.numberOfVariables (%d)'%(nGiven,nVar)) if isinstance(labels, numpy.ndarray): return self._evaluate_numpy(numpy.require(labels,dtype=label_type)) elif isinstance(labels, list): return self._evaluate_list(labels) elif isinstance(labels, LabelVector): return self._evaluate_vector(labels) else: raise RuntimeError( \"%s is not an supperted type for arument ``labels`` in ``evaluate``\" %(str(type(labels)) ,) )"}
{"text_id": "8896", "text": "docstring: def needs_riak(method): method.needs_riak = True return method"}
{"text_id": "8897", "text": "docstring: def _ordkey(self, key): def norm(key, alpha): if key is None: return None elif isinstance(key, str) or isinstance(key, Alphabet): key = str(key) if len(key) == 1: return alpha.ord(key) if len(key) == 0: return None return np.asarray(alpha.ords(key)) elif isinstance(key, slice): start = norm(key.start, alpha) stop = norm(key.stop, alpha) step = key.step return slice(start, stop, step) else: return key if isinstance(key, tuple): return tuple([norm(k, a) for k, a in zip(key, self.alphabets)]) else: return norm(key, self.alphabets[0])"}
{"text_id": "8898", "text": "docstring: def compute(self): self.clear_data() if len(self.data_frame) == 0: self.text.insert(\"1.0\", 'WARNING: data needs to be loaded first!') return cols = self.data_frame.columns.values self.data_frame['Original'] = np.arange(1, len(self.data_frame)+1) temp = self.data_frame[[cols[0], cols[1]]] self.data_frame['AveAB'] = temp.mean(axis=1) self.data_frame['Log2(B/A)'] = np.log2(self.data_frame[cols[1]]/self.data_frame[cols[0]]) self.data_frame['FC'] = self.add_FC(self.data_frame[cols[0]], self.data_frame[cols[1]]) self.data_frame.sort_values(by='AveAB', ascending=False, inplace=True) self.data_frame.reset_index(drop=True, inplace=True) zs = [] for i, row in enumerate(self.data_frame['Log2(B/A)']): zs.append(self.sliding_zscore(self.data_frame['Log2(B/A)'], i, self.window.get())) self.data_frame['Z-Score'] = zs self.data_frame.sort_values(by='Original', inplace=True) self.data_frame.reset_index(drop=True, inplace=True) self.data_frame.drop('Original', axis=1, inplace=True) self.fit_Gaussian() self.p_values() self.BH_correction() self.data_frame['candidate'] = self.data_frame['FDR'].map(self.set_candidates) self.root.clipboard_clear() self.print_frame() self.status.set(\"%s\", \"computed %s z-scores\" % len(self.data_frame))"}
{"text_id": "8899", "text": "docstring: def add_latent_cg(self, n_latent: int = 1, seed: int = -1, connectionprob=.95, strength=.5, disscale=.3, ctsscale=.2, marginalize=True): if seed != -1: print('Model_PW.add_latent_CG: Set seed to %d' % (seed)) np.random.seed(seed) lbda_ho = np.zeros((n_latent, self.meta['n_cg'])) alpha_h = np.zeros(n_latent) mat_r_h = np.zeros((n_latent, self.meta['ltot'])) def get_entry(dim, prob=.8, offset=.7, scale=.3, samesign=True): if np.random.random() > prob: return np.zeros(dim) if samesign: sign = -1 if np.random.random() > .5: sign = 1 group = np.empty(dim) for i in range(dim): if not samesign: sign = -1 if np.random.random() > 0.5: sign = 1 group[i] = sign * (offset + scale * np.random.random()) return group glims = self.meta['cat_glims'] for i in range(n_latent): for r in range(self.meta['n_cat']): mat_r_h[i, glims[r]+1:glims[r+1]] = \\ get_entry(self.meta['sizes'][r]-1, prob=connectionprob, offset=strength, scale=disscale) for s in range(self.meta['n_cg']): lbda_ho[i, s] = get_entry(1, prob=connectionprob, offset=strength + .3, scale=ctsscale) scale = 1.0 if self.meta[ 'n_cg'] > 0: tmp = np.dot(lbda_ho.T, lbda_ho) while True: lamin = np.min(np.linalg.eigvals(self.mat_lbda - scale * tmp)) if lamin < .2: scale *= 2 / 3 else: break if scale != 1.0: print(\"\"\"Warning(model_pw.add_latent_gaussians): made precision matrix of full model PD, c=%f\"\"\" % (scale)) fac = np.sqrt(scale) mat_b = np.empty( (self.meta['n_cg'] + n_latent, self.meta['n_cg'] + n_latent)) mat_b[:-n_latent, :-n_latent] = self.mat_lbda mat_b[:-n_latent, -n_latent:] = fac * lbda_ho.T mat_b[-n_latent:, :-n_latent] = fac * lbda_ho mat_b[-n_latent:, -n_latent:] = np.eye(n_latent) if self.meta['n_cg'] > 0: if self.meta['n_cat'] > 0: self.mat_r = np.concatenate((self.mat_r, mat_r_h), axis=0) self.alpha = np.concatenate((self.alpha, alpha_h)) else: self.alpha = alpha_h self.mat_r = mat_r_h self.meta['n_cg'] += n_latent self.mat_lbda = mat_b if marginalize: drop_idx = [self.meta['n_cg'] - i - 1 for i in range(n_latent)] return self.marginalize(drop_idx=drop_idx, verb=0)"}
{"text_id": "8900", "text": "docstring: def _normalize(x): return (x - x.min())/x.ptp()"}
{"text_id": "8901", "text": "docstring: def legendre(x, y): z = pow(x, (y-1)//2, y) if z > 1: z = -1 return z"}
{"text_id": "8902", "text": "docstring: def init_app(self, app): app.config.setdefault('BASIC_AUTH_FORCE', False) app.config.setdefault('BASIC_AUTH_REALM', '') app.config.setdefault('BASIC_AUTH_EXCLUDE', []) @app.before_request def require_basic_auth(): if not current_app.config['BASIC_AUTH_FORCE']: return else: print(request.path) if request.method == \"OPTIONS\" or 'favicon.ico' in request.path: return Response(\"{}\", status=204, mimetype='application/json') req = request method, endpoint = request.method, request.url_rule.endpoint print(\"{0}:{1}\".format(endpoint, method)) exclude = current_app.config['BASIC_AUTH_EXCLUDE'] if not \"{0}:{1}\".format(endpoint, method) in exclude and not self.authenticate(): return self.challenge()"}
{"text_id": "8903", "text": "docstring: def apparent_encoding(self): if self._response.apparent_encoding: return self._response.apparent_encoding.upper() return None"}
{"text_id": "8904", "text": "docstring: def translation_matrix(t): R = np.eye(4) R[:3,3] = np.array([t[0],t[1],t[2]]) return R"}
{"text_id": "8905", "text": "docstring: def seed_based(self) -> nb.nifti1: print(\"We're in\") roi_data = read_files.read_fmri(self.roi) data = read_files.read_fmri(self.path, get_shape=True) shape = None if isinstance(data, tuple): shape = data[1] data = data[0] seed_map = connectivity.seed_map(roi_data, data) if shape is not None: seed_map = seed_map[0,:].reshape(shape[:3]) return_val = seed_map else: return_val = None print(\"ROI done\") if self.seed_prefix == '' or self.seed_prefix == str(Path('')): print('WARNING: no prefix added, map will not be saved') else: img = nb.Nifti1Image(seed_map, affine=None) split_roi = os.path.split(self.roi) path = os.path.join(split_roi[0], self.seed_prefix + '_' + split_roi[-1]) nb.save(img, path) return return_val"}
{"text_id": "8906", "text": "docstring: def reduce(self): animationCurves = self.filter.getAnimationCurves() settings = self.settings.getSettings() rate = 0 num = len(animationCurves) self.progress.setRange(0, num) with utils.UndoChunkContext(): for i, animationCurve in enumerate(animationCurves): r = KeyframeReduction(animationCurve) rate += r.reduce(**settings) self.progress.setFormat(animationCurve) self.progress.setValue(i+1) print \"< KeyframeReductionWidget.reduce() \" \\ \"| overall-reduction-rate: {0:,.2f}% >\".format(rate/num)"}
{"text_id": "8907", "text": "docstring: def process_document(self, document, **kwargs): start = time.time() if self._log_file is not None: map_annotations_logger.addHandler(file_handler(self._log_file)) map_annotations_logger.setLevel(self._log_level) ref_annotation = document.annotation(self._annotation_name) ref_annotations = ref_annotation.annotations values = set([a.value for a in ref_annotations]) new_annotations = [Tag(self._mapping.get(annotation.value, annotation.value), annotation.lb, annotation.ub) for annotation in ref_annotations if self._mapping.get(annotation.value, None) != u\"\"] document.add_annotation(Annotation(self._annotation_name, reference=ref_annotation.reference, annotations=new_annotations)) laps = time.time() - start map_annotations_logger.info('in %s', timedelta(seconds=laps))"}
{"text_id": "8908", "text": "docstring: def CreateSnapshot(self, request, context): context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)"}
{"text_id": "8909", "text": "docstring: def render_node_masks(self, raw_data_directory: str, destination_directory: str, mask_type: MaskType): print(\"Extracting Masks from Muscima++ Dataset...\") node_classes = parse_node_classes( os.path.join(raw_data_directory, \"v2.0\", \"specifications\", \"mff-muscima-mlclasses-annot.xml\")) for index, node_class in enumerate(node_classes): self.class_to_color_mapping[node_class.name] = index + 1 file_paths = self.__get_all_file_paths(raw_data_directory) for xml_file, png_file in tqdm(file_paths, desc=\"Generating mask images\"): original_image = Image.open(png_file) nodes = read_nodes_from_file(xml_file) destination_filename = os.path.basename(xml_file).replace(\".xml\", \".png\") if mask_type == MaskType.NODES_SEMANTIC_SEGMENTATION: self.__render_masks_of_nodes_for_semantic_segmentation(nodes, destination_directory, destination_filename, original_image.width, original_image.height) if mask_type == MaskType.STAFF_LINES_INSTANCE_SEGMENTATION: self.__render_masks_of_staff_lines_for_instance_segmentation(nodes, destination_directory, destination_filename, original_image.width, original_image.height) if mask_type == MaskType.STAFF_BLOBS_INSTANCE_SEGMENTATION: self.__render_masks_of_staff_blob_for_instance_segmentation(nodes, destination_directory, destination_filename, original_image.width, original_image.height) original_image.close()"}
{"text_id": "8910", "text": "docstring: def newtons_method(func, initial_guess, iterations=100,tolerance = 10**-7,verbose = False, show_fxn=False): if isinstance(initial_guess,Number): jacobians = [] x0 = initial_guess fxn = func(initial_guess) fpxn = fxn.jacobian(initial_guess) x1 = x0 - fxn/fpxn jacobians.append(fpxn) for i in range(iterations): if abs(fxn.val) > 1e-7: x0 = x1 fxn = func(x0) fpxn = fxn.jacobian(x0) jacobians.append(fpxn) x1 = x0- fxn / fpxn if show_fxn: return x1, jacobians,fxn else: return x1, jacobians elif isinstance(initial_guess,Array): jacobians = [] if isinstance(func(initial_guess),tuple): for i in range(iterations): if i == 0: x0= initial_guess else: x0 = x1 fxn = func(x0) fpxn = [] if verbose: print(i,x0,fxn) vector = [] for n in range(len(fxn)): vector.append(fxn[n].val) if np.linalg.norm(vector)< tolerance: break for k in range(len(fxn)): fpxn_row = [] for j in range(len(x0)): fpxn_row.append(fxn[k].jacobian(x0[j])) fpxn.append(fpxn_row) jacobians.append(fpxn) x1 = x0 - np.dot(np.linalg.inv(fpxn),fxn) if show_fxn: return x1, jacobians,fxn else: return x1, jacobians else: for i in range(iterations): if i == 0: x0= initial_guess else: x0 = x1 fxn = func(x0) fpxn = [] if verbose: print(i,x0,fxn) if abs(fxn.val)< tolerance: break for j in range(len(x0)): fpxn.append(fxn.jacobian(x0[j])) jacobians.append(fpxn) print(fpxn) x1 = x0 - np.dot(np.reciprocal(fpxn),fxn) if show_fxn: return x1, jacobians,fxn else: return x1, jacobians"}
{"text_id": "8911", "text": "docstring: def read_formatted_vars(lines): formatted_var_regex = \"(?P<format_var>{.*?})\" for line in lines: for match in re.finditer(formatted_var_regex, line): yield match.group(\"format_var\")"}
{"text_id": "8912", "text": "docstring: def wavelengths(self): return self._wavelengths"}
{"text_id": "8913", "text": "docstring: def compute(self, inputs, outputs): opts = self.options vec_size = opts['vec_size'] for var in self._vars: ax = self._vars[var]['axis'] invar = self._input_names[var] vals = [inputs[invar[i]] for i in range(vec_size)] outputs[var][...] = np.stack(vals, axis=ax)"}
{"text_id": "8914", "text": "docstring: def _draw_attachments(self) -> None: self._draw_label(\"------------ ATTACHMENTS ------------\", centered=True, color=\"red\") self.row_number += 2"}
{"text_id": "8915", "text": "docstring: def main(self): three_q = [] labels = [\"first\", \"second\", \"third\"] t = Tagger().tagger() q_dict = json.loads(t) while (len(three_q) < 4): comb = self.randomize() if self.get_q(q_dict, comb): if self.get_q(q_dict, comb) not in three_q: three_q.append(self.get_q(q_dict, comb)) else: self.get_q(q_dict, self.randomize()) else: self.get_q(q_dict, self.randomize()) return three_q"}
{"text_id": "8916", "text": "docstring: def _create_examples(self, lines, set_type): examples = [] for (i, line) in enumerate(lines): if i == 0: continue guid = \"%s-%s\" % (set_type, i) print('{}th line: {}'.format(i, line)) text_a = line[3] text_b = line[4] label = line[0] examples.append( InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)) return examples"}
{"text_id": "8917", "text": "docstring: def apply_colormap(image: np.ndarray, colormap=cv.COLORMAP_VIRIDIS) -> np.ndarray: image = image - np.min(image) return cv.applyColorMap(np.uint8(255 * (image / np.max(image))), colormap)"}
{"text_id": "8918", "text": "docstring: def _ExpungeDeletableProjects(self, cnxn): request_deadline = time.time() + RUN_DURATION_LIMIT deletable_project_rows = self.services.project.project_tbl.Select( cnxn, cols=['project_id'], state='deletable', limit=100) deletable_project_ids = [row[0] for row in deletable_project_rows] expunged_project_ids = set() for project_id in deletable_project_ids: for _part in self._ExpungeParts(cnxn, project_id): expunged_project_ids.add(project_id) if time.time() > request_deadline: return list(expunged_project_ids) return list(expunged_project_ids)"}
{"text_id": "8919", "text": "docstring: def keywithmaxval(d): v = list(d.values()) k = list(d.keys()) return k[v.index(max(v))]"}
{"text_id": "8920", "text": "docstring: def memFree(self): return self.data.free_memory"}
{"text_id": "8921", "text": "docstring: def release_outputs(self): with self._check_api_type_scope(types.PipelineAPIType.SCHEDULED): if not self._built: raise RuntimeError(\"Pipeline must be built first.\") return self._pipe.ReleaseOutputs()"}
{"text_id": "8922", "text": "docstring: def finish(self): if self.writer: self.logger.debug('Finishing writer') self.writer.finish() self.plot.close() del self.plot"}
{"text_id": "8923", "text": "docstring: def para2matrix(cell_para, radians=True, format=\"upper\"): a = cell_para[0] b = cell_para[1] c = cell_para[2] alpha = cell_para[3] beta = cell_para[4] gamma = cell_para[5] if radians is not True: alpha *= rad beta *= rad gamma *= rad cos_alpha = np.cos(alpha) cos_beta = np.cos(beta) cos_gamma = np.cos(gamma) sin_gamma = np.sin(gamma) sin_alpha = np.sin(alpha) matrix = np.zeros([3, 3]) if format == \"lower\": c1 = c * cos_beta c2 = (c * (cos_alpha - (cos_beta * cos_gamma))) / sin_gamma matrix[0][0] = a matrix[1][0] = b * cos_gamma matrix[1][1] = b * sin_gamma matrix[2][0] = c1 matrix[2][1] = c2 matrix[2][2] = np.sqrt(c ** 2 - c1 ** 2 - c2 ** 2) elif format == \"symmetric\": pass elif format == \"upper\": a3 = a * cos_beta a2 = (a * (cos_gamma - (cos_beta * cos_alpha))) / sin_alpha matrix[2][2] = c matrix[1][2] = b * cos_alpha matrix[1][1] = b * sin_alpha matrix[0][2] = a3 matrix[0][1] = a2 matrix[0][0] = np.sqrt(a ** 2 - a3 ** 2 - a2 ** 2) return matrix"}
{"text_id": "8924", "text": "docstring: def _is_not_empty(attribute: Any, value: Any) -> bool: if value == dict(): return False elif value == list(): return False else: return True"}
{"text_id": "8925", "text": "docstring: def update_mu_batch_h(self, batch_ind, update_func, grad_func): if 0 not in self.fixed_factors: grad_func['grad_h'](batch_ind) update_func['train_h'](batch_ind) grad_func['grad_w'](batch_ind)"}
{"text_id": "8926", "text": "docstring: def split_list(source_list, splitter_algo: str, splitter_args, randomizer): shard = [] if splitter_algo == 'randomFraction': count = round(len(source_list) * float(splitter_args['fraction'])) val_indexes = randomizer.sample(range(0, len(source_list)), count) val_indexes = sorted(val_indexes, reverse=True) for i in val_indexes: shard.append(source_list[i]) del (source_list[i]) else: logging.error(f\"Unknown validation algorithm '{splitter_algo}', not splitting list.\") return shard"}
{"text_id": "8927", "text": "docstring: def UDIV(dividend, divisor): if isinstance(dividend, BitVec): return dividend.udiv(divisor) elif isinstance(divisor, BitVec): return divisor.rudiv(dividend) assert dividend >= 0 or divisor > 0 return dividend // divisor"}
{"text_id": "8928", "text": "docstring: def condition_equals( cls, lhs: typing.Any, rhs: typing.Any, ) -> \"ICfnConditionExpression\": return jsii.sinvoke(cls, \"conditionEquals\", [lhs, rhs])"}
{"text_id": "8929", "text": "docstring: def addPrograms(self, programs=[], actors=[], cmd=None): if not cmd: cmd = self.defaultCmd if not programs: programs = [] for name, cmdr in g.commanders.items(): if cmdr.needsAuth and name not in self.programs: programs.append(name) if self.debug > 3: Misc.log('auth.addPrograms', 'adding programs %s with actors=%s' % (programs, actors)) for prog in programs: if prog in self.programs: cmd.warn( 'permsTxt=%s' % (Misc.qstr( 'Program %s already has an authorization entry, ' 'which will not be modified.' % (prog)))) continue prog = prog.upper() self.programs[prog] = {} self.setActorsForProgram(prog, actors, cmd=cmd) self.genProgramsKey(cmd=cmd)"}
{"text_id": "8930", "text": "docstring: def restore_backend(): pandas.set_option(\"plotting.backend\", \"matplotlib\") yield pandas.set_option(\"plotting.backend\", \"matplotlib\")"}
{"text_id": "8931", "text": "docstring: def submit_transaction(self, signed_tx_file, cleanup=False): result = self.run_cli( f\"{self.cli} transaction submit \" f\"--tx-file {signed_tx_file} {self.network}\" ) if result.stderr: raise ShelleyError(f\"Unable to submit transaction: {result.stderr}\") if cleanup: self._cleanup_file(signed_tx_file)"}
{"text_id": "8932", "text": "docstring: def _parse_drcov_header(self, f): version_line = f.readline().strip() self.version = int(version_line.split(\":\")[1]) flavor_line = f.readline().strip() self.flavor = flavor_line.split(\":\")[1] assert self.version == 2, \"Only drcov version 2 log files supported\""}
{"text_id": "8933", "text": "docstring: def main(): parser = argparse.ArgumentParser() parser.add_argument('-a', '--autolaunch_browser', \\ help='Auto launches a browser', \\ action='store_true') parser.add_argument('-l', '--payload',\\ help='payload to POST') parser.add_argument('-m', '--method', \\ metavar='get, post, etc.', \\ help='Define HTTP request method ex: -m post', \\ choices=['get', 'patch', 'post', 'put', 'delete', 'head', 'trace'], \\ nargs=1, \\ required=True) parser.add_argument('-p', '--port', \\ metavar='8080',\\ help='Port to start local HTTP server on. Default is 8080', \\ type=int) parser.add_argument('-r', '--rqhdr', \\ type=str, \\ nargs=1, \\ help='Specify optional header(s) for authentication, JWT, i.e. Authorization: Bearer eyJhbGciOiJSUz... yadda yadda') parser.add_argument('-u', '--url', \\ metavar='http://foo.com', \\ required=True, help='Define vulnerable CORS targert URL ex: https://site.com/') parser.add_argument('-v', '--verbose', \\ help='Enable verbosity', \\ action='store_true') args = parser.parse_args() runcores = Cores(args.autolaunch_browser, args.rqhdr, args.method, args.port, args.url, args.payload, args.verbose) runcores.dir_check() runcores.get_internal_address() runcores.javascript_template() runcores.html_template() if args.autolaunch_browser is True: runcores.browser_launch() runcores.server_start()"}
{"text_id": "8934", "text": "docstring: async def futures_account_trades(self, **params): return await self.client_helper(\"futures_account_trades\", **params)"}
{"text_id": "8935", "text": "docstring: def translateButton(self, event): if event.buttons() & QtCore.Qt.LeftButton: state = self.findCursorState(event.modifiers()) state2Button = {0: QtCore.Qt.LeftButton, 1: QtCore.Qt.MidButton, 2: QtCore.Qt.RightButton} if state2Button.has_key(state): return state2Button[state] return event.buttons()"}
{"text_id": "8936", "text": "docstring: def factory(self, **args): return crud_request(r=self, **args)"}
{"text_id": "8937", "text": "docstring: def uppaal_system_declaration(evaluator, ast, state): inst_data = {} for decl_ast in ast[\"decls\"]: res = evaluator.eval_ast(decl_ast, state) if decl_ast[\"astType\"] == \"Instantiation\": inst_data[res[\"instance_name\"]] = res system_instances = evaluator.eval_ast(ast[\"systemDecl\"], state) return {\"system_instances\": system_instances, \"instance_data\": inst_data}"}
{"text_id": "8938", "text": "docstring: def show(name): ret = {} cmd = 'systemctl show {0}.service'.format(name) for line in __salt__['cmd.run'](cmd).splitlines(): comps = line.split('=') name = comps[0] value = '='.join(comps[1:]) if value.startswith('{'): value = value.replace('{', '').replace('}', '') ret[name] = {} for item in value.split(' ; '): comps = item.split('=') ret[name][comps[0].strip()] = comps[1].strip() elif name in ('Before', 'After', 'Wants'): ret[name] = value.split() else: ret[name] = value return ret"}
{"text_id": "8939", "text": "docstring: def update_looking_for(profile_tree, looking_for): div = profile_tree.xpath(\"//div[@id = 'what_i_want']\")[0] looking_for['gentation'] = div.xpath(\".//li[@id = 'ajax_gentation']/text()\")[0].strip() looking_for['ages'] = replace_chars(div.xpath(\".//li[@id = 'ajax_ages']/text()\")[0].strip()) looking_for['near'] = div.xpath(\".//li[@id = 'ajax_near']/text()\")[0].strip() looking_for['single'] = div.xpath(\".//li[@id = 'ajax_single']/text()\")[0].strip() try: looking_for['seeking'] = div.xpath(\".//li[@id = 'ajax_lookingfor']/text()\")[0].strip() except: pass"}
{"text_id": "8940", "text": "docstring: def send_mail(subject: str, body: str, from_email: str, to_emails: list): django_send_mail( subject, body, from_email, to_emails, fail_silently=False, )"}
{"text_id": "8941", "text": "docstring: def last(self) -> datetime.date: return self.__dates__[-1]"}
{"text_id": "8942", "text": "docstring: def canvas(name=\"canvas1\", size=(800, 600)): import ROOT canvas = ROOT.gROOT.FindObject(name) if canvas: return canvas else: width, height = size return ROOT.TCanvas(name, name, width, height)"}
{"text_id": "8943", "text": "docstring: def write_categ_lmdb(workfile, categories, LMDB_path, gray): categ_db = [] for idx, categ in enumerate(categories): categ_db.append( lmdb.open(os.path.join(LMDB_path, categ), map_size=int(1e12), map_async=True, writemap=True, meminit=False)) curr_idx = np.zeros(len(categories), dtype='int') with open(workfile, 'r') as f: for line in f.readlines(): avi_file, label_file = line.strip().split(' ') file_frames = avi_to_frame_list(avi_file, gray) file_labels = label_file_to_labels(label_file) assert len(file_frames) == len(file_labels), \\ 'Frames and Labels do not match in length!' db_label = avi_file.split(\"/\")[-1].split(\"_\")[0] index = categories.index(db_label) curr_idx[index] = write_data_to_lmdb(categ_db[index], file_frames, file_labels, curr_idx[index]) return curr_idx"}
{"text_id": "8944", "text": "docstring: def sampleMarkov(state, cpt): return np.random.choice(cpt.shape[0],p=cpt[state])"}
{"text_id": "8945", "text": "docstring: def _check_y_pos(im, y_pos, radius): if y_pos - radius < 0: im = np.concatenate( [np.zeros((np.abs(y_pos - radius), im.shape[1])), im], 0 ) return im"}
{"text_id": "8946", "text": "docstring: def handleerror(self, request, client_address): print(\"Client {} disconnected\".format(client_address)) super(request, client_address)"}
{"text_id": "8947", "text": "docstring: def _config_variables(config_file, section_name): _config = ConfigParser.RawConfigParser() with open(config_file) as handle: _config.readfp(handle) return dict(_config.items(section_name))"}
{"text_id": "8948", "text": "docstring: def check_string(cls, string): return ( string.startswith(\"community::\") and string.count(\"::\") == 2 )"}
{"text_id": "8949", "text": "docstring: def forward(self, g: Data, nstep: Optional[int] = None, output_dynamics: bool = False) -> Union[Tensor, Tuple]: model_device = g.x1.device size1 = g.x1.shape[0] size2 = g.x2.shape[0] factor_types = g.x1[:, 0].long() masks = [(factor_types==self.ftypes[i]) for i in range(self.num_factor_types)] assert ( sum([sum(m.long()).item() for m in masks])==g.x1.shape[0] ), \"size incorrect {}:{}\".format(sum([len(m) for m in masks]), g.x1.shape[0]) if nstep is None: nstep = self.nstep h1 = torch.zeros(g.x1.shape[0], self.nstate, device=model_device) h2 = torch.zeros(g.x2.shape[0], self.nstate, device=model_device) f1 = self._get_f1(g) if self.init_method==\"encode\": for i in range(self.num_factor_types): h1[masks[i]] = self.encoder1_list[i](g.x1[masks[i]]) h2 = self.encoder2(g.x2) elif self.init_method==\"randn\": torch.nn.init.normal_(h1) torch.nn.init.normal_(h2) elif self.init_method==\"zero\": pass else: raise NotImplementedError(\"init method not implemented\") if self.use_factor_net: factor_matrix = self.factor_net(f1).view(-1, self.nstate_message, self.nstate_message) else: factor_matrix = None h1list = [h1] + [None for _ in range(self.nlayer)] h2list = [h2] + [None for _ in range(self.nlayer)] alpha1list = [None for _ in range(self.nlayer + 1)] alpha2list = [None for _ in range(self.nlayer + 1)] m1list = [torch.zeros(*h1.shape[:-1], self.nstate_message) ] + [None for _ in range(self.nlayer)] m2list = [torch.zeros(*h2.shape[:-1], self.nstate_message) ] + [None for _ in range(self.nlayer)] for t in range(1, self.nlayer + 1): h1list[t], h2list[t], m1list[t], m2list[t], alpha1list[t], alpha2list[t] = self.one_step(t - 1, g, h1list[t - 1], h2list[t - 1], masks=masks, return_attention_weights=True, f1=f1, factor_matrix=factor_matrix, calculate_factor_matrix=False) if self.nlayer > 0: alpha1list[0] = torch.zeros_like(alpha1list[-1]) alpha2list[0] = torch.zeros_like(alpha2list[-1]) yhat1 = yhat2 = None if self.decode_method in [\"varstate_mlp\", \"varstate_linear\"]: yhat2 = self.decoder2(h2list[-1]) elif self.decode_method in [\"varseries_mlp\", \"varseries_linear\"]: yhat2 = self.decoder2( torch.cat(h2list[-self.decode_series_len:], dim=-1) ) elif self.decode_method in [\"varseriesmean_mlp\", 'varseriesmean_linear']: yhat2 = self.decoder2( torch.mean( torch.stack(h2list[-self.decode_series_len:], dim=-1), dim=-1 ).squeeze() ) elif self.decode_method in [\"varseriesmlp_mean\", \"varserieslinear_mean\"]: inp = torch.stack(h2list[-self.decode_series_len:], dim=0) yhat2 = self.decoder2(inp).mean(dim=0) else: raise NotImplementedError( \"Decode method not implemented. And should be checkecd earlier\") if not output_dynamics: return yhat2 else: return (yhat2, [ h1list, h2list, m1list, m2list, alpha1list, alpha2list ])"}
{"text_id": "8950", "text": "docstring: def create(*, db_session, user_in: UserRegister) -> DispatchUser: password = bytes(user_in.password, \"utf-8\") user = DispatchUser(**user_in.dict(exclude={\"password\"}), password=password) db_session.add(user) default_org = organization_service.get_default(db_session=db_session) db_session.add( DispatchUserOrganization( dispatch_user_id=user.id, organization_id=default_org.id, role=UserRoles.member.value ) ) default_project = project_service.get_default(db_session=db_session) db_session.add( DispatchUserProject( dispatch_user_id=user.id, project_id=default_project.id, role=UserRoles.member.value ) ) db_session.commit() return user"}
{"text_id": "8951", "text": "docstring: def train(self, training_set_inputs, training_set_outputs, num_iteration): for iteration in range(num_iteration): output = self.predict(training_set_inputs) error = training_set_outputs - output adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output)) self.weights += adjustment"}
{"text_id": "8952", "text": "docstring: def testValue(self): basic_pkg = get_vistrails_basic_pkg_id() p = ModuleParam() p.type = \"Float\" p.identifier = basic_pkg assert p.value() == 0.0 p.strValue = \"1.5\" assert p.value() == 1.5 p.type = \"Integer\" p.identifier = basic_pkg p.strValue = \"\" assert p.value() == 0 p.strValue = \"2\" assert p.value() == 2 p.type = \"String\" p.identifier = basic_pkg p.strValue = \"\" assert p.value() == \"\" p.strValue = \"test\" assert p.value() == \"test\" p.type = \"Boolean\" p.identifier = basic_pkg p.strValue = \"\" assert p.value() == False p.strValue = \"False\" assert p.value() == False p.strValue = \"True\" assert p.value() == True"}
{"text_id": "8953", "text": "docstring: def reseed(self, seed_seq=None): if seed_seq is None: seed_seq = SeedSequence() elif isinstance(seed_seq, SeedSequence): seed_seq = seed_seq elif isinstance(seed_seq, Seeder): seed_seq = seed_seq.seed_seq else: seed_seq = SeedSequence(seed_seq) seed_seq = seed_seq.spawn(1)[0] self.seed_seq = seed_seq self.rng = default_rng(self.seed_seq)"}
{"text_id": "8954", "text": "docstring: def _gen_code_has_security_label_method(self) -> list: self._add_tql_imports() return [ f'{self.i1}@property', f'{self.i1}def has_security_label(self):', f'{self.i2}\"\"\"Return **SecurityLabel** for further filtering.\"\"\"', f'{self.i2}# first-party', ( f'{self.i2}from tcex.api.tc.v3.security_labels.security_label_filter import ' f'SecurityLabelFilter' ), '', f'{self.i2}security_labels = SecurityLabelFilter(Tql())', ( f'''{self.i2}self._tql.add_filter('hasSecurityLabel', ''' '''TqlOperator.EQ, security_labels, TqlType.SUB_QUERY)''' ), f'{self.i2}return security_labels', '', ]"}
{"text_id": "8955", "text": "docstring: def from_dict(cls, action_dict): return cls( action_dict['action_type'], action_dict['action_customization_args'], action_dict['schema_version'])"}
{"text_id": "8956", "text": "docstring: def assign_sub(ref, value, use_locking=None, name=None): if ref.dtype._is_ref_dtype: return gen_state_ops.assign_sub( ref, value, use_locking=use_locking, name=name) return ref.assign_sub(value)"}
{"text_id": "8957", "text": "docstring: def reset(self): if self.running: self.__exit__(None, None, None) self.radiant_stitcher = self.sticher_factory() self.dire_stitcher = self.sticher_factory() self.__enter__() return self.radiant_stitcher, self.dire_stitcher"}
{"text_id": "8958", "text": "docstring: def calculate_chi(self, a0=1, chi0=0.0): ai = self._data[::-1, 4] ** self.thetaref a0 = a0 ** self.thetaref chi = [chi0] for n in range(len(ai)): if n > 0: dx = self._data[n, 3] - self._data[n - 1, 3] chi.append(chi[n - 1] + (a0 * dx / ai[n])) self._data[:, 5] = chi[::-1]"}
{"text_id": "8959", "text": "docstring: def publish(arguments): delivery_path = get_path_setting('handoffs') / arguments.handoff_name / 'translations' if not delivery_path.exists(): print('Folder does not exist: {}. Exiting.'.format(delivery_path)) exit() deliverable = ho.get_deliverable(delivery_path, arguments.defer, arguments.subset) ho.register_new_localized_content(deliverable) ho.relink_articles(deliverable) ho.upload_images(deliverable) ho.upload_articles(deliverable) ho.print_publish_email(deliverable, arguments.handoff_name) print('\\nProcess done\\n')"}
{"text_id": "8960", "text": "docstring: def forward(self, characters): batch, num_tokens, num_chars = characters.size() char_emb = self.embeddings( characters.view(batch*num_tokens, num_chars)) mask = (characters != self.PAD_CHAR) mask = mask.view(batch*num_tokens, num_chars, 1).float() char_emb = char_emb * mask char_emb = torch.transpose(char_emb, 1, 2) _outputs = [ self.activation(_layer(char_emb)).max(dim=2)[0] for _layer in self.layers] if len(_outputs) > 1: output = torch.cat(_outputs, dim=1) else: output = _outputs[0] if self.projection: output = self.projection(output) output = output.view(batch, num_tokens, self.output_dim) return output"}
{"text_id": "8961", "text": "docstring: def dispatchCommand(self, box): cmd = box[COMMAND] responder = self.locator.locateResponder(cmd) if responder is None: description = \"Unhandled Command: %r\" % (cmd,) return fail(RemoteAmpError( UNHANDLED_ERROR_CODE, description, False, local=Failure(UnhandledCommand()))) return maybeDeferred(responder, box)"}
{"text_id": "8962", "text": "docstring: def infer_tensor(self, faces, target_embs): embs = [] for img in faces: if self.tta: with torch.no_grad(): mirror = trans.functional.hflip(img) emb = self.model(self.test_transform(img).to(self.conf.device).unsqueeze(0)) emb_mirror = self.model(self.test_transform(mirror).to(self.conf.device).unsqueeze(0)) embs.append(l2_norm(emb + emb_mirror)) else: with torch.no_grad(): embs.append(self.model(self.test_transform(img).to(self.conf.device).unsqueeze(0))) source_embs = torch.cat(embs) diff = source_embs.unsqueeze(-1) - target_embs.transpose(1, 0).unsqueeze(0).to(self.conf.device) dist = torch.sum(torch.pow(diff, 2), dim=1) minimum, min_idx = torch.min(dist, dim=1) min_idx[minimum > self.threshold] = -1 return min_idx, minimum, source_embs"}
{"text_id": "8963", "text": "docstring: def update_classpath(path): for prefix, replacement in DEPRECATION_RULES: if path.startswith(prefix): new_path = path.replace(prefix, replacement, 1) warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path), ScrapyDeprecationWarning) return new_path return path"}
{"text_id": "8964", "text": "docstring: def convert_htanh(with_htanh_filename, no_htanh_filename): with open(with_htanh_filename, \"r\") as original_network: with open(no_htanh_filename, \"w\") as no_htanh_network: in_hard_tanh = False weights = None biases = None for line in original_network: if line.strip() == \"HardTanh\": in_hard_tanh = True no_htanh_network.write(\"ReLU\\n\") elif in_hard_tanh and not weights: weights = line no_htanh_network.write(line) elif in_hard_tanh and not biases: assert \",\" not in line bias = float(line.strip(\"\\n[]\")) no_htanh_network.write(\"[{}]\\n\".format(bias + 1.0)) no_htanh_network.write(\"ReLU\\n\") no_htanh_network.write(\"[[-1.0]]\\n\") no_htanh_network.write(\"[2.0]\\n\") no_htanh_network.write(\"Affine\\n\") no_htanh_network.write(\"[[-1.0]]\\n\") no_htanh_network.write(\"[1.0]\\n\") in_hard_tanh = False else: no_htanh_network.write(line)"}
{"text_id": "8965", "text": "docstring: def load_bib(bib_file: Union[str, Path]) -> list: if isinstance(bib_file, str): bib_file = Path(bib_file) if not bib_file.exists(): raise FileNotFoundError(f\"{bib_file} does not exist\") with open(bib_file, \"r\") as bibtex_file: parser = BibTexParser(common_strings=True) parser.customization = convert_to_unicode bib_database = bibtexparser.load(bibtex_file, parser=parser) return bib_database.entries"}
{"text_id": "8966", "text": "docstring: def _wait_for_job_running(self): _LOGGER.debug('Waiting for Job running (%s)...', self._job_name) running = False while not running: api_response = None try: api_response = _CORE_API.\\ list_namespaced_pod(namespace=_NAMESPACE, label_selector=\"job-name={name}\" .format(name=self._job_name)) except client.rest.ApiException: _LOGGER.debug('Failed to get API response. Server busy?') except urllib3.exceptions.MaxRetryError: _LOGGER.debug(\"MaxRetryError. Server busy?\") if api_response and api_response.items: status = api_response.items[0].status.phase if status in ['Running', 'Succeeded', 'Failed', 'Unknown']: running = True self._pod_name = api_response.items[0].metadata.name if not running: time.sleep(_POLL_PERIOD_S) _LOGGER.debug('Job running (%s).', self._job_name)"}
{"text_id": "8967", "text": "docstring: def enable_local_cache(proxy,enabled=True): try: proxy.____local_cache__ = {} if enabled else None except: pass return proxy"}
{"text_id": "8968", "text": "docstring: def deserialize(self, data): minval, maxval = float(\"-inf\"), float(\"inf\") def rdeserialize(data, minval, maxval): if data[0] == \"None\": data.pop(0) return None val = int(data.pop(0)) if minval < val < maxval: root = TreeNode(val) root.left = rdeserialize(data, minval, val) root.right = rdeserialize(data, val, maxval) return root data = data.split(\",\") return rdeserialize(data, minval, maxval)"}
{"text_id": "8969", "text": "docstring: async def publish_event(business: Business, filing: Filing): try: payload = {'filing': { 'identifier': business.identifier, 'legalName': business.legal_name, 'filingId': filing.id, 'effectiveDate': filing.effective_date.isoformat(), 'legalFilings': get_filing_types(filing.filing_json) } } subject = APP_CONFIG.ENTITY_EVENT_PUBLISH_OPTIONS['subject'] await qsm.service.publish(subject, payload) except Exception as err: capture_message('Queue Publish Event Error: filing.id=' + str(filing.id) + str(err), level='error') logger.error('Queue Publish Event Error: filing.id=%s', filing.id, exc_info=True)"}
{"text_id": "8970", "text": "docstring: def recruit_macrophages(self, state: State) -> None: from nlisim.modules.mip1b import MIP1BState from nlisim.util import TissueType, activation_function macrophage: MacrophageState = state.macrophage mip1b: MIP1BState = state.mip1b voxel_volume: float = state.voxel_volume space_volume: float = state.space_volume lung_tissue = state.lung_tissue num_live_macrophages = len(macrophage.cells.alive()) avg = ( macrophage.recruitment_rate * np.sum(mip1b.grid) * (1 - num_live_macrophages / macrophage.max_ma) / (mip1b.k_d * space_volume) ) number_to_recruit = max( np.random.poisson(avg) if avg > 0 else 0, macrophage.min_ma - num_live_macrophages ) if number_to_recruit > 0: activation_voxels = zip( *np.where( np.logical_and( activation_function( x=mip1b.grid, k_d=mip1b.k_d, h=self.time_step / 60, volume=voxel_volume, b=macrophage.rec_bias, ) < rg.uniform(size=mip1b.grid.shape), lung_tissue != TissueType.AIR, ) ) ) dz_field: np.ndarray = state.grid.delta(axis=0) dy_field: np.ndarray = state.grid.delta(axis=1) dx_field: np.ndarray = state.grid.delta(axis=2) for coordinates in rg.choice( tuple(activation_voxels), size=number_to_recruit, replace=True ): vox_z, vox_y, vox_x = coordinates z = state.grid.z[vox_z] y = state.grid.y[vox_y] x = state.grid.x[vox_x] dz = dz_field[vox_z, vox_y, vox_x] dy = dy_field[vox_z, vox_y, vox_x] dx = dx_field[vox_z, vox_y, vox_x] self.create_macrophage( state=state, x=x + rg.uniform(-dx / 2, dx / 2), y=y + rg.uniform(-dy / 2, dy / 2), z=z + rg.uniform(-dz / 2, dz / 2), )"}
{"text_id": "8971", "text": "docstring: def __parse_headers(self, headers): for header in headers: key, value = header.split(': ') self.headers[key] = value"}
{"text_id": "8972", "text": "docstring: def html_style(): styles = \"\"\" table.diff {font-family:\"Courier\"; border:medium; font-size:90%; color:Black} .diff_header {background-color:#e0e0e0} td.diff_header {text-align:right} .diff_next {background-color:#c0c0c0} .diff_add {background-color:#aaffaa} .diff_chg {background-color:#ffff77} .diff_sub {background-color:#ffaaaa}\"\"\" template = \"\"\" <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"> <html> <head> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=%(charset)s\" /> <title> ConfDiff </title> <style type=\"text/css\">%(styles)s</style> </head> <body> <h1 style=\"font-family:monospace;text-align:center;color:DodgerBlue;background-color:White;margin:0;padding:0;\"> <b>Configuration Difference (ConfDiff)</b> </h1> <h4 style=\"font-family:monospace;text-align:center;padding:0px;margin-top:auto;margin-bottom:auto;\"><span id='date-time'></span></h4> %(legend)s %(table)s <br> </body> <script> var dt = new Date(); document.getElementById('date-time').innerHTML=dt; </script> </html>\"\"\" table_template = \"\"\" <table class=\"diff\" id=\"difflib_chg_%(prefix)s_top\" cellspacing=\"0\" cellpadding=\"3\" align=\"center\" rules=\"groups\" > <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup> %(header_row)s <tbody> %(data_rows)s </tbody> </table>\"\"\" legend = \"\"\" <span style=\"text-align:center;background-color:white;color:Black;font-size:16px;padding:0px;margin-top:auto;margin-bottom:auto;\"> <p style=\"font-family:Courier;\"><b>Legends: </b> <a class=\"diff_add\">&nbsp;Added&nbsp</a> <a class=\"diff_chg\"> Changed</a> <a class=\"diff_sub\"> Deleted</a></p> </span> <span style=\"text-align:center;background-color:white;color:Black;font-size:16px;padding:0px;margin-top:auto;margin-bottom:auto;\"> <p style=\"font-family:Courier;\">(<a><u>f</u></a>)irst change; <a>(<a><u>n</u></a>)ext change; <a>(<a><u>t</u></a>)op</p> </span> \"\"\" d = difflib.HtmlDiff(wrapcolumn=100) d._file_template = template d._styles = styles d._legend = legend d._table_template = table_template return d"}
{"text_id": "8973", "text": "docstring: def generate_trajectory(self, box_width, box_height, batch_size): samples = self.options.sequence_length dt = 0.02 sigma = 5.76 * 2 b = 0.13 * 2 * np.pi mu = 0 self.border_region = 0.03 position = np.zeros([batch_size, samples+2, 2]) head_dir = np.zeros([batch_size, samples+2]) position[:,0,0] = np.random.uniform(-box_width/2, box_width/2, batch_size) position[:,0,1] = np.random.uniform(-box_height/2, box_height/2, batch_size) head_dir[:,0] = np.random.uniform(0, 2*np.pi, batch_size) velocity = np.zeros([batch_size, samples+2]) random_turn = np.random.normal(mu, sigma, [batch_size, samples+1]) random_vel = np.random.rayleigh(b, [batch_size, samples+1]) v = np.abs(np.random.normal(0, b*np.pi/2, batch_size)) for t in range(samples+1): v = random_vel[:,t] turn_angle = np.zeros(batch_size) if not self.options.periodic: is_near_wall, turn_angle = self.avoid_wall(position[:,t], head_dir[:,t], box_width, box_height) v[is_near_wall] *= 0.25 turn_angle += dt*random_turn[:,t] velocity[:,t] = v*dt update = velocity[:,t,None]*np.stack([np.cos(head_dir[:,t]), np.sin(head_dir[:,t])], axis=-1) position[:,t+1] = position[:,t] + update head_dir[:,t+1] = head_dir[:,t] + turn_angle if self.options.periodic: position[:,:,0] = np.mod(position[:,:,0] + box_width/2, box_width) - box_width/2 position[:,:,1] = np.mod(position[:,:,1] + box_height/2, box_height) - box_height/2 head_dir = np.mod(head_dir + np.pi, 2*np.pi) - np.pi traj = {} traj['init_hd'] = head_dir[:,0,None] traj['init_x'] = position[:,1,0,None] traj['init_y'] = position[:,1,1,None] traj['ego_v'] = velocity[:,1:-1] ang_v = np.diff(head_dir, axis=-1) traj['phi_x'], traj['phi_y'] = np.cos(ang_v)[:,:-1], np.sin(ang_v)[:,:-1] traj['target_hd'] = head_dir[:,1:-1] traj['target_x'] = position[:,2:,0] traj['target_y'] = position[:,2:,1] return traj"}
{"text_id": "8974", "text": "docstring: def _do_merge(orig_files, out_file, config, region): if not utils.file_exists(out_file): with file_transaction(out_file) as tx_out_file: _check_samples_nodups(orig_files) prep_files = run_multicore(p_bgzip_and_index, [[x, config] for x in orig_files], config) input_vcf_file = \"%s-files.txt\" % utils.splitext_plus(out_file)[0] with open(input_vcf_file, \"w\") as out_handle: for fname in prep_files: out_handle.write(fname + \"\\n\") bcftools = config_utils.get_program(\"bcftools\", config) output_type = \"z\" if out_file.endswith(\".gz\") else \"v\" region_str = \"-r {}\".format(region) if region else \"\" cmd = \"{bcftools} merge -O {output_type} {region_str} `cat {input_vcf_file}` > {tx_out_file}\" do.run(cmd.format(**locals()), \"Merge variants\") if out_file.endswith(\".gz\"): bgzip_and_index(out_file, config) return out_file"}
{"text_id": "8975", "text": "docstring: def audiomnist_fb_audionet(fs, input_len, overlap=75, num_filters=64, filter_type=5, num_classes=10, data_format='channels_first'): if data_format not in ['channels_first', 'channels_last']: raise ValueError('Unknown data format: ', data_format) if data_format == 'channels_first': input_shape = (1, input_len) else: input_shape = (input_len, 1) filter_length = int(fs/100) fb_stride = int((100 - overlap)*filter_length/100) raw_input = Input(shape=input_shape) filters = kernels.create_filter_layer(filter_type, fs, filter_length) x = filterconvolution.FilterConvolution(filter_number=num_filters, filter_type=filters, padding='same', data_format=data_format, activation='relu', strides = fb_stride)(raw_input) if filters.COSINE_AND_SINE_FILTER: x = filterconvolution.Modulus(data_format=data_format, logscale=True)(x) x = Conv1D(64, 3, padding='same', activation='relu', data_format='channels_last')(x) x = MaxPooling1D(2, strides=2)(x) x = Conv1D(128, 3, padding='same', activation='relu')(x) x = MaxPooling1D(2, strides=2)(x) x = Conv1D(128, 3, padding='same', activation='relu')(x) x = MaxPooling1D(2, strides=2)(x) x = Flatten()(x) x = Dense(512)(x) x = Dropout(0.5)(x) x = Dense(256)(x) x = Dropout(0.5)(x) out = Dense(num_classes, activation='softmax')(x) model = Model(inputs=[raw_input], outputs=[out]) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) return model"}
{"text_id": "8976", "text": "docstring: def path_suffix(self): return self._path_suffix"}
{"text_id": "8977", "text": "docstring: def poll_issue_close_stale(api): __log.info(\"Checking for stale issues...\") issues = gh.issues.get_oldest_open_issues(api, settings.URN) __log.info(\"Got the oldest %d open issues\" % len(issues)) for issue in issues: number = issue[\"number\"] last_updated = arrow.get(issue[\"updated_at\"]) now = arrow.utcnow() delta = (now - last_updated).total_seconds() __log.info(\"Issue %d has not been updated in %d seconds\" % (number, delta)) if delta > settings.ISSUE_STALE_THRESHOLD: __log.info(\"/vote close issue %d\" % number) body = \"/vote close \\n\\nThis issue hasn't been active for a while. \" + \\ \"To keep it open, react with :-1:\" gh.comments.leave_comment(api, settings.URN, number, body)"}
{"text_id": "8978", "text": "docstring: def test(self, dataset: BaseADDataset, device: str = 'cuda', set_split=\"test\", n_jobs_dataloader: int = 0): if self.trainer is None: self.trainer = DeepSVDDTrainer(self.objective, self.R, self.c, self.nu, device=device, n_jobs_dataloader=n_jobs_dataloader) self._test(self.trainer, dataset, set_split) return self"}
{"text_id": "8979", "text": "docstring: def dataset_type(df,df_query,df_query_negative,type_dataframe): df_positive = df.drop(columns='negative_concept') df_positive = df_positive.explode('concept_ids').reset_index(drop=True) df_positive_final = df_positive.merge(df_query, how = 'inner', on = ['concept_ids']) df_positive_final.insert(0, 'split', type_dataframe) df_positive_final.insert(1, 'score', 1) df_negative = df.drop(columns='concept_ids') df_negative = df_negative.explode('negative_concept').reset_index(drop=True) df_negative_final = df_negative.merge(df_query_negative, how = 'inner', on = ['negative_concept']) df_negative_final = df_negative_final.rename({'negative_concept': 'concept_ids'}, axis=1) df_negative_final.insert(0, 'split', type_dataframe) df_negative_final.insert(1, 'score', 0) dataframe = pd.concat([df_positive_final,df_negative_final], ignore_index=True) dataframe[\"sentence2\"] = dataframe.apply(lambda row: \",\".join([x for x in row[[\"prefLabel\", \"altlabels\"]] if not pd.isna(x)]),axis=1) dataframe = dataframe.drop(columns=['prefLabel','altlabels']) final_dataframe = dataframe.rename({'body_grobid': 'sentence1'}, axis=1) return final_dataframe"}
{"text_id": "8980", "text": "docstring: def checkTesseract(): if os.name == 'posix': code = os.system(tesseractCommand + \" > /dev/null 2>&1\") elif os.name == 'nt': code = os.system(tesseractCommand + \" > nul 2>&1\") if code == 0: return True elif code == 256: return True else: return False"}
{"text_id": "8981", "text": "docstring: def _install_interrupt_handler(self, interrupt_key, callback, installer='modman'): if interrupt_key in self.external_interrupts: raise InterruptAlreadyInstalledError('interrupt already installed') self.logger.debug('custom interrupt \"{}\" was installed, calls \"{}\"' .format(interrupt_key, callback)) self.external_interrupts[interrupt_key] =\\ ModuleManagerMethod(call=callback, owner=installer)"}
{"text_id": "8982", "text": "docstring: def __clear_buffers(self): try: self._port.reset_input_buffer() self._port.reset_output_buffer() except AttributeError: self._port.flushInput() self._port.flushOutput()"}
{"text_id": "8983", "text": "docstring: def _quick_dropdown(self, options, chosen='', label=''): menu = cmds.optionMenu(label=label) for option in options: cmds.menuItem(label=option) if chosen: cmds.optionMenu(menu, e=True, value=chosen) return menu"}
{"text_id": "8984", "text": "docstring: def store_burrows(self): burrow_tracks = self.result['burrows/tracks'] ground_polygon = geometry.Polygon(self.get_ground_polygon_points()) for burrow in self.burrows: track_ids = [track_id for track_id, burrow_last in self.active_burrows() if burrow_last.intersects(burrow)] if len(track_ids) > 1: track_longest, length_max = None, 0 for track_id in track_ids: burrow_last = burrow_tracks[track_id].last if burrow_last.length > length_max: track_longest, length_max = track_id, burrow_last.length burrow.merge(burrow_last) try: polygon = burrow.polygon.intersection(ground_polygon) except geos.TopologicalError: continue if polygon.is_empty: continue try: burrow.contour = regions.get_enclosing_outline(polygon) except TypeError: continue if burrow.linestring.length > 0: line = burrow.linestring.intersection(ground_polygon) else: line = None if isinstance(line, geometry.multilinestring.MultiLineString): index_longest = np.argmax(l.length for l in line) line = line[index_longest] is_line = isinstance(line, geometry.linestring.LineString) if not is_line or line.is_empty or line.length <= 1: end_points = self.burrow_estimate_exit(burrow) if end_points is not None and len(end_points) > 0: self.calculate_burrow_centerline(burrow, point_start=end_points[0]) else: burrow.centerline = None else: line = np.array(line, np.double) line[0] = curves.get_projection_point(self.ground.linestring, line[0]) burrow.centerline = line if burrow.is_valid: if len(track_ids) > 1: burrow_tracks[track_longest].append(self.frame_id, burrow) elif len(track_ids) == 1: burrow_tracks[track_ids[0]].append(self.frame_id, burrow) else: burrow_track = BurrowTrack(self.frame_id, burrow) burrow_tracks.append(burrow_track) self.burrows = [b.copy() for _, b in self.active_burrows(time_interval=0)]"}
{"text_id": "8985", "text": "docstring: def save_transformed_sample(opts, transforms, n_sample=3): if n_sample == -1: logger.info(f\"n_sample={n_sample}: Save full transformed corpus.\") elif n_sample == 0: logger.info(f\"n_sample={n_sample}: no sample will be saved.\") return elif n_sample > 0: logger.info(f\"Save {n_sample} transformed example/corpus.\") else: raise ValueError(f\"n_sample should >= -1, get {n_sample}.\") corpora = get_corpora(opts, is_train=True) datasets_iterables = build_corpora_iters( corpora, transforms, opts.data, skip_empty_level=opts.skip_empty_level) sample_path = os.path.join( os.path.dirname(opts.save_data), CorpusName.SAMPLE) os.makedirs(sample_path, exist_ok=True) for c_name, c_iter in datasets_iterables.items(): dest_base = os.path.join( sample_path, \"{}.{}\".format(c_name, CorpusName.SAMPLE)) with open(dest_base + \".src\", 'w', encoding=\"utf-8\") as f_src,\\ open(dest_base + \".tgt\", 'w', encoding=\"utf-8\") as f_tgt: for i, item in enumerate(c_iter): maybe_example = DatasetAdapter._process(item, is_train=True) if maybe_example is None: continue src_line, tgt_line = (maybe_example['src']['src'], maybe_example['tgt']['tgt']) f_src.write(src_line + '\\n') f_tgt.write(tgt_line + '\\n') if n_sample > 0 and i >= n_sample: break"}
{"text_id": "8986", "text": "docstring: def signal_control(func): init_signal_control(func) @wraps(func) def signal_control_wrapper(*args, **kwargs): signal_name_dict = { pre_init: 'pre_init', post_init: 'post_init', pre_save: 'pre_save', post_save: 'post_save', pre_delete: 'pre_delete', post_delete: 'post_delete', m2m_changed: 'm2m_changed', pre_migrate: 'pre_migrate', post_migrate: 'post_migrate', user_logged_in: 'user_logged_in', user_logged_out: 'user_logged_out', user_login_failed: 'user_login_failed', } signal_name = func.__name__ if kwargs.get('instance', None): model_name = kwargs['instance']._meta.model.__name__ else: model_name = None if kwargs.get('instance', None): app_name = kwargs['instance']._meta.model._meta.app_label else: app_name = find_app_name(func.__code__.co_filename, signal_name) signal = kwargs['signal'] signal_type_name = signal_name_dict.get(signal, 'unknown') lookup_data = dict(app_name=app_name, model_name=model_name, signal_name=signal_name, signal_type=signal_type_name) default_data = dict(app_name=app_name, model_name=model_name, signal_name=signal_name, signal_type=signal_type_name, enabled=True) control_instance = SignalControl.objects.get_or_create(**lookup_data, defaults=default_data)[0] if control_instance.enabled == False: return None else: return func(*args, **kwargs) return signal_control_wrapper"}
{"text_id": "8987", "text": "docstring: def datetime_range_difference( minuend: DateTimeRange, subtrahend: DateTimeRange ) -> List[DateTimeRange]: if subtrahend.start <= minuend.start and subtrahend.end >= minuend.end: return [] elif subtrahend.end <= minuend.start or subtrahend.start >= minuend.end: return [copy(minuend)] elif subtrahend.start > minuend.start and subtrahend.end < minuend.end: return [ DateTimeRange(start=minuend.start, end=subtrahend.start), DateTimeRange(start=subtrahend.end, end=minuend.end), ] else: if subtrahend.start <= minuend.start: return [DateTimeRange(start=subtrahend.end, end=minuend.end)] else: return [DateTimeRange(start=minuend.start, end=subtrahend.start)]"}
{"text_id": "8988", "text": "docstring: def storage_pools_update(context, storage_pools): session = get_session() with session.begin(): storage_pool_refs = [] for storage_pool in storage_pools: LOG.debug('updating storage_pool {0}:'.format( storage_pool.get('id'))) query = _storage_pool_get_query(context, session) result = query.filter_by(id=storage_pool.get('id') ).update(storage_pool) if not result: LOG.error(exception.StoragePoolNotFound(storage_pool.get( 'id'))) else: storage_pool_refs.append(result) return storage_pool_refs"}
{"text_id": "8989", "text": "docstring: def pageStart(self, partialContent): if partialContent and not self.requestedPartial: raise ValueError(\"we shouldn't get partial content response if we didn't want it!\") if self.waiting: try: if not self.file: self.file = self.openFile(partialContent) except IOError: self.deferred.errback(failure.Failure())"}
{"text_id": "8990", "text": "docstring: def isbi_get_data_montage(imgs_path, msks_path, nb_rows, nb_cols, rng): logger = logging.getLogger(funcname()) imgs, msks = tiff.imread(imgs_path), tiff.imread(msks_path) / 255 montage_imgs = np.empty((nb_rows * imgs.shape[1], nb_cols * imgs.shape[2]), dtype=np.float32) montage_msks = np.empty((nb_rows * imgs.shape[1], nb_cols * imgs.shape[2]), dtype=np.int8) idxs = np.arange(imgs.shape[0]) rng.shuffle(idxs) idxs = iter(idxs) for y0 in range(0, montage_imgs.shape[0], imgs.shape[1]): for x0 in range(0, montage_imgs.shape[1], imgs.shape[2]): y1, x1 = y0 + imgs.shape[1], x0 + imgs.shape[2] idx = next(idxs) montage_imgs[y0:y1, x0:x1] = imgs[idx] montage_msks[y0:y1, x0:x1] = msks[idx] return montage_imgs, montage_msks"}
{"text_id": "8991", "text": "docstring: def concat_along_axis(x_list, axis): if len(x_list) < 1: return x_list return ConcatOp(x_list, [x.axes[x.axes.index(axis)] for x in x_list])"}
{"text_id": "8992", "text": "docstring: def plot_serie_erro(erroD, erroC): xspace = np.array([128, 256, 512, 1024, 2048]) plt.clf() plt.plot(xspace, erroD) plt.plot(xspace, erroC) plt.ylim(0, 0.12) plt.legend(['Erro Teste D', 'Erro Teste C']) plt.ylabel('Erro quadr\u00e1tico') plt.xlabel('N') plt.suptitle('Evolu\u00e7\u00e3o do erro em fun\u00e7\u00e3o de N') plt.savefig('{}.png'.format('plots/erroXn')) plt.show()"}
{"text_id": "8993", "text": "docstring: def interpolate_color(self, tree=None): if tree is None: tree = self.T for c in tree.get_nonterminals(): tmp_color = np.array([0,0,0]) terms = c.get_terminals() leaf_count = 0 for leaf in terms: if leaf.color: tmp_color+=np.array([leaf.color.red, leaf.color.green, leaf.color.blue]) leaf_count+=1 if leaf_count: c.color = (tmp_color/leaf_count).tolist() else: c.color = (0,0,0)"}
{"text_id": "8994", "text": "docstring: def CellStartsWith(regex, cell): if len(cell['source']) == 0: return False return re.match(regex, cell['source'][0])"}
{"text_id": "8995", "text": "docstring: def _getWaiters(self): return self._adevs"}
{"text_id": "8996", "text": "docstring: async def on_ready(): logger.info(f\"{GREEN}{bot.user.name} online and connected to Discord.{ENDC}\") guilds = \"\" for guild in bot.guilds: guilds += guild.name + ', ' logger.debug(f\"{GREEN}Connected to:{ENDC} {guilds[:-2]}.\")"}
{"text_id": "8997", "text": "docstring: def generate_regression_sample(n_samples, n_features): X = numpy.random.normal(size=[n_samples, n_features]) columns = [\"column\" + str(x) for x in range(n_features)] X = pandas.DataFrame(X, columns=columns) y = expit(numpy.sum(X, axis=1)) + numpy.random.normal(size=n_samples) * 0.05 return X, y"}
{"text_id": "8998", "text": "docstring: def create_osmshift_buttons(doc_link: str, button_labels: List[str], name: str, visibilities: List[List[bool]] ) -> List[go.layout.Updatemenu]: titles = [f'<a href=\"{doc_link}\">{label} {name}</a>' for label in button_labels] return [ go.layout.Updatemenu( type='buttons', buttons=[ dict( label=label, method='update', args=[{'visible': visibility}, {'title': button_title}] ) for label, visibility, button_title in zip(button_labels, visibilities, titles) ] ) ]"}
{"text_id": "8999", "text": "docstring: def see_sub(self, compressed_data=None): SCHEDULER.is_occupied[\"subcam\"] = True self.set_sub_image(compressed_data) road_lines = self.get_road_line_by_sub() if road_lines is not None and len(road_lines) is not 4: if SCHEDULER.debug_option[\"warn_road_lines\"]: rospy.logwarn(\"[LIB_EYE] road_lines: \") pprint(road_lines) if not isinstance(road_lines, list): self.info_sub[\"has_line\"] = False elif len(road_lines) < 2: self.info_sub[\"has_line\"] = False else: self.info_sub[\"has_line\"] = True if road_lines and len(road_lines) > 1: sum_x, sum_y = 0, 0 divider = len(road_lines) * 2 for line in road_lines: sum_x = sum_x + line['points'][0] + line['points'][2] sum_y = sum_y + line['points'][1] + line['points'][3] average_x, average_y = sum_x / divider, sum_y / divider self._buffer_sub.append(average_x) if len(self._buffer_sub) == BUF_SIZE: self.info_sub[\"center\"] = int(sum(self._buffer_sub) / BUF_SIZE) - self.threshold_sub_center self._buffer_sub.pop(0) else: rospy.logdebug(\"[LIB_EYE] len(_buffer_sub): {:d}\".format(len(self._buffer_sub))) if road_lines: slope = 0 for line in road_lines: slope = slope + line['slope'] self.info_sub[\"slope\"] = float(\"{:.2f}\".format( 1 + (slope / len(road_lines)))) image_center = int(self.info_sub[\"center\"] + (IMAGE_SUB_WIDTH /2)) line_center = [{ \"points\": [image_center, 0, image_center, 480] }] self.draw_lines(line_center, self.images[\"sub/canny\"], color=\"blue\") self.draw_lines(road_lines, self.images[\"sub/canny\"], color=\"blue\") self.publish_image() return self.info_sub"}
{"text_id": "9000", "text": "docstring: def handle_dict(data: list, output_path: str, title: str) -> None: pd.set_option('colheader_justify', 'center') df = pd.DataFrame(data, index=None) df.fillna(\"\", inplace=True) df = df.reindex(create_srg_export.HEADERS, axis=1) with open(pathlib.Path(output_path), 'w+') as f: html_table = df.to_html().replace(\"\\\\n\", \"<br>\") html_page = HTML_OUTPUT_TEMPLATE.format(title=title, table=html_table) f.write(html_page)"}
{"text_id": "9001", "text": "docstring: def plot_tree(self): print(\"\\n\") self.tree_rek(self.clusters, \"\") pass"}
{"text_id": "9002", "text": "docstring: def EmbedWindow(self, window_name, docked_name=None, size_w=-1, size_h=-1, pid=0, area_add=1, area_allowed=15, timeout=500): with self._lock: if not docked_name: docked_name = window_name self._check_connection() command = 'WinProcDock' self._send_line(command) self._send_line(docked_name) self._send_line(window_name) self._send_array([size_w, size_h]) self._send_line(str(pid)) self._send_int(area_allowed) self._send_int(area_add) self._send_int(timeout) result = self._rec_int() self._check_status() return result > 0"}
{"text_id": "9003", "text": "docstring: def delete(self): self.skype.conn(\"DELETE\", \"{0}/users/ME/conversations/{1}/messages\".format(self.skype.conn.msgsHost, self.id), auth=SkypeConnection.Auth.RegToken)"}
{"text_id": "9004", "text": "docstring: def enum_assemble(node, neighbors, prev_nodes=None, prev_amap=None, max_ncand=2000): if prev_nodes is None: prev_nodes = [] if prev_amap is None: prev_amap = [] all_attach_confs = [] singletons = [nei_node['nid'] for nei_node in neighbors + prev_nodes if nei_node['mol'].GetNumAtoms() == 1] def search(cur_amap, depth): if len(all_attach_confs) > max_ncand: return if depth == len(neighbors): all_attach_confs.append(cur_amap) return nei_node = neighbors[depth] cand_amap = enum_attach(node['mol'], nei_node, cur_amap, singletons) cand_smiles = set() candidates = [] for amap in cand_amap: cand_mol = local_attach(node['mol'], neighbors[:depth+1], prev_nodes, amap) cand_mol = sanitize(cand_mol) if cand_mol is None: continue smiles = get_smiles(cand_mol) if smiles in cand_smiles: continue cand_smiles.add(smiles) candidates.append(amap) if len(candidates) == 0: return for new_amap in candidates: search(new_amap, depth + 1) search(prev_amap, 0) cand_smiles = set() candidates = [] for amap in all_attach_confs: cand_mol = local_attach(node['mol'], neighbors, prev_nodes, amap) cand_mol = Chem.MolFromSmiles(Chem.MolToSmiles(cand_mol)) smiles = Chem.MolToSmiles(cand_mol) if smiles in cand_smiles: continue cand_smiles.add(smiles) Chem.Kekulize(cand_mol) candidates.append((smiles, cand_mol, amap)) return candidates"}
{"text_id": "9005", "text": "docstring: def unbox(self, dictionary): self.code = dictionary.get('code') self.message = dictionary.get('message')"}
{"text_id": "9006", "text": "docstring: def draw_loop_edge(self, edge, vertex): ctx = self.context ctx.set_source_rgba(*edge.color) ctx.set_line_width(edge.width) radius = vertex.size * 1.5 center_x = vertex.position[0] + cos(pi / 4) * radius / 2.0 center_y = vertex.position[1] - sin(pi / 4) * radius / 2.0 ctx.arc(center_x, center_y, radius / 2.0, 0, pi * 2) ctx.stroke()"}
{"text_id": "9007", "text": "docstring: def entropy(dataset): n = len(dataset) nPos = len([x for x in dataset if x.positive]) nNeg = n - nPos if nPos == 0 or nNeg == 0: return 0.0 return -float(nPos)/n * log2(float(nPos)/n) + \\ -float(nNeg)/n * log2(float(nNeg)/n)"}
{"text_id": "9008", "text": "docstring: def group_user_del_linux(group, user): assert group_check(group), \"Group does not exist: %s\" % (group) if group_user_check(group, user): group_for_user = run(\"getent group | egrep -v '^%s:' | grep '%s' | awk -F':' '{print $1}' | grep -v %s; true\" % (group, user, user)).splitlines() if group_for_user: sudo(\"usermod -G '%s' '%s'\" % (\",\".join(group_for_user), user)) else: sudo(\"usermod -G '' '%s'\" % (user))"}
{"text_id": "9009", "text": "docstring: def extract_geometric_plane(polygon: Polygon, plane_triangle_indices, tri_mesh: HalfEdgeTriangulation, normal: np.ndarray): all_points = np.asarray(polygon.exterior.coords) normal_ransac, centroid, _ = estimate_plane(all_points) return dict(point=centroid, normal=normal, all_points=all_points, area=polygon.area, normal_ransac=normal_ransac)"}
{"text_id": "9010", "text": "docstring: def from_pretrained(cls, filename=None, num_filters=128, **kwargs): if filename is None: name = (cls.model_name, num_filters) if name in MODEL_WEIGHTS_URL: weights_url = MODEL_WEIGHTS_URL[name] print(f'Loading pretrained model {name[0]} with F={name[1]}...') checkpoint = torch.hub.load_state_dict_from_url(weights_url, map_location='cpu') else: raise ValueError(f'pretrained weights not found for model {name}, please specify a checkpoint') else: checkpoint = torch.load(filename, map_location='cpu') net = cls(checkpoint['num_classes'], num_filters=num_filters, **kwargs) net.load_checkpoint(checkpoint) return net"}
{"text_id": "9011", "text": "docstring: def wave_stbl_2d(choice='axial'): bif = np.loadtxt('twod_wave_stble_test.ode.axial.allinfo.dat') dat_g = bif[:,3] dat_re = bif[:,6] dat_im = bif[:,7] fig = plt.figure(figsize=(8,3)) ax = fig.add_subplot(121) nu = np.linspace(.00,1.,200) lam = np.linspace(-1,1,200) X,Y = np.meshgrid(nu,lam,indexing='xy') z = L_ana(X,Y) z[z>=5] = 5 z[z<=-5] = -5 ax.plot(dat_g,dat_re,color='black',lw=2,label='Re') ax.plot(dat_g,dat_im,color='gray',lw=2) ax.plot(dat_g,-dat_im,color='gray',lw=2,label='Im') ax.plot([0,.33],[0,0],lw=2,color='gray') cs = ax.contour(X,Y,z,levels=[0.]) ax.plot([0,1],[0,0],color='gray',ls='--') cs.collections[0].set_color('black') cs.collections[0].set_linewidth(2) ax.set_ylabel(r'$\\lambda_1$') ax.set_xlabel(r'$\\nu$') ax2 = fig.add_subplot(122) lam = np.linspace(-.05,.25,150) X,Y = np.meshgrid(nu,lam,indexing='xy') z2 = L_ana(X,Y,choice=2) z2[z2>=5] = 5 z2[z2<=-5] = -5 cs2 = ax2.contour(X,Y,z2,levels=[0.]) cs2.collections[0].set_color('black') cs2.collections[0].set_linewidth(2) ax2.plot([0,1],[0,0],color='gray',ls='--') ax2.set_ylabel(r'$\\lambda_2$') ax2.set_xlabel(r'$\\nu$') ax.set_xlim(0,1.) ax2.set_xlim(0,1.) ax.set_ylim(-1.5,1.5) ax2.set_ylim(-.05,.25) ax.legend(loc=2) return fig"}
{"text_id": "9012", "text": "docstring: def _get_higher_res(S: int, slide_writer: PyramidWriter, X: typing.Tuple[int,int] = None, Y: typing.Tuple[int,int] = None, Z: typing.Tuple[int,int] = (0,1)): scale_info = slide_writer.scale_info(S) if X == None: X = [0,scale_info['size'][0]] if Y == None: Y = [0,scale_info['size'][1]] if X[1] > scale_info['size'][0]: X[1] = scale_info['size'][0] if Y[1] > scale_info['size'][1]: Y[1] = scale_info['size'][1] if str(S)==slide_writer.scale_info(-1)['key']: with ProcessManager.thread(): with bfio.BioReader(slide_writer.image_path,max_workers=1) as br: image = br[Y[0]:Y[1],X[0]:X[1],Z[0]:Z[1],...].squeeze() slide_writer.store_chunk(image,str(S),(X[0],X[1],Y[0],Y[1])) return image else: image = np.zeros((Y[1]-Y[0],X[1]-X[0]),dtype=slide_writer.dtype) subgrid_dims = [[2*X[0],2*X[1]],[2*Y[0],2*Y[1]]] for dim in subgrid_dims: while dim[1]-dim[0] > CHUNK_SIZE: dim.insert(1,dim[0] + ((dim[1] - dim[0]-1)//CHUNK_SIZE) * CHUNK_SIZE) def load_and_scale(*args,**kwargs): sub_image = _get_higher_res(**kwargs) with ProcessManager.thread(): image = args[0] x_ind = args[1] y_ind = args[2] image[y_ind[0]:y_ind[1],x_ind[0]:x_ind[1]] = kwargs['slide_writer'].scale(sub_image) with ThreadPoolExecutor(1) as executor: for y in range(0,len(subgrid_dims[1])-1): y_ind = [subgrid_dims[1][y] - subgrid_dims[1][0],subgrid_dims[1][y+1] - subgrid_dims[1][0]] y_ind = [np.ceil(yi/2).astype('int') for yi in y_ind] for x in range(0,len(subgrid_dims[0])-1): x_ind = [subgrid_dims[0][x] - subgrid_dims[0][0],subgrid_dims[0][x+1] - subgrid_dims[0][0]] x_ind = [np.ceil(xi/2).astype('int') for xi in x_ind] executor.submit(load_and_scale, image,x_ind,y_ind, X=subgrid_dims[0][x:x+2], Y=subgrid_dims[1][y:y+2], Z=Z, S=S+1, slide_writer=slide_writer) slide_writer.store_chunk(image,str(S),(X[0],X[1],Y[0],Y[1])) return image"}
{"text_id": "9013", "text": "docstring: def custom_headers(self) -> Sequence['outputs.GetTrafficManagerProfileMonitorConfigCustomHeaderResult']: return pulumi.get(self, \"custom_headers\")"}
{"text_id": "9014", "text": "docstring: def build_blog_pagination_dict(post_db: dict, posts_per_page: int = 10) -> dict: all_posts = sort_posts(post_db, 'date') n_pages = math.ceil(len(all_posts) / posts_per_page) paginated_blog = {} paginated_blog['all_posts'] = all_posts.copy() paginated_blog['n_pages'] = n_pages for page_number in range(n_pages + 1)[1:]: paginated_blog[page_number] = {} paginated_blog[page_number]['posts'] = [all_posts.pop(0) for _ in range(posts_per_page) if len(all_posts) > 0] paginated_blog[page_number]['url'] = '/' if (page_number == 1) else f'/{Params.BLOG_PATH}page/{page_number}/' return paginated_blog"}
{"text_id": "9015", "text": "docstring: def multi_call(ops): _log.debug(\"Calling out to system : %s\", ops) fd, name = tempfile.mkstemp(text=True) f = os.fdopen(fd, \"w\") f.write(\"set -e\\n\") cmds = [ \" \".join(op) + \"\\n\" for op in ops ] for cmd in cmds: f.write(\"echo Executing : \" + cmd) f.write(cmd) f.close() check_call([\"bash\", name]) os.remove(name)"}
{"text_id": "9016", "text": "docstring: def remove_right(self, right): left_removed = self._left_bin.remove_key(self._bwd, right) for x in left_removed: self._right_bin.remove_item(self._fwd, x, right)"}
{"text_id": "9017", "text": "docstring: def write_positions_snps( vcf_format: str, output_prefix: str, het_only: bool = False, r2: float = 0, samples=None, wasp_dir: str = DIR, keep_filtered_vcfs: bool = False ): write_positions_and_filtered_vcfs( vcf_format, output_prefix, het_only=het_only, r2=r2, samples=samples ) output_dir = os.path.dirname(output_prefix) get_snps_from_vcfs( output_dir, output_dir, wasp_dir=wasp_dir ) if not keep_filtered_vcfs: for chr in range(1, 23): os.remove('{}.chr{}.vcf.gz'.format(output_prefix, chr))"}
{"text_id": "9018", "text": "docstring: def write_multiple_predictions_csv(self, matrix): csv_filename = '{:s}-likelihoods.csv'.format(self.AGE_GROUP) csv_filepath = os.path.join(self.output_dir_path, csv_filename) with open(csv_filepath, 'wb') as f: UnicodeWriter(f).writerows(matrix)"}
{"text_id": "9019", "text": "docstring: def calc_intersection(counts_sim, counts_real, shots): intersection = 0 for key in counts_real.keys(): if key not in counts_sim.keys(): counts_sim[key] = 0 for key in counts_sim.keys(): if key not in counts_real.keys(): counts_real[key] = 0 intersection = intersection + min(counts_sim[key], counts_real[key]) return intersection / shots"}
{"text_id": "9020", "text": "docstring: async def client_send(ctx, msg, embed=1): try: msg = str(msg) if embed == 1: m = await ctx.send(\"```\" + msg + \"```\") else: m = await ctx.send(msg) return m except: return 1"}
{"text_id": "9021", "text": "docstring: def importFromDatabase(self,soil:dict) -> tuple: lat,lon = soil['coords'] name = soil['name'] self._reflectance = soil['reflectance'] self._wavelengths = soil['wavelengths'] self._gl = soil['params'] self._f = interp1d(self._wavelengths,self._reflectance,kind=\"quadratic\") return name,lat,lon"}
{"text_id": "9022", "text": "docstring: def GetDescription(self): author = self.GetAuthor() comment_date = self.GetCreatedOn() comment_text = self.GetContent() if not comment_text: comment_text = \"(No text was entered with this change)\" comment_text = comment_text.replace(\"<b>\", \"\") comment_text = comment_text.replace(\"</b>\", \"\") comment_text = WrapText(comment_text, 82) body = \"```\\n\" + comment_text + \"\\n```\" footer = \"\\n\\nOriginal issue reported on code.google.com by `%s` on %s\" % ( author, TryFormatDate(comment_date)) if self.GetLabels(): labels_added = [] labels_removed = [] for label in self.GetLabels(): if label.startswith(\"-\"): labels_removed.append(label[1:]) else: labels_added.append(label) footer += \"\\n\" if labels_added: footer += \"- **Labels added**: %s\\n\" % (\", \".join(labels_added)) if labels_removed: footer += \"- **Labels removed**: %s\\n\" % (\", \".join(labels_removed)) attachmentLines = [] for attachment in self._comment[\"attachments\"] if \"attachments\" in self._comment else []: if \"isDeleted\" in attachment: continue link = \"https://storage.googleapis.com/google-code-attachments/%s/issue-%d/comment-%d/%s\" % ( self.GetIssue().GetProjectName(), self.GetIssue().GetId(), self.GetId(), attachment[\"fileName\"]) def has_extension(extension): return attachment[\"fileName\"].lower().endswith(extension) is_image_attachment = False for extension in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".gif\"]: is_image_attachment |= has_extension(\".png\") if is_image_attachment: line = \" * *Attachment: %s<br>![%s](%s)*\" % ( attachment[\"fileName\"], attachment[\"fileName\"], link) else: line = \" * *Attachment: [%s](%s)*\" % (attachment[\"fileName\"], link) attachmentLines.append(line) if len(attachmentLines) > 0: footer += \"\\n<hr>\\n\" + \"\\n\".join(attachmentLines) return body + footer"}
{"text_id": "9023", "text": "docstring: def _readMoreXML(self, container, xmlNode): BaseKnapsackModel._readMoreXML(self, container, xmlNode) specs = self.getInputSpecs()() specs.parseNode(xmlNode) for node in specs.subparts: name = node.getName() val = node.value if name == 'capacity': self.capacity = val"}
{"text_id": "9024", "text": "docstring: def __is_final_unigram(self, node): return self.__unigram_weight(node) > 0"}
{"text_id": "9025", "text": "docstring: def voucher_status(row): T = current.T if hasattr(row, \"fin_voucher\"): row = row.fin_voucher if hasattr(row, \"balance\") and row.balance is not None: if row.balance > 0: return T(\"Issued##fin\") else: return T(\"Redeemed##fin\") else: return current.messages[\"NONE\"]"}
{"text_id": "9026", "text": "docstring: def delete_integration_definition(self, integration_name, **kwargs): \"\"\"Replaces any number of fields within an existing integration definition This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.delete_integration_definition(integration_name, async_req=True) >>> result = thread.get() :param integration_name: (required) :type integration_name: str :param async_req: Whether to execute the request asynchronously. :type async_req: bool, optional :param _preload_content: if False, the urllib3.HTTPResponse object will be returned without reading/decoding response data. Default is True. :type _preload_content: bool, optional :param _request_timeout: timeout setting for this request. If one number provided, it will be total request timeout. It can also be a pair (tuple) of (connection, read) timeouts. :return: Returns the result object. If the method is called asynchronously, returns the request thread. :rtype: None \"\"\" kwargs['_return_http_data_only'] = True return self.delete_integration_definition_with_http_info(integration_name, **kwargs)"}
{"text_id": "9027", "text": "docstring: def _map_elements_idx(a1, a2, return_array=False): idx_a1 = np.where(np.in1d(a1, a2))[0] idx_a2 = np.where(np.in1d(a2, a1))[0] result = list(zip(idx_a1, idx_a2)) if return_array: return np.atleast_2d(np.array(result)) else: return result"}
{"text_id": "9028", "text": "docstring: def tenant(self) -> str: return pulumi.get(self, \"tenant\")"}
{"text_id": "9029", "text": "docstring: def select(self, label_index, unlabel_index, model=None, epsilon=0.5, **kwargs): if len(unlabel_index) <= 1: return unlabel_index unlabel_index = self._check_multi_label_ind(unlabel_index) label_index = self._check_multi_label_ind(label_index) unlab_data, _, data_ind = get_Xy_in_multilabel(index=unlabel_index, X=self.X, y=self.y) W = unlabel_index.get_matrix_mask(mat_shape=self.y.shape, fill_value=1, sparse=False) if model is not None: assert isinstance(model, LabelRankingModel), 'Model for selection must be LabelRanking model in ' \\ 'AUDI algorithm. Try to pass model=None to use the ' \\ 'default model' self._lr_model = model if not self._lr_model._init_flag: lab_data, lab_lab, _ = get_Xy_in_multilabel(index=label_index, X=self.X, y=self.y) self._lr_model.fit(lab_data, lab_lab) else: lab_data, lab_lab, _ = get_Xy_in_multilabel(index=label_index, X=self.X, y=self.y) self._lr_model.fit(lab_data, lab_lab) pres, labels = self._lr_model.predict(unlab_data) avgP = np.mean(np.sum(self.y[label_index.get_unbroken_instances(), :] == 1, axis=1)) insvals = -np.abs((np.sum(labels == 1, axis=1) - avgP) / np.fmax(np.sum(W[data_ind, :] == 1, axis=1), epsilon)) selected_ins = np.argmin(insvals) pres_mask = np.asarray(1 - W[data_ind], dtype=bool) pres_tmp = pres[:, 0:-1] pres_tmp[pres_mask] = np.NINF pres[:, 0:-1] = pres_tmp sel_ins_label_mask = pres_mask[selected_ins] dis = np.abs(pres[selected_ins, 0:-1] - pres[selected_ins, -1]) selected_lab = np.argmin(dis) if sel_ins_label_mask[selected_lab]: argsorted_dis = np.argsort(dis) for dis_ind in argsorted_dis: if not sel_ins_label_mask[dis_ind]: selected_lab = dis_ind break selected_ins = data_ind[selected_ins] return [(selected_ins, selected_lab)]"}
{"text_id": "9030", "text": "docstring: def _get_line_as_dict(line: List[Word], header: List[Word]) -> dict: out = {} column_headers = [w.text for w in header] header_x = [w.x for w in header] for i in range(len(line)): word = line[i] nearest = find_nearest(header_x, word.x) col = column_headers[nearest] out[col] = word.text return out"}
{"text_id": "9031", "text": "docstring: def add_variable(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None): if context.in_graph_mode(): existing_variables = set(tf_variables.global_variables()) else: existing_variables = [] if regularizer is not None: raise RuntimeError('Variable regularization not supported in Eager ' 'mode.') if dtype is None: dtype = self.dtype or dtypes.float32 self._set_scope(None) with vs.variable_scope( self._scope, reuse=(self.built or self._reuse)) as scope: with ops.name_scope(scope.original_name_scope): variable = vs.get_variable(name, shape=shape, initializer=initializer, dtype=dtypes.as_dtype(dtype), constraint=constraint, trainable=trainable and self.trainable) if variable in existing_variables: return variable if regularizer: if isinstance(variable, tf_variables.PartitionedVariable): for v in variable: with ops.colocate_with(v.op): with ops.name_scope(name + '/Regularizer'): regularization = regularizer(v) if regularization is not None: self.add_loss(regularization) else: with ops.colocate_with(variable.op): with ops.name_scope(name + '/Regularizer'): regularization = regularizer(variable) if regularization is not None: self.add_loss(regularization) if trainable: self._trainable_weights.append(variable) else: self._non_trainable_weights.append(variable) return variable"}
{"text_id": "9032", "text": "docstring: def plot_feature_importances(df, n = 15, color = 'blue', threshold = None): df = df.sort_values('importance', ascending = False).reset_index(drop = True) df['importance_normalized'] = df['importance'] / df['importance'].sum() df['cumulative_importance'] = np.cumsum(df['importance_normalized']) plt.rcParams['font.size'] = 12 plt.style.use('fivethirtyeight') df.loc[:(n - 1), :].plot.barh(y = 'importance_normalized', x = 'feature', color = color, edgecolor = 'k', figsize = (12, 8), legend = False, linewidth = 2) plt.yticks(size = 14) plt.xticks(size = 16) plt.xlabel('Normalized Importance', size = 20); plt.ylabel(''); plt.title(f'{n} Most Important Features', size = 24) plt.gca().invert_yaxis() if threshold: plt.figure(figsize = (8, 6)) plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-') plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); plt.title('Cumulative Feature Importance', size = 18); importance_index = np.min(np.where(df['cumulative_importance'] > threshold)) plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red') plt.show(); print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, 100 * threshold)) return df"}
{"text_id": "9033", "text": "docstring: def create_descriptors(self): descriptors = DeviceDescriptorCollection() We'll add the major components of the descriptors we we want. The collection we build here will be necessary to create a standard endpoint. We'll need a device descriptor... with descriptors.DeviceDescriptor() as d: d.idVendor = 0x16d0 d.idProduct = 0xf3b d.iManufacturer = \"LUNA\" d.iProduct = \"Stress Test\" d.iSerialNumber = \"no serial\" d.bNumConfigurations = 1 ... and a description of the USB configuration we'll provide. with descriptors.ConfigurationDescriptor() as c: with c.InterfaceDescriptor() as i: i.bInterfaceNumber = 0 with i.EndpointDescriptor() as e: e.bEndpointAddress = 0x80 | BULK_ENDPOINT_NUMBER e.wMaxPacketSize = MAX_BULK_PACKET_SIZE return descriptors"}
{"text_id": "9034", "text": "docstring: def format_line_diff(base: str, other: str) -> str: buffer = [] if base: buffer.append(f\"<{base}\") if other: buffer.append(f\">{other}\") return click.style(f\"@@ {' '.join(buffer)} @@\", fg='cyan')"}
{"text_id": "9035", "text": "docstring: def _ratio(var1: int, var2: int) -> float: return float(var1/var2) if var2 != 0 else 0"}
{"text_id": "9036", "text": "docstring: def _dir_list(dir_path): import pathlib path_list = pathlib.Path(dir_path).glob('**/*') return path_list"}
{"text_id": "9037", "text": "docstring: def load_configuration(file_path): with open(file_path, 'r') as file: data = json.load(file) configurations = [] for configuration_data in data: configuration : Configuration = _from_json(configuration_data, Configuration) configurations.append(configuration) return configurations"}
{"text_id": "9038", "text": "docstring: def GetMaxBatchSize(self, run_params): if run_params.dynamic_engine: return None return 100"}
{"text_id": "9039", "text": "docstring: def life_or_death(state, neighbours): if state == 1: if neighbours == 2 or neighbours == 3: return 1 else: return 0 else: if neighbours == 3: return 1 else: return 0"}
{"text_id": "9040", "text": "docstring: def decode_control(control): control_type = str(control['controlType']) criticality = bool(control['criticality']) control_value = bytes(control['controlValue']) unprocessed = None if control_type == '1.2.840.113556.1.4.319': control_resp, unprocessed = decoder.decode(control_value, asn1Spec=RealSearchControlValue()) control_value = dict() control_value['size'] = int(control_resp['size']) control_value['cookie'] = bytes(control_resp['cookie']) elif control_type == '1.2.840.113556.1.4.841': control_resp, unprocessed = decoder.decode(control_value, asn1Spec=DirSyncControlResponseValue()) control_value = dict() control_value['more_results'] = bool(control_resp['MoreResults']) control_value['cookie'] = bytes(control_resp['CookieServer']) elif control_type == '1.3.6.1.1.13.1' or control_type == '1.3.6.1.1.13.2': control_resp, unprocessed = decoder.decode(control_value, asn1Spec=SearchResultEntry()) control_value = dict() control_value['result'] = attributes_to_dict(control_resp['attributes']) if unprocessed: if log_enabled(ERROR): log(ERROR, 'unprocessed control response in substrate') raise LDAPControlError('unprocessed control response in substrate') return control_type, {'description': Oids.get(control_type, ''), 'criticality': criticality, 'value': control_value}"}
{"text_id": "9041", "text": "docstring: def _addURLtoFileHandleService(self, externalURL, mimetype=None, md5=None, fileSize=None): fileName = externalURL.split('/')[-1] externalURL = utils.as_url(externalURL) fileHandle = {'concreteType': 'org.sagebionetworks.repo.model.file.ExternalFileHandle', 'fileName' : fileName, 'externalURL' : externalURL, 'contentMd5' : md5, 'contentSize': fileSize} if mimetype is None: (mimetype, enc) = mimetypes.guess_type(externalURL, strict=False) if mimetype is not None: fileHandle['contentType'] = mimetype return self.restPOST('/externalFileHandle', json.dumps(fileHandle), self.fileHandleEndpoint)"}
{"text_id": "9042", "text": "docstring: def make_widget_lsdtt_chi_mapping(self): widget_dicts = [] self.pp_widget = self.make_vertical_widgets(widget_dict = self.preprocessing) self.print_widget = self.make_vertical_widgets(widget_dict = self.basic_raster_printing) self.trimming_widget = self.make_vertical_widgets(widget_dict = self.trimming_options) self.geojson_widget = self.make_vertical_widgets(widget_dict = self.geojson) self.burner_widget = self.make_vertical_widgets(widget_dict = self.burner_calculations) widget_dicts.append(self.preprocessing) widget_dicts.append(self.basic_raster_printing) widget_dicts.append(self.trimming_options) widget_dicts.append(self.geojson) widget_dicts.append(self.burner_calculations) basic_accordion = widgets.Accordion(children=[self.pp_widget, self.print_widget, self.trimming_widget,self.geojson_widget,self.burner_widget]) basic_accordion.set_title(0, 'Preprocessing') basic_accordion.set_title(1, 'Basic raster printing') basic_accordion.set_title(2, 'Raster trimming') basic_accordion.set_title(3, 'Convert points to geojson') basic_accordion.set_title(4, 'Burning raster data to csv') self.basic_channel_network_widget = self.make_vertical_widgets(widget_dict = self.basic_channel_network_calculations) self.basin_extraction_widget = self.make_vertical_widgets(widget_dict = self.basin_extraction_calculations) widget_dicts.append(self.basic_channel_network_calculations) widget_dicts.append(self.basin_extraction_calculations) basic_accordion3 = widgets.Accordion(children=[self.basic_channel_network_widget,self.basin_extraction_widget]) basic_accordion3.set_title(0, 'Basic channel network') basic_accordion3.set_title(1, 'Basin extraction') self.chi_widget = self.make_vertical_widgets(widget_dict = self.chi_calculations) widget_dicts.append(self.chi_calculations) basic_accordion4 = widgets.Accordion(children=[self.chi_widget]) basic_accordion4.set_title(0, 'Simple chi. (Use lsdtt-chi-analysis for more complex chi analysis)') tab_nest = widgets.Tab() tab_nest.children = [basic_accordion, basic_accordion3, basic_accordion4] tab_nest.set_title(0, 'Prep and basic print') tab_nest.set_title(1, 'Channels and basins') tab_nest.set_title(2, 'Chi ananlysis') self.widget_list = widget_dicts return tab_nest"}
{"text_id": "9043", "text": "docstring: def frame(self) -> stack.FrameSnapshot: raise NotImplementedError"}
{"text_id": "9044", "text": "docstring: def record(self, partition, num_bytes, num_records): self.unrecorded_partitions.remove(partition) self.total_bytes += num_bytes self.total_records += num_records if not self.unrecorded_partitions: self.sensors.bytes_fetched.record(self.total_bytes) self.sensors.records_fetched.record(self.total_records)"}
{"text_id": "9045", "text": "docstring: def shutdown(self) -> None: assert self.running_dir is not None, \"shutdown called before deploy\" if self.server_process is None: assert self.server_process_pid is not None, \"No server id to kill\" os.kill(self.server_process_pid, signal.SIGTERM) else: self.server_process.terminate() self.server_process.wait() sh.rm(shlex.split(\"-rf \" + self.running_dir))"}
{"text_id": "9046", "text": "docstring: def convert_json_to_tex(data): vqw_data, subcol = insert_vertical_qw(data) states_init = get_initial_states(vqw_data) rows_formed = convert_columns_to_rows(states_init) vqw_indices = vqw_append(subcol) init_texed, initial_states = tex_initial_states(rows_formed) gates_sub = substitute_gates(init_texed, vqw_indices, subcol, initial_states) final_sub = replace_with_cw(gates_sub) return final_sub"}
{"text_id": "9047", "text": "docstring: def add_face(self, mesh_name, patch_id, v0, v1, v2): ret_val = self._add_face(mesh_name.encode(), patch_id, v0, v1, v2) return ret_val"}
{"text_id": "9048", "text": "docstring: def elitExperiment(self): miracle = random.randint(0, 100) if miracle < 20 and len(self._listSolutions) > 2: sortedSoluces = sorted(self._listSolutions, key=lambda sol: sol.getDistance()) bestCopy = Solution(sortedSoluces[0].getProblem(), copy.deepcopy(sortedSoluces[0].getPath())) secondCopy = Solution(sortedSoluces[1].getProblem(), copy.deepcopy(sortedSoluces[1].getPath())) bestCopy.cross(secondCopy) self._listSolutions[-1] = bestCopy self._listSolutions[-2] = secondCopy"}
{"text_id": "9049", "text": "docstring: def toggle_hm_port(identity_service, local_unit_name, enabled=True): session = session_from_identity_service(identity_service) try: nc = init_neutron_client(session) port = lookup_hm_port(nc, local_unit_name) except NEUTRON_TEMP_EXCS as e: raise APIUnavailable('neutron', 'ports', e) if not port: ch_core.hookenv.log('When attempting to toggle admin status of port, ' 'we unexpectedly found that no port exists for ' 'unit.', level=ch_core.hookenv.WARNING) return nc.update_port(port['id'], {'port': {'admin_state_up': enabled}})"}
{"text_id": "9050", "text": "docstring: def find_contract(address: Any) -> Any: address = to_address(address) if address not in _contract_map: return None return _contract_map[address]"}
{"text_id": "9051", "text": "docstring: def _format_submodules(self, submodules): filled_submodule_templates = [] for path, (status, err_msg) in submodules.items(): scenario_1 = isinstance(status, dict) and err_msg is None scenario_2 = status is None and isinstance(err_msg, str) assert scenario_1 or scenario_2 if scenario_1: if any(status[k] for k in ('is_detached', 'n_commits_ahead', 'n_commits_behind', 'n_staged', 'n_not_staged', 'n_untracked')): info = 'working tree is dirty' style = 'red' else: info = 'working tree is clean' style = 'green' else: info = err_msg style = 'red' if err_msg.startswith('HEAD') else 'green' info_fmt = self.apply_style(info, style) sm_mapping = {'submodule_path': path, 'submodule_info': info_fmt} filled_sm_template = SINGLE_SUBMODULE.safe_substitute(sm_mapping) filled_submodule_templates.append(filled_sm_template) return '\\n'.join(filled_submodule_templates)"}
{"text_id": "9052", "text": "docstring: def _get(self): if not hasattr(self, _get_internal_name(attr_name)): if not hasattr(self, _get_default_value_name(attr_name)): raise AttributeError( 'Local value for attribute `{}` has not been set. You can ' 'initialize it locally (`self.{} = <initial-value>`) or set a ' 'global value using the `initialize_local_attributes` ' 'helper.'.format(attr_name, attr_name)) default_attr_fn = getattr(self, _get_default_value_name(attr_name)) if not callable(default_attr_fn): raise AttributeError('Default value initializer must be callable.') _set(self, default_attr_fn()) return getattr(self, _get_internal_name(attr_name))"}
{"text_id": "9053", "text": "docstring: def from_int(int_val, bits): ret = Symbol() ret._internal_repr = bitarray.util.int2ba(int_val, length=bits) return ret"}
{"text_id": "9054", "text": "docstring: def loss(self, mlvl_mask_preds_x, mlvl_mask_preds_y, mlvl_cls_preds, gt_labels, gt_masks, img_metas, gt_bboxes=None, **kwargs): num_levels = self.num_levels num_imgs = len(gt_labels) featmap_sizes = [featmap.size()[-2:] for featmap in mlvl_mask_preds_x] pos_mask_targets, labels, \\ xy_pos_indexes = \\ multi_apply(self._get_targets_single, gt_bboxes, gt_labels, gt_masks, featmap_sizes=featmap_sizes) mlvl_pos_mask_targets = [[] for _ in range(num_levels)] mlvl_pos_mask_preds_x = [[] for _ in range(num_levels)] mlvl_pos_mask_preds_y = [[] for _ in range(num_levels)] mlvl_labels = [[] for _ in range(num_levels)] for img_id in range(num_imgs): for lvl in range(num_levels): mlvl_pos_mask_targets[lvl].append( pos_mask_targets[img_id][lvl]) mlvl_pos_mask_preds_x[lvl].append( mlvl_mask_preds_x[lvl][img_id, xy_pos_indexes[img_id][lvl][:, 1]]) mlvl_pos_mask_preds_y[lvl].append( mlvl_mask_preds_y[lvl][img_id, xy_pos_indexes[img_id][lvl][:, 0]]) mlvl_labels[lvl].append(labels[img_id][lvl].flatten()) temp_mlvl_cls_preds = [] for lvl in range(num_levels): mlvl_pos_mask_targets[lvl] = torch.cat( mlvl_pos_mask_targets[lvl], dim=0) mlvl_pos_mask_preds_x[lvl] = torch.cat( mlvl_pos_mask_preds_x[lvl], dim=0) mlvl_pos_mask_preds_y[lvl] = torch.cat( mlvl_pos_mask_preds_y[lvl], dim=0) mlvl_labels[lvl] = torch.cat(mlvl_labels[lvl], dim=0) temp_mlvl_cls_preds.append(mlvl_cls_preds[lvl].permute( 0, 2, 3, 1).reshape(-1, self.cls_out_channels)) num_pos = 0. loss_mask = [] for pred_x, pred_y, target in \\ zip(mlvl_pos_mask_preds_x, mlvl_pos_mask_preds_y, mlvl_pos_mask_targets): num_masks = pred_x.size(0) if num_masks == 0: loss_mask.append((pred_x.sum() + pred_y.sum()).unsqueeze(0)) continue num_pos += num_masks pred_mask = pred_y.sigmoid() * pred_x.sigmoid() loss_mask.append( self.loss_mask(pred_mask, target, reduction_override='none')) if num_pos > 0: loss_mask = torch.cat(loss_mask).sum() / num_pos else: loss_mask = torch.cat(loss_mask).mean() flatten_labels = torch.cat(mlvl_labels) flatten_cls_preds = torch.cat(temp_mlvl_cls_preds) loss_cls = self.loss_cls( flatten_cls_preds, flatten_labels, avg_factor=num_pos + 1) return dict(loss_mask=loss_mask, loss_cls=loss_cls)"}
{"text_id": "9055", "text": "docstring: def popular(): user_id = request.args.get('userID') if not user_id: raise BadRequest('userID is required') current_item_id = request.args.get('currentItemID') num_results = request.args.get('numResults', default = 25, type = int) if num_results < 1: raise BadRequest('numResults must be greater than zero') if num_results > 100: raise BadRequest('numResults must be less than 100') feature = request.args.get('feature') filter_ssm = request.args.get('filter', filter_purchased_cstore_param_name) if filter_ssm == 'cstore': filter_ssm = filter_cstore_param_name elif filter_ssm == 'purchased': filter_ssm = filter_purchased_cstore_param_name app.logger.info(f\"Filter SSM for /recommendations: {filter_ssm}\") fully_qualify_image_urls = request.args.get('fullyQualifyImageUrls', '0').lower() in [ 'true', 't', '1'] try: response = get_products( feature = feature, user_id = user_id, current_item_id = current_item_id, num_results = num_results, default_campaign_arn_param_name='/retaildemostore/personalize/popular-items-arn', default_filter_arn_param_name=filter_ssm, fully_qualify_image_urls = fully_qualify_image_urls ) app.logger.debug(f\"Recommendations response to be returned: {response}\") return response except Exception as e: app.logger.exception('Unexpected error generating recommendations', e) raise BadRequest(message = 'Unhandled error', status_code = 500)"}
{"text_id": "9056", "text": "docstring: def pause_or_stop(self, c, script_ID): status = self.scheduler.get_running_status(script_ID) if status is None: try_confirm = \"Trying to confirm Pause/Stop of script with ID {0} \" try_confirm += \"but it was not running\" raise Exception(try_confirm.format(script_ID)) yield status.pause() returnValue(status.should_stop)"}
{"text_id": "9057", "text": "docstring: def happy_github(mocker, monkeypatch, teams_and_members): github_instance = MagicMock() github_instance.get_user.side_effect = ( lambda user: User(login=user) if user in [USER, NOT_OWNER] else raise_404() ) type(github_instance).oauth_scopes = PropertyMock( return_value=REQUIRED_TOKEN_SCOPES ) usernames = set( itertools.chain(*[members for _, members in teams_and_members.items()]) ) def get_user(username): if username in [*usernames, USER, NOT_OWNER]: user = MagicMock(spec=github.NamedUser.NamedUser) type(user).login = PropertyMock(return_value=username) return user else: raise_404() github_instance.get_user.side_effect = get_user monkeypatch.setattr(github, \"GithubException\", GithubException) mocker.patch( \"github.Github\", side_effect=lambda login_or_token, base_url: github_instance, ) return github_instance"}
{"text_id": "9058", "text": "docstring: def price(self) -> int: pass"}
{"text_id": "9059", "text": "docstring: def unmarshal(self, data: bytes) -> None: offset, processing_bitset = 0, False for argument in self.__slots__: data_type = self.amqp_type(argument) if offset == 7 and processing_bitset: data = data[1:] offset = 0 if processing_bitset and data_type != 'bit': offset = 0 processing_bitset = False data = data[1:] consumed, value = decode.by_type(data, data_type, offset) if data_type == 'bit': offset += 1 processing_bitset = True consumed = 0 setattr(self, argument, value) if consumed: data = data[consumed:]"}
{"text_id": "9060", "text": "docstring: def update(dt): action = np.array([0.0, 0.0]) if key_handler[key.UP]: action = np.array([0.44, 0.0]) if key_handler[key.DOWN]: action = np.array([-0.44, 0]) if key_handler[key.LEFT]: action = np.array([0.8, 1]) if key_handler[key.RIGHT]: action = np.array([0.8, -1]) if key_handler[key.SPACE]: action = np.array([0, 0]) if key_handler[key.LSHIFT]: action *= 1.5 obs, reward, done, info = env.step(action) print('step_count = %s, reward=%.3f' % (env.unwrapped.step_count, reward)) if key_handler[key.RETURN]: from PIL import Image im = Image.fromarray(obs) im.save('screen.png') if done: print('done!') env.reset() env.render() env.render()"}
{"text_id": "9061", "text": "docstring: def generate_create_table_command(self): statement = \"CREATE TABLE IF NOT EXISTS {} (\".format( self.data.get('table_name')) statement += \", \".join([ \"{} {}\".format(key, value) for key, value in self.data.get('table_structure').items() ]) statement += \")\" return statement"}
{"text_id": "9062", "text": "docstring: def format_sdist_header_metadata(data, filename): description = get_header_description(data) config_items = python_version_check(data) attrs = dict(config_items) name = pop_key(attrs, 'Name', None) basename = path.basename(filename) if name is None: name = basename.split('-')[0] package_data = { 'name': name, 'summary': pop_key(attrs, 'Summary', None), 'license': pop_key(attrs, 'License', None), } release_data = { 'version': pop_key(attrs, 'Version'), 'description': pop_key(attrs, 'Description', description), 'home_page': pop_key(attrs, 'Home-page', None), } file_data = { 'basename': basename, 'attrs': { 'packagetype': 'sdist', 'python_version': 'source', } } deps = [] exts = {} environments = {} for key, val in config_items: if key in ['Requires-Dist', 'Requires']: name, extras, const, marker, url = parse_specification(val) name = norm_package_name(name) specs = const.split(',') new_specs = [] for spec in specs: pos = [i for i, c in enumerate(spec) if c in '0123456789'] if pos: pos = pos[0] comp, spec_ = spec[:pos].strip(), spec[pos:].strip() new_specs.append((comp, spec_)) if marker: if marker.startswith('extra'): marker = marker.replace('extra', '') marker = marker.replace('==', '').strip() ext = marker.rsplit(' ')[-1] if '\"' in ext or \"'\" in ext: ext = ext[1:-1] if ext not in exts: exts[ext] = [{'name': name, 'specs': new_specs}] else: exts[ext].append({'name': name, 'specs': new_specs}) else: if marker not in environments: environments[marker] = [{'name': name, 'specs': new_specs}] else: environments[marker].append({'name': name, 'specs': new_specs}) else: deps.append({ 'name': name, 'specs': new_specs, }) deps.sort(key=lambda o: o['name']) new_exts = [] for key, values in exts.items(): new_exts.append({'name': key, 'depends': values}) new_environments = [] for key, values in environments.items(): new_environments.append({'name': key, 'depends': values}) file_data.update(dependencies={ 'has_dep_errors': False, 'depends': deps, 'extras': new_exts, 'environments': new_environments, }) return package_data, release_data, file_data"}
{"text_id": "9063", "text": "docstring: def _rename_graph_names( self, kw, renamer: Union[Callable[[RenArgs], str], Mapping[str, str]] ) -> None: from .modifier import dep_renamed def with_errors_logged(fn, ren_args: RenArgs) -> str: ok = False try: ret = fn(ren_args) ok = True return ret finally: if not ok: log.warning(\"Failed to rename %s\", ren_args) def rename_driver(ren_args: RenArgs) -> str: new_name = old_name = ren_args.name if isinstance(renamer, cabc.Mapping): dst = renamer.get(old_name) if callable(dst) or (dst and isinstance(dst, str)): new_name = dep_renamed(old_name, dst) elif callable(renamer): dst = renamer(ren_args) if dst and isinstance(dst, str): new_name = dst else: raise AssertionError( f\"Invalid `renamer` {renamer!r} should have been caught earlier.\" ) if (not new_name and old_name) or not isinstance(new_name, str): raise ValueError( f\"Must rename {old_name!r} into a non-empty string, got {new_name!r}!\" ) return new_name def rename_subdocs(ren_args): parts = getattr(ren_args.name, \"_jsonp\", None) if parts: path = \"/\".join( rename_driver( ren_args._replace(typ=ren_args.typ + \".jsonpart\", name=p) ) for p in parts ) ren_args = ren_args._replace(name=dep_renamed(ren_args.name, path)) return rename_driver(ren_args) ren_args = RenArgs(None, self, None) kw[\"name\"] = with_errors_logged( rename_driver, ren_args._replace(typ=\"op\", name=kw.get(\"name\", self.name)) ) ren_args = ren_args._replace(typ=\"need\") kw[\"needs\"] = [ with_errors_logged(rename_subdocs, ren_args._replace(name=n)) for n in kw.get(\"needs\", self.needs) ] ren_args = ren_args._replace(typ=\"provide\") renamed_provides = { n: with_errors_logged(rename_subdocs, ren_args._replace(name=n)) for n in kw.get(\"provides\", self.provides) } kw[\"provides\"] = list(renamed_provides.values()) if hasattr(self, \"aliases\"): ren_args = ren_args._replace(typ=\"alias\") kw[\"aliases\"] = [ ( renamed_provides[k], with_errors_logged(rename_subdocs, ren_args._replace(name=v)), ) for k, v in kw.get(\"aliases\", self.aliases) ]"}
{"text_id": "9064", "text": "docstring: def flush(from_test=False): auto_commit = not from_test if frappe.are_emails_muted(): msgprint(_(\"Emails are muted\")) from_test = True smtpserver_dict = frappe._dict() for email in get_queue(): if cint(frappe.defaults.get_defaults().get(\"hold_queue\"))==1: break if email.name: smtpserver = smtpserver_dict.get(email.sender) if not smtpserver: smtpserver = SMTPServer() smtpserver_dict[email.sender] = smtpserver send_one(email.name, smtpserver, auto_commit, from_test=from_test)"}
{"text_id": "9065", "text": "docstring: def extract_image_server(canvas): url = urlparse(canvas['images'][0]['resource']['service']['@id']) parts = url.path.split('/') parts.pop() base_path = '/'.join(parts) host = url.hostname if url.port is not None: host = '{h}:{p}'.format(h=url.hostname, p=url.port) return '{s}://{h}{p}'.format(s=url.scheme, h=host, p=base_path)"}
{"text_id": "9066", "text": "docstring: def UpdateRecursiveMakeArgs(infile, outfile, suffix): old_includes = ' INCLUDES=' new_includes = ' INCLUDES%s_$$i=' % suffix for line in infile: if '$(MAKE)' in line and old_includes in line: line = line.replace(old_includes, new_includes) print '%s: updated recursive make command line' % infile.name print >>outfile, line,"}
{"text_id": "9067", "text": "docstring: def testDefaultDenyEgressCreation(self): acl = gcp_hf.HierarchicalFirewall( policy.ParsePolicy(HEADER_OPTION_EGRESS + TERM_DENY_EGRESS, self.naming), EXP_INFO) expected = json.loads(EXPECTED_DENY_EGRESS) self.assertEqual(expected, json.loads(self._StripAclHeaders(str(acl))))"}
{"text_id": "9068", "text": "docstring: def list(self): return [v[0] for v in self]"}
{"text_id": "9069", "text": "docstring: def part_one(data: str) -> int: total_paper_amount = 0 for box in data.splitlines(): total_paper_amount += paper_amount(box) return total_paper_amount"}
{"text_id": "9070", "text": "docstring: def GetSelAlpha(*args, **kwargs): return _stc.StyledTextCtrl_GetSelAlpha(*args, **kwargs)"}
{"text_id": "9071", "text": "docstring: def on_timer_2(e: ap.TimerEvent, options: _RectOptions) -> None: rectangle: ap.Rectangle = options['rectangle'] rectangle.fill_color = ap.String('#0af')"}
{"text_id": "9072", "text": "docstring: def lookup_lut(self, point): rhoatm = s.array(self.rhoatm_interp(point)).ravel() sphalb = s.array(self.sphalb_interp(point)).ravel() transm = s.array(self.transm_interp(point)).ravel() transup = s.array(self.transm_interp(point)).ravel() return rhoatm, sphalb, transm, transup"}
{"text_id": "9073", "text": "docstring: def check_item_does_not_exist(context, item): uuids = [] reports = context.get(f\"report/{context.uuid[item]}\") if item == \"report\" else context.get(\"report/\") for report in reports[\"reports\"]: uuids.append(report[\"report_uuid\"]) uuids.extend(report[\"subjects\"].keys()) for subject in report[\"subjects\"].values(): uuids.extend(subject[\"metrics\"].keys()) for metric in subject[\"metrics\"].values(): uuids.extend(metric[\"sources\"].keys()) assert_false(context.uuid[item] in uuids)"}
{"text_id": "9074", "text": "docstring: def _init_module_cache(): if len(FieldTranslation._modules) < len(FieldTranslation._model_module_paths): for module_path in FieldTranslation._model_module_paths: FieldTranslation._modules[module_path] = importlib.import_module(module_path) return True return False"}
{"text_id": "9075", "text": "docstring: def _render_execution_errors(self, formatter): tb_lexer = pygments.lexers.get_lexer_by_name(\"py3tb\", stripall=True) executions = [] for orig_execution in self.eoexecutor.execution_stats: execution = copy.deepcopy(orig_execution) if self.eoexecutor.STATS_ERROR in execution: execution[self.eoexecutor.STATS_ERROR] = pygments.highlight(execution[self.eoexecutor.STATS_ERROR], tb_lexer, formatter) executions.append(execution) return executions"}
{"text_id": "9076", "text": "docstring: def build_probe(self,__placements=None): self.updateparams() if __placements is None: __placements = np.zeros(self.N) __placements[3:10] = np.ones(7) self.probe_design_vector = np.cumsum(__placements) else: self.probe_design_vector = np.cumsum(__placements)"}
{"text_id": "9077", "text": "docstring: def index(request): username = request.session.get('username', False) profile = request.session.get('profile', False) if(username): context = {'username': username,'profile':profile} return render(request, 'MedTAG_sket_dock_App/index.html', context) else: return redirect('MedTAG_sket_dock_App:login')"}
{"text_id": "9078", "text": "docstring: def create(self, model, feature_set): assert model is not None, 'Model cannot be none.' assert feature_set in self.p.featureSets.keys(), 'Feature set unavailable' self.model = model self.mName = type(model).__name__ self.feature_set = feature_set self.prepare_data() self.analyse() path = self.p.mainDir + 'Documentation/v{}/{}_{}.pdf'.format(self.p.version, self.mName, feature_set) if not os.path.exists(path[:path.rfind('/')]): os.makedirs(path[:path.rfind('/')]) self.add_page() self.add_h1('{}'.format(self.name)) self.add_h2('{} - v{}'.format(self.mName, self.p.version)) self.add_text(self.model_description) self.model_performance() self.validation() self.parameters() self.features() self.data() self.score_board() self.output(self.p.mainDir + 'Documentation/v{}/{}_{}.pdf'.format(self.p.version, self.mName, self.feature_set))"}
{"text_id": "9079", "text": "docstring: def _from_openapi_data(cls, *args, **kwargs): \"\"\"MediationGrant - a model defined in OpenAPI Keyword Args: _check_type (bool): if True, values for parameters in openapi_types will be type checked and a TypeError will be raised if the wrong type is input. Defaults to True _path_to_item (tuple/list): This is a list of keys or values to drill down to the model in received_data when deserializing a response _spec_property_naming (bool): True if the variable names in the input data are serialized names, as specified in the OpenAPI document. False if the variable names in the input data are pythonic names, e.g. snake case (default) _configuration (Configuration): the instance to use when deserializing a file_type parameter. If passed, type conversion is attempted If omitted no type conversion is done. _visited_composed_classes (tuple): This stores a tuple of classes that we have traveled through so that if we see that class again we will not use its discriminator again. When traveling through a discriminator, the composed schema that is is traveled through is added to this set. For example if Animal has a discriminator petType and we pass in \"Dog\", and the class Dog allOf includes Animal, we move through Animal once using the discriminator, and pick Dog. Then in Dog, we will make an instance of the Animal class but this time we won't travel through its discriminator because we passed in _visited_composed_classes = (Animal,) id (str): Message identifier. [optional] type (str): Message type. [optional] endpoint (str): endpoint on which messages destined for the recipient are received.. [optional] routing_keys ([str]): [optional] \"\"\" _check_type = kwargs.pop(\"_check_type\", True) _spec_property_naming = kwargs.pop(\"_spec_property_naming\", False) _path_to_item = kwargs.pop(\"_path_to_item\", ()) _configuration = kwargs.pop(\"_configuration\", None) _visited_composed_classes = kwargs.pop(\"_visited_composed_classes\", ()) self = super(OpenApiModel, cls).__new__(cls) if args: raise ApiTypeError( \"Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments.\" % ( args, self.__class__.__name__, ), path_to_item=_path_to_item, valid_classes=(self.__class__,), ) self._data_store = {} self._check_type = _check_type self._spec_property_naming = _spec_property_naming self._path_to_item = _path_to_item self._configuration = _configuration self._visited_composed_classes = _visited_composed_classes + (self.__class__,) for var_name, var_value in kwargs.items(): if ( var_name not in self.attribute_map and self._configuration is not None and self._configuration.discard_unknown_keys and self.additional_properties_type is None ): continue setattr(self, var_name, var_value) return self"}
{"text_id": "9080", "text": "docstring: def delete(self, key): key = self._validate_key(key) command = 'delete %s\\r\\n' % key resp = self._send_command(command) if resp != 'DELETED\\r\\n' and resp != 'NOT_FOUND\\r\\n': raise ClientException('delete failed', resp)"}
{"text_id": "9081", "text": "docstring: def nom_size(self): return XY2D(y=float(self.meta['Length y[A]']), x=float(self.meta['Length x[A]']))"}
{"text_id": "9082", "text": "docstring: def _validate_chemistry_and_plasma_parameters(self): validate_chemistry(self.chemistry) validate_plasma_parameters(self.plasma_params) if not set(self.plasma_params.feeds).issubset(self.chemistry.species_ids): raise PlasmaParametersValidationError( \"Feed gas species defined in the plasma parameters are inconsistent \" \"with the chemistry species ids!\" )"}
{"text_id": "9083", "text": "docstring: def unflatten(dictionary: dict, sep: str = '.', recursive: bool = False, levels=None): duplicate_key_warning_str = (\"Duplicate key detected in recursive dictionary unflattening. \" \"Overwriting previous entries of '{}'.\") if levels is not None: if not isinstance(levels, tuple) and not isinstance(levels, list): levels = [levels] if len(levels) == 0: raise ValueError(\"Need at least one level to unflatten when levels != None.\") if not isinstance(levels[0], int): raise TypeError(f\"Levels must be list or set of integers, got type {type(levels[0])}.\") result_dict = dict() for key, value in dictionary.items(): if isinstance(value, dict) and recursive: value = unflatten(value, sep=sep, recursive=True, levels=levels) parts = key.split(sep) if levels is not None: key_levels = levels.copy() for ix in range(len(key_levels)): if key_levels[ix] < 0: new_ix = len(parts) + key_levels[ix] - 1 if key_levels[ix] == -1: new_ix = max(0, new_ix) if new_ix < 0: raise IndexError(f\"Dictionary key level out of bounds. ({new_ix} < 0).\") key_levels[ix] = new_ix if key_levels[ix] >= len(parts): raise IndexError(f\"Dictionary key level {key_levels[ix]} out of bounds for size {len(parts)}.\") key_levels = sorted(key_levels) key_levels = list(set(key_levels)) new_parts = [] ix_current = 0 for l in key_levels: new_parts.append(sep.join(parts[ix_current:l+1])) ix_current = l+1 if ix_current < len(parts): new_parts.append(sep.join(parts[ix_current::])) parts = new_parts d = result_dict for part in parts[:-1]: if part not in d: d[part] = dict() elif not isinstance(d[part], dict): logging.warning(duplicate_key_warning_str.format(part)) d[part] = dict() d = d[part] last_key = parts[-1] if last_key in d: if isinstance(value, dict): intersection = set(d[last_key].keys()).intersection(value.keys()) if len(intersection) > 0: logging.warning(duplicate_key_warning_str.format(last_key)) d[last_key] = merge_dicts(d[last_key], value) else: logging.warning(duplicate_key_warning_str.format(last_key)) d[last_key] = value else: d[last_key] = value return result_dict"}
{"text_id": "9084", "text": "docstring: def kb_pretty_print(b): r = [] v = 0 for l, n in StatCollector.BYTE_MAP: if b > n: v = round(float(b) / n, 1) r.append(str(v) + l) break if len(r) == 0: return '{0}KB'.format(str(b)) else: return ' '.join(r)"}
{"text_id": "9085", "text": "docstring: def forward(self, img): fill = self.fill if isinstance(img, Tensor): if isinstance(fill, (int, float)): fill = [float(fill)] * F.get_image_num_channels(img) else: fill = [float(f) for f in fill] img_size = F.get_image_size(img) ret = self.get_params(self.degrees, self.translate, self.scale, self.shear, img_size) return F.affine(img, *ret, interpolation=self.interpolation, fill=fill)"}
{"text_id": "9086", "text": "docstring: def finalize(self): if self.timer: self.afterTimerCallback() if self.triggerAtExit and self.foundEvent: self.callback(self.foundEvent)"}
{"text_id": "9087", "text": "docstring: def _get_magic_mock_file_results_location(self, file_location: str) -> str: return f\"{file_location}.results\""}
{"text_id": "9088", "text": "docstring: def helixIndex(self, point: QPointF) -> Vec2T: x = int(int(point.x()) / _BW) y = int(int(point.y()) / _BW) return (x, y)"}
{"text_id": "9089", "text": "docstring: def _ResolveRefs(self, deps_list): results = collections.OrderedDict() for submods in deps_list: for name, data in sorted(submods.iteritems()): url = data.url revision = data.revision if not revision: revision = 'master' if not SHA1_RE.match(revision): sha1 = self._Resolve(url, revision) data = SubmodData(url=url, revision=sha1) results.pop(name, None) results[name] = data return results"}
{"text_id": "9090", "text": "docstring: def configure_ssl_off(units, model_name=None, max_wait=60): logging.debug('Setting ssl charm config option: off') config = {'ssl': 'off'} zaza.model.set_application_config('rabbitmq-server', config, model_name=model_name) wait_for_cluster(model_name) ret = _retry_validate_ssl_disabled_units(units) if ret: raise Exception(ret)"}
{"text_id": "9091", "text": "docstring: def col_values_pairs(self, df, features): df_features= df[features] df_describe= df_features.describe() numnerical_col= df_describe.columns categorical_col= list(set(features)- set(numnerical_col)) col_type_dict={} col_value_dict={} for f in numnerical_col: col_type_dict[f]=\"slider\" col_value_dict[f]= self.get_min_max_featurs(df_describe, f) for f in categorical_col: categ,col_type= all_categories(df, f) col_type_dict[f]= col_type col_value_dict[f]= categ return col_value_dict, col_type_dict"}
{"text_id": "9092", "text": "docstring: def from_txid(cls, hdwallet, txid): sess = hdwallet._session db_tx_query = sess.query(DbTransaction). \\ filter(DbTransaction.wallet_id == hdwallet.wallet_id, DbTransaction.txid == to_bytes(txid)) db_tx = db_tx_query.scalar() if not db_tx: return fee_per_kb = None if db_tx.fee and db_tx.size: fee_per_kb = int((db_tx.fee / db_tx.size) * 1024) network = Network(db_tx.network_name) inputs = [] for inp in db_tx.inputs: sequence = 0xffffffff if inp.sequence: sequence = inp.sequence inp_keys = [] if inp.key_id: key = hdwallet.key(inp.key_id) if key.key_type == 'multisig': db_key = sess.query(DbKey).filter_by(id=key.key_id).scalar() for ck in db_key.multisig_children: inp_keys.append(ck.child_key.public.hex()) else: inp_keys = key.key() inputs.append(Input( prev_txid=inp.prev_txid, output_n=inp.output_n, keys=inp_keys, unlocking_script=inp.script, script_type=inp.script_type, sequence=sequence, index_n=inp.index_n, value=inp.value, double_spend=inp.double_spend, witness_type=inp.witness_type, network=network, address=inp.address)) outputs = [] for out in db_tx.outputs: address = '' public_key = b'' if out.key_id: key = hdwallet.key(out.key_id) address = key.address if key.key_type != 'multisig': if key.key() and not isinstance(key.key(), Address): public_key = key.key().public_hex outputs.append(Output(value=out.value, address=address, public_key=public_key, lock_script=out.script, spent=out.spent, output_n=out.output_n, script_type=out.script_type, network=network)) return cls(hdwallet=hdwallet, inputs=inputs, outputs=outputs, locktime=db_tx.locktime, version=db_tx.version, network=network, fee=db_tx.fee, fee_per_kb=fee_per_kb, size=db_tx.size, txid=to_hexstring(txid), date=db_tx.date, confirmations=db_tx.confirmations, block_height=db_tx.block_height, input_total=db_tx.input_total, output_total=db_tx.output_total, rawtx=db_tx.raw, status=db_tx.status, coinbase=db_tx.coinbase, verified=db_tx.verified)"}
{"text_id": "9093", "text": "docstring: def detect_red_light_random(I): bounding_boxes = [] data_path = 'data/kernel' idx = np.random.randint(172) kernel = Image.open(os.path.join(data_path,'kernel'+str(idx+1)+'.jpg')) kernel = np.asarray(kernel) box_height = kernel.shape[0] box_width = kernel.shape[1] r_kernel = kernel[:,:,0] r_kernel = r_kernel / np.linalg.norm(r_kernel) r_I = I[:,:,0] threshold = 0.9 for row in range(r_I.shape[0]-box_height): for col in range(r_I.shape[1]-box_width): r_I_part = r_I[row:row+box_height,col:col+box_width] r_I_part = r_I_part / np.linalg.norm(r_I_part) convolution = np.sum(np.multiply(r_kernel,r_I_part)) if convolution > threshold and (not bounding_boxes or (row > tl_row + 5 and col > tl_col+5)): tl_row = row tl_col = col br_row = tl_row + box_height br_col = tl_col + box_width bounding_boxes.append([tl_row,tl_col,br_row,br_col]) for i in range(len(bounding_boxes)): assert len(bounding_boxes[i]) == 4 return bounding_boxes"}
{"text_id": "9094", "text": "docstring: def _add_empty_public_partitions(self, col, public_partitions, aggregator_fn): self._add_report_stage( \"Adding empty partitions to public partitions that are missing in \" \"data\") empty_accumulators = self._ops.map( public_partitions, lambda partition_key: (partition_key, aggregator_fn([])), \"Build empty accumulators\") return self._ops.flatten( col, empty_accumulators, \"Join public partitions with partitions from data\")"}
{"text_id": "9095", "text": "docstring: def require_modules(modnames): import importlib import pytest for modname in modnames: try: importlib.import_module(modname) except (ImportError, ModuleNotFoundError): pytest.skip(\"test requires {}\".format(modname))"}
{"text_id": "9096", "text": "docstring: def _flushOutputs(self): if self.outputs is None: return while len(self.outputs) > 1: output, timeout = self.outputs.pop(0) timeout.cancel() output([self.outgoingAck, []])"}
{"text_id": "9097", "text": "docstring: def glob_ext(self): if self.all_paths: return self.path = self.path.glob_path_withext() self.raw_rule = None"}
{"text_id": "9098", "text": "docstring: def fig_work_above_threshold( scores: np.ndarray, objs: list, threshold: float, theta: float, n_sensors: int, work_name: str, save_dir: Path, ): work_idx = objs.index(work_name) if (scores[:, work_idx] < threshold).all(): print(f\"No networks with workplace coverage > {threshold}, skipping figure\") return fig, ax = plt.subplots(1, 1, figsize=(10, 5)) networks_swarmplot(scores, objs, thresholds={work_name: threshold}, ax=ax) save_fig( fig, f\"multiobj_theta{theta}_{n_sensors}sensors_workabove{round(threshold * 100)}cov.png\", save_dir, )"}
{"text_id": "9099", "text": "docstring: def energy_power_factor(self) -> \"TasmotaEnergyFactorConnector\": return self._get_or_create_connector(TasmotaEnergyFactorConnector)"}
{"text_id": "9100", "text": "docstring: def translations(self): return Translations(self)"}
{"text_id": "9101", "text": "docstring: def fill_list(files, queried=False, custom_order=False): global cached_filenames, cached_timestamps, cached_data if custom_order: queried = True files = filter(exists, [join(get_save_dir(), f) for f in files]) timestamps = [getmtime(join(get_save_dir(), f)) for f in files] if queried or files != cached_filenames or timestamps != cached_timestamps: lines = [] if not custom_order: files = reversed(sorted(files, key=lambda i: getmtime(join(get_save_dir(), i)))) for pad in files: pad_path = join(get_save_dir(), pad) if isfile(pad_path): pad_path = join(get_save_dir(), pad) with open(pad_path) as pad_file: info = PadInfo(pad_file) if info.isEmpty: if bool(int(vim.eval(\"g:pad#show_dir\"))): tail = info.folder + u'\\u2e25 '.encode('utf-8') + \"[EMPTY]\" else: tail = \"[EMPTY]\" else: if bool(int(vim.eval(\"g:pad#show_dir\"))): tail = info.folder + u'\\u2e25 '.encode('utf-8') + u'\\u21b2'.encode('utf-8').join((info.summary, info.body)) else: tail = u'\\u21b2'.encode('utf-8').join((info.summary, info.body)) lines.append(pad + \" @ \" + tail) else: pass if not queried: cached_data = lines cached_timestamps = timestamps cached_filenames = files def add_natural_timestamp(matchobj): id_string = matchobj.group(\"id\") mtime = str(int(getmtime(join(get_save_dir(), matchobj.group(\"id\")))*1000000)) return id_string + \" @ \" + natural_timestamp(mtime).ljust(19) + \" \u2502\" if not queried: lines = [re.sub(\"(?P<id>^.*?) @\", add_natural_timestamp, line) for line in cached_data] else: lines = [re.sub(\"(?P<id>^.*?) @\", add_natural_timestamp, line) for line in lines] if vim.eval('&modifiable') != '1': vim.current.buffer.options['modifiable'] = True del vim.current.buffer[:] vim.current.buffer.append(list(lines)) vim.command(\"normal! dd\")"}
{"text_id": "9102", "text": "docstring: def bgp_table_parser(bgp_table): bgp_table = bgp_table.strip() for bgp_entry in bgp_table.splitlines(): bgp_table_fields = bgp_entry.split() try: if re.search(r\"Shut.*Admin\", bgp_entry): ( peer_ip, bgp_version, remote_as, msg_rcvd, msg_sent, _, _, _, uptime, state_1, state_2, ) = bgp_table_fields state_pfxrcd = \"{} {}\".format(state_1, state_2) else: ( peer_ip, bgp_version, remote_as, msg_rcvd, msg_sent, _, _, _, uptime, state_pfxrcd, ) = bgp_table_fields except ValueError: raise ValueError( \"Unexpected entry ({}) in BGP summary table\".format(bgp_table_fields) ) is_enabled = True try: received_prefixes = int(state_pfxrcd) is_up = True except ValueError: received_prefixes = -1 is_up = False if re.search(r\"Shut.*Admin\", state_pfxrcd): is_enabled = False if not is_up: uptime = -1 if uptime != -1: uptime = bgp_time_conversion(uptime) yield { peer_ip: { \"is_enabled\": is_enabled, \"uptime\": uptime, \"remote_as\": helpers.as_number(remote_as), \"is_up\": is_up, \"description\": \"\", \"received_prefixes\": received_prefixes, } }"}
{"text_id": "9103", "text": "docstring: def _update_host(self, session, hostname, filename): env = EnvironmentEntity(uuid=self.run.environment_uuid) configdata = self.run.get_object(filename).get('data', {}) host = HostEntity(hostname=hostname, environment=env.identity) host = HostEntity.find(session, host.identity) if host is None: logger.warning('Unable to locate host {}'.format(hostname)) return configfiles = [] for filename, metadata in configdata.items(): _, name = os.path.split(filename) configfile = ConfigfileEntity( path=filename, host=host.identity, md5=metadata.get('md5'), contents=metadata.get('contents'), is_binary=metadata.get('is_binary'), name=name ) configfile.update(session, self.time_in_ms) configfiles.append(configfile) host.configfiles.update(session, configfiles, self.time_in_ms)"}
{"text_id": "9104", "text": "docstring: def testUpdateSharedFields(self): s3db = current.s3db auth = current.auth ftable = s3db.org_office stable = s3db.org_site assertEqual = self.assertEqual row = ftable[self.office_id] row.update_record(realm_entity=row[\"pe_id\"]) site_id = row[\"site_id\"] auth.update_shared_fields(ftable, self.office_id, realm_entity=None) site = stable[site_id] assertEqual(site[\"realm_entity\"], None) auth.update_shared_fields(ftable, self.office_id, realm_entity=row[\"realm_entity\"]) site = stable[site_id] assertEqual(site[\"realm_entity\"], row[\"realm_entity\"])"}
{"text_id": "9105", "text": "docstring: def recv_video_frame_ndarray_with_stats( self, stat_window_size: int = 210, draw_stats: Optional[ str ] = \"{width}x{height} {kilobytes_per_second} kB/s {frames_per_second} FPS\", ) -> Iterator[Tuple[\"np.ndarray\", tutk.FrameInfoStruct, Dict[str, int]]]: stat_window = [] for frame_ndarray, frame_info in self.recv_video_frame_ndarray(): stat_window.append(frame_info) if len(stat_window) > stat_window_size: stat_window = stat_window[len(stat_window) - stat_window_size :] if len(stat_window) > 1: stat_window_start = ( stat_window[0].timestamp + stat_window[0].timestamp_ms / 1_000_000 ) stat_window_end = ( stat_window[-1].timestamp + stat_window[-1].timestamp_ms / 1_000_000 ) stat_window_duration = stat_window_end - stat_window_start stat_window_total_size = sum( b.frame_len for b in stat_window[:-1] ) bytes_per_second = int( stat_window_total_size / stat_window_duration ) frames_per_second = int(len(stat_window) / stat_window_duration) else: bytes_per_second = 0 stat_window_duration = 0 frames_per_second = 0 stats = { \"bytes_per_second\": bytes_per_second, \"kilobytes_per_second\": int(bytes_per_second / 1000), \"window_duration\": stat_window_duration, \"frames_per_second\": frames_per_second, \"width\": frame_ndarray.shape[1], \"height\": frame_ndarray.shape[0], } if draw_stats: text = draw_stats.format(**stats) cv2.putText( frame_ndarray, text, (50, 50), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA, ) cv2.putText( frame_ndarray, text, (50, 50), cv2.FONT_HERSHEY_DUPLEX, 1, (255, 255, 255), 1, cv2.LINE_AA, ) yield frame_ndarray, frame_info, stats"}
{"text_id": "9106", "text": "docstring: def unbuckle_uid(self) -> str: if self.__buckled: return self.__sequence.metadata[\"id\"] else: raise RuntimeError(\"Nucleotide instance is not buckled.\")"}
{"text_id": "9107", "text": "docstring: def radec2altaz(ra, dec, obstime, lat=None, lon=None, debug=False): if lat is None: lat = obstime.lat.degree if lon is None: lon = obstime.lon.degree obstime = Time(obstime.isot, format='isot', scale='utc', location=(lon, lat)) j2000 = Time(\"2000-01-01T12:00:00.0\", format='isot', scale='utc') dt = (obstime - j2000).value tstring = obstime.isot.split(\"T\")[-1] segments = tstring.split(\":\") ut = float(segments[0]) + float(segments[1]) / 60.0 + float(segments[2]) / 3600 lst = obstime.sidereal_time('mean').deg HA = lst - ra while HA < 0.0 or HA > 360.0: s = -np.sign(HA) HA += s * 360.0 dec *= np.pi / 180.0 lat *= np.pi / 180.0 HA *= np.pi / 180.0 alt = np.arcsin(np.sin(dec) * np.sin(lat) + np.cos(dec) * np.cos(lat) * np.cos(HA)) az = np.arccos((np.sin(dec) - np.sin(alt) * np.sin(lat)) / (np.cos(alt) * np.cos(lat))) if np.sin(HA) > 0: az = 2.0 * np.pi - az if debug: print( \"UT: \", ut) print( \"LST: \", lst) print( \"HA: \", HA * 180.0 / np.pi) return alt * 180.0 / np.pi, az * 180.0 / np.pi"}
{"text_id": "9108", "text": "docstring: def _sales(self): data = self.data.get_row_list('RawImport', 'imported_sale = false and imported_customer = true and BalanceImpact = \\'Credit\\'') if data is None: showinfo('INFO', 'There are no sales transcations to import.') return 0 count = 0 for item in data: if item['Name'] != '' and item['Name'] != 'PayPal': rec = { 'date': item['Date'], 'customer_ID': self.data.get_id_by_row('Customer', 'name', item['Name']), 'raw_import_ID': int(item['ID']), 'status_ID': self.data.get_id_by_row('SaleStatus', 'name', 'complete'), 'transaction_uuid': item['TransactionID'], 'gross': self.data.convert_value(item['Gross'], float), 'fees': self.data.convert_value(item['Fee'], float), 'shipping': self.data.convert_value(item['Shipping'], float), 'notes': item['Subject'] + '\\n' +item['ItemTitle'], 'committed': False} self.data.insert_row('SaleRecord', rec) count+=1 self.data.update_row_by_id('RawImport', {'imported_sale':True}, item['ID']) self.data.commit() return count"}
{"text_id": "9109", "text": "docstring: def parse_version(ver): m = RE_VER.match(ver) if m is None: raise ValueError(\"'{}' is not a valid version\".format(ver)) major = int(m.group('major')) minor = int(m.group('minor')) if m.group('minor') else 0 micro = int(m.group('micro')) if m.group('micro') else 0 if m.group('type'): release = PRE_REL_MAP[m.group('type')] pre = int(m.group('pre')) else: release = \"final\" pre = 0 dev = m.group('dev') if m.group('dev') else 0 if m.group('dev'): dev = int(m.group('dev')) release = '.dev-' + release if pre else '.dev' else: dev = 0 post = int(m.group('post')) if m.group('post') else 0 return Version(major, minor, micro, release, pre, post, dev)"}
{"text_id": "9110", "text": "docstring: def drawToString(d, showBoundary=rl_config.showBoundary): s = getStringIO() drawToFile(d, s, showBoundary=showBoundary) return s.getvalue()"}
{"text_id": "9111", "text": "docstring: def _RecvLoop(self): while self.isActive: try: sz, = unpack('!i', self._socket.readAll(4)) with self._varz.recv_time.Measure(): with self._varz.recv_latency.Measure(): buf = BytesIO(self._socket.readAll(sz)) self._varz.messages_recv() gevent.spawn(self._ProcessReply, buf) except Exception as e: self._Shutdown(e) break"}
{"text_id": "9112", "text": "docstring: async def async_set_wifi_led_off(self): if self._device_features & FEATURE_SET_WIFI_LED == 0: return await self._try_command( \"Turning the wifi led off failed.\", self._plug.set_wifi_led, False )"}
{"text_id": "9113", "text": "docstring: def predict(self, labels, image=None): import pyclesperanto_prototype as cle labels = cle.push(labels) selected_features, gt = self._make_features(self.classifier.feature_specification, labels, None, image) output = cle.create_like(selected_features[0].shape) parameters = {} for i, f in enumerate(selected_features): parameters['in' + str(i)] = cle.push(f) parameters['out'] = output cle.execute(None, self.classifier.opencl_file, \"predict\", selected_features[0].shape, parameters) cle.set_column(output, 0, 0) result_labels = cle.create_labels_like(labels) cle.replace_intensities(labels, output, result_labels) return result_labels"}
{"text_id": "9114", "text": "docstring: def make_pretty_money(amount): amount = Decimal(amount) amount = amount.quantize(Decimal(\"1\"), rounding=ROUND_HALF_UP) amount = \"${:,.0f}\".format(amount) return amount"}
{"text_id": "9115", "text": "docstring: def mean(values, weights=None, metrics_collections=None, updates_collections=None, name=None): if context.executing_eagerly(): raise RuntimeError('tf.metrics.mean is not supported when eager execution ' 'is enabled.') with variable_scope.variable_scope(name, 'mean', (values, weights)): values = math_ops.cast(values, dtypes.float32) total = metric_variable([], dtypes.float32, name='total') count = metric_variable([], dtypes.float32, name='count') if weights is None: num_values = math_ops.cast(array_ops.size(values), dtypes.float32) else: values, _, weights = _remove_squeezable_dimensions( predictions=values, labels=None, weights=weights) weights = weights_broadcast_ops.broadcast_weights( math_ops.cast(weights, dtypes.float32), values) values = math_ops.multiply(values, weights) num_values = math_ops.reduce_sum(weights) update_total_op = state_ops.assign_add(total, math_ops.reduce_sum(values)) with ops.control_dependencies([values]): update_count_op = state_ops.assign_add(count, num_values) def compute_mean(_, t, c): return math_ops.div_no_nan(t, math_ops.maximum(c, 0), name='value') mean_t = _aggregate_across_replicas( metrics_collections, compute_mean, total, count) update_op = math_ops.div_no_nan( update_total_op, math_ops.maximum(update_count_op, 0), name='update_op') if updates_collections: ops.add_to_collections(updates_collections, update_op) return mean_t, update_op"}
{"text_id": "9116", "text": "docstring: def apply(self, example, is_train=False, stats=None, **kwargs): if is_train: example['src'] = self._token_drop(example['src'], stats) example['tgt'] = self._token_drop(example['tgt'], stats) return example"}
{"text_id": "9117", "text": "docstring: def state(self): if self.rest.data and self._condition in self.rest.data: if self._condition == 'relative_humidity': return int(self.rest.data[self._condition][:-1]) else: return self.rest.data[self._condition] if self._condition == 'alerts': if self.rest.alerts: return len(self.rest.alerts) else: return 0 return STATE_UNKNOWN"}
{"text_id": "9118", "text": "docstring: def any(self): ret = self._spv.Reduce(pcb.logical_or(), pcb.ifthenelse(pcb.bind2nd(pcb.not_equal_to(),0), pcb.set(1), pcb.set(0))) == 1 return ret"}
{"text_id": "9119", "text": "docstring: def create_optimizer(algorithm, dim, link_lengths, seed): max_bound = np.sum(link_lengths) bounds = [(-max_bound, max_bound), (-max_bound, max_bound)] initial_sol = np.zeros(dim) batch_size = 36 num_emitters = 1 if algorithm in [\"og_map_elites_ind\", \"og_map_elites_line_ind\"]: num_emitters = 2 if algorithm in [ \"map_elites\", \"map_elites_line\", \"cma_me_imp\", \"og_map_elites\", \"og_map_elites_line\", \"og_map_elites_ind\", \"og_map_elites_line_ind\", \"omg_mega\", \"cma_mega\", \"cma_mega_adam\", ]: archive = GridArchive((100, 100), bounds, seed=seed) else: raise ValueError(f\"Algorithm `{algorithm}` is not recognized\") emitter_seeds = [None] * num_emitters if seed is None else list( range(seed, seed + num_emitters)) if algorithm in [\"map_elites\"]: emitters = [ GaussianEmitter(archive, initial_sol, 0.1, batch_size=batch_size, seed=s) for s in emitter_seeds ] elif algorithm in [\"map_elites_line\"]: emitters = [ IsoLineEmitter(archive, initial_sol, iso_sigma=0.1, line_sigma=0.2, batch_size=batch_size, seed=s) for s in emitter_seeds ] elif algorithm in [\"og_map_elites\"]: emitters = [ GradientEmitter(archive, initial_sol, sigma0=0.1, sigma_g=100.0, measure_gradients=False, normalize_gradients=False, bounds=None, batch_size=batch_size // 2, seed=s) for s in emitter_seeds ] elif algorithm in [\"og_map_elites_ind\"]: emitters = [ GradientEmitter(archive, initial_sol, sigma0=0.0, sigma_g=100.0, measure_gradients=False, normalize_gradients=False, bounds=None, batch_size=batch_size // 3, seed=emitter_seeds[0]) ] + [ GaussianEmitter(archive, initial_sol, 0.1, batch_size=batch_size // 3, seed=emitter_seeds[1]) ] elif algorithm in [\"og_map_elites_line\"]: emitters = [ GradientEmitter(archive, initial_sol, sigma0=0.1, sigma_g=100.0, line_sigma=0.2, operator_type='isolinedd', measure_gradients=False, normalize_gradients=False, bounds=None, batch_size=batch_size // 2, seed=s) for s in emitter_seeds ] elif algorithm in [\"og_map_elites_line_ind\"]: emitters = [ GradientEmitter(archive, initial_sol, sigma0=0.0, sigma_g=100.0, measure_gradients=False, normalize_gradients=False, bounds=None, batch_size=batch_size // 3, seed=emitter_seeds[0]) ] + [ IsoLineEmitter(archive, initial_sol, iso_sigma=0.1, line_sigma=0.2, batch_size=batch_size // 3, seed=emitter_seeds[1]) ] elif algorithm in [\"omg_mega\"]: emitters = [ GradientEmitter(archive, initial_sol, sigma0=0.0, sigma_g=1.0, measure_gradients=True, normalize_gradients=True, bounds=None, batch_size=batch_size // 2, seed=s) for s in emitter_seeds ] elif algorithm in [\"cma_mega\"]: emitters = [ GradientImprovementEmitter(archive, initial_sol, sigma_g=0.05, stepsize=1.0, gradient_optimizer=\"gradient_ascent\", normalize_gradients=True, selection_rule=\"mu\", bounds=None, batch_size=batch_size - 1, seed=s) for s in emitter_seeds ] elif algorithm in [\"cma_mega_adam\"]: emitters = [ GradientImprovementEmitter(archive, initial_sol, sigma_g=0.05, stepsize=0.002, gradient_optimizer=\"adam\", normalize_gradients=True, selection_rule=\"mu\", bounds=None, batch_size=batch_size - 1, seed=s) for s in emitter_seeds ] elif algorithm in [\"cma_me_imp\"]: emitters = [ ImprovementEmitter(archive, initial_sol, 0.1, batch_size=batch_size, seed=s) for s in emitter_seeds ] return Optimizer(archive, emitters)"}
{"text_id": "9120", "text": "docstring: def finish_stroke(): pass"}
{"text_id": "9121", "text": "docstring: def humidity(self): self._perform_reading() temp_scaled = ((self._t_fine * 5) + 128) / 256 var1 = (self._adc_hum - (self._humidity_calibration[0] * 16)) - ( (temp_scaled * self._humidity_calibration[2]) / 200 ) var2 = ( self._humidity_calibration[1] * ( ((temp_scaled * self._humidity_calibration[3]) / 100) + ( ( ( temp_scaled * ((temp_scaled * self._humidity_calibration[4]) / 100) ) / 64 ) / 100 ) + 16384 ) ) / 1024 var3 = var1 * var2 var4 = self._humidity_calibration[5] * 128 var4 = (var4 + ((temp_scaled * self._humidity_calibration[6]) / 100)) / 16 var5 = ((var3 / 16384) * (var3 / 16384)) / 1024 var6 = (var4 * var5) / 2 calc_hum = (((var3 + var6) / 1024) * 1000) / 4096 calc_hum /= 1000 if calc_hum > 100: calc_hum = 100 if calc_hum < 0: calc_hum = 0 return calc_hum"}
{"text_id": "9122", "text": "docstring: def _getSupportedCiphers(): supportedCiphers = [] cs = [b'aes256-ctr', b'aes256-cbc', b'aes192-ctr', b'aes192-cbc', b'aes128-ctr', b'aes128-cbc', b'cast128-ctr', b'cast128-cbc', b'blowfish-ctr', b'blowfish-cbc', b'3des-ctr', b'3des-cbc'] for cipher in cs: algorithmClass, keySize, modeClass = SSHCiphers.cipherMap[cipher] try: Cipher( algorithmClass(b' ' * keySize), modeClass(b' ' * (algorithmClass.block_size // 8)), backend=default_backend(), ).encryptor() except UnsupportedAlgorithm: pass else: supportedCiphers.append(cipher) return supportedCiphers"}
{"text_id": "9123", "text": "docstring: def vecsym(cls, v): D,P = v.data.shape[:2] Nv = v.data[0,0].size tmp = numpy.sqrt(1 + 8*Nv) if abs(int(tmp) - tmp) > 1e-16: raise ValueError('size of v does not match any possible symmetric matrix') N = (int(tmp) - 1)//2 A = cls(numpy.zeros((D,P,N,N))) count = 0 for row in range(N): for col in range(row,N): A[row,col] = A[col,row] = v[count] count +=1 return A"}
{"text_id": "9124", "text": "docstring: def add_to_report(report_message, file_provided=None): report_file = \"\\n\\n---\\n\" + report_message if file_provided: file_provided = str(file_provided) if file_provided.endswith('.xml') or file_provided.endswith('.json'): report_file = '' else: report_file = 'File: ' + file_provided + '\\n\\n' + report_file report_message = '\ud83d\uddd1 File: ' + file_provided + ' - ' + report_message if report_file: report_file = \"\\n\\n---\\n\" + report_file logging.info('[' + datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\") + '] ' + report_message.replace('\\n', '')) with open(root / '../REPORT.md', 'a') as f: f.write(report_file)"}
{"text_id": "9125", "text": "docstring: def _get_synset_urls() -> Tuple[Dict[str, Optional[List[str]]], Dict[str, str]]: synsets, synset_mapping = parse_synset_mapping(SYNSET_MAPPING_FILENAME) synsets_to_urls = download_image_urls(URLS_FILENAME, synsets) return synsets_to_urls, synset_mapping"}
{"text_id": "9126", "text": "docstring: def from_spec(cls, name, spec): if isinstance(spec, cls): return spec elif isinstance(spec, dict): parents = spec.get('parents', []) if isinstance(parents, string_types): parents = [parents] role = cls(name, grants=spec.get('grants', []), parents=parents) else: role = cls(name, grants=spec) return role"}
{"text_id": "9127", "text": "docstring: def check_deepcopy(classifier): classifier_copy = deepcopy(classifier) assert type(classifier) == type(classifier_copy) assert set(classifier.get_params().keys()) == set(classifier_copy.get_params().keys())"}
{"text_id": "9128", "text": "docstring: def rx_check_expected(self, exp): if not self.validSerialDev: print( r\"[ERROR] No valid serial device.\" ) return False resp = self.ser.read_until(exp) if resp == exp: return True else: print(r\"[ERROR] resp: {} != exp: {}\".format( resp, exp )) return False"}
{"text_id": "9129", "text": "docstring: def all_names (object): def do_object (object, names): if inspect.ismodule (object): do_module (object, names) elif inspect.isclass (object): do_class (object, names) elif hasattr (object, '__class__'): names.add ('__class__') do_class (object.__class__, names) return names def do_module (module, names): if hasattr (module, '__all__'): names.update(module.__all__) for i in module.__all__: do_object (getattr (module, i), names) else: names.update(dir (module)) for i in dir (module): do_object (getattr (module, i), names) return names def do_class (object, names): ns = dir (object) names.update(ns) if hasattr (object, '__bases__'): for i in object.__bases__: do_object (i, names) return names return do_object (object, set([]))"}
{"text_id": "9130", "text": "docstring: def train(self, X, X_prime, y, alg=\"fict\"): if alg == \"fict\": err, fairness_violations = self.fictitious_play(X, X_prime, y) return err, fairness_violations else: raise Exception(\"Specified algorithm is invalid\")"}
{"text_id": "9131", "text": "docstring: def log_probability(value, mean, variance): a=log( 1 / (sqrt(2 * variance**2 *kPI))) b = -((value-mean)**2 / 2* variance**2) c=a+b return c"}
{"text_id": "9132", "text": "docstring: def _validate_gift_cards(checkout: Checkout): if ( not checkout.gift_cards.count() == checkout.gift_cards.active(date=date.today()).count() ): msg = \"Gift card has expired. Order placement cancelled.\" raise NotApplicable(msg)"}
{"text_id": "9133", "text": "docstring: def main(argv): args = parseCommandLine(argv) if args.source == 'cam': video = cv2.VideoCapture(args.id) if not video.isOpened(): print('Error opening webcam of id {}'.format(args.id)) sys.exit(-1) fps = 0 frameCount = 0 sourceName = 'Webcam #{}'.format(args.id) else: video = cv2.VideoCapture(args.file) if not video.isOpened(): print('Error opening video file {}'.format(args.file)) sys.exit(-1) fps = int(video.get(cv2.CAP_PROP_FPS)) frameCount = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) sourceName = args.file video.set(cv2.CAP_PROP_FRAME_WIDTH, 1280); video.set(cv2.CAP_PROP_FRAME_HEIGHT, 720); data = VideoData() font = cv2.FONT_HERSHEY_SIMPLEX scale = 1 thick = 1 glow = 3 * thick color = (255, 255, 255) paused = False frameNum = 0 while True: if not paused: start = datetime.now() ret, img = video.read() if ret: frame = img.copy() else: paused = True drawInfo(frame, frameNum, frameCount, paused, fps, args.source) data.detect(frame) data.draw(frame) cv2.imshow(sourceName, frame) if paused: key = cv2.waitKey(0) else: end = datetime.now() delta = (end - start) if fps != 0: delay = int(max(1, ((1 / fps) - delta.total_seconds()) * 1000)) else: delay = 1 key = cv2.waitKey(delay) if key == ord('q') or key == ord('Q') or key == 27: break elif key == ord('p') or key == ord('P'): paused = not paused elif args.source == 'video' and (key == ord('r') or key == ord('R')): frameNum = 0 video.set(cv2.CAP_PROP_POS_FRAMES, frameNum) elif args.source == 'video' and paused and key == 2424832: frameNum -= 1 if frameNum < 0: frameNum = 0 video.set(cv2.CAP_PROP_POS_FRAMES, frameNum) elif args.source == 'video' and paused and key == 2555904: frameNum += 1 if frameNum >= frameCount: frameNum = frameCount - 1 elif args.source == 'video' and key == 2162688: frameNum -= (fps * 10) if frameNum < 0: frameNum = 0 video.set(cv2.CAP_PROP_POS_FRAMES, frameNum) elif args.source == 'video' and key == 2228224: frameNum += (fps * 10) if frameNum >= frameCount: frameNum = frameCount - 1 video.set(cv2.CAP_PROP_POS_FRAMES, frameNum) elif key == 7340032: showHelp(sourceName, frame.shape) if not paused: frameNum += 1 video.release() cv2.destroyAllWindows()"}
{"text_id": "9134", "text": "docstring: def fib(n): a, b = 0, 1 while a < n: a a, b = b, a + b"}
{"text_id": "9135", "text": "docstring: async def render(self) -> str: pass"}
{"text_id": "9136", "text": "docstring: def create_variables_in_class_scope(method): @functools.wraps(method) def wrapper(obj, *args, **kwargs): def default_context_manager(reuse=None): variable_scope = obj.variable_scope return tf.variable_scope(variable_scope, reuse=reuse) variable_scope_context_manager = getattr(obj, \"_enter_variable_scope\", default_context_manager) graph = tf.get_default_graph() with variable_scope_context_manager() as tmp_variable_scope: variable_scope = tmp_variable_scope with variable_scope_ops._pure_variable_scope( variable_scope, reuse=tf.AUTO_REUSE) as pure_variable_scope: name_scope = variable_scope.original_name_scope if name_scope[-1] != \"/\": name_scope += \"/\" with tf.name_scope(name_scope): sub_scope = snt_util.to_snake_case(method.__name__) with tf.name_scope(sub_scope) as scope: out_ops = method(obj, *args, **kwargs) return out_ops return wrapper"}
{"text_id": "9137", "text": "docstring: async def async_setup_entry(hass, config_entry, async_add_entities): entities = [] name = config_entry.data[CONF_NAME] sensor_data = HabitipyData(hass.data[DOMAIN][config_entry.entry_id]) await sensor_data.update() for sensor_type in SENSORS_TYPES: entities.append(HabitipySensor(name, sensor_type, sensor_data)) for task_type in TASKS_TYPES: entities.append(HabitipyTaskSensor(name, task_type, sensor_data)) async_add_entities(entities, True)"}
{"text_id": "9138", "text": "docstring: def _RegisterUnknownFlagSetter(self, setter): self.__dict__['__set_unknown'] = setter"}
{"text_id": "9139", "text": "docstring: def f(self,N_sel,t): dNdt=[] N0=np.copy(self.N) if(self.livings()[0][0]==0): basals_func=N_sel[0]*self.gamma0*self.R+N_sel[0]*np.dot(self.gamma[0],N0) dNdt=np.append(dNdt,basals_func) for i in range(len(self.animals())): func=-self.alpha*N_sel[i]-self.beta*N_sel[i]**2+N_sel[i]*np.dot(self.gamma[i],N0) dNdt=np.append(dNdt,func) return dNdt"}
{"text_id": "9140", "text": "docstring: def londontube_vertices(): stream = pkg_resources.resource_stream(__name__, 'data/londontube_vertices.csv') return pd.read_csv(stream)"}
{"text_id": "9141", "text": "docstring: def find_coefficients(airfoil, alpha, Reynolds=0, iteration=10, NACA=True, delete=False, PANE=False, GDES=False, dir=\"\"): filename = file_name(airfoil, alpha, reynolds=Reynolds, output='Polar') if not os.path.isfile(dir + filename): call(airfoil, alpha, Reynolds=Reynolds, output='Polar', iteration=iteration, NACA=NACA, PANE=PANE, GDES=GDES, dir=dir) coefficients = {} Data = output_reader(dir + filename, output='Polar', delete=False) for key in Data: try: coefficients[key] = Data[key][0] except: coefficients[key] = None if delete: os.remove(dir + filename) return coefficients"}
{"text_id": "9142", "text": "docstring: def write_combs_json(outpath, combs): with open(outpath, \"w+\") as outfile: for i, comb in enumerate(combs): case = { \"CASE_\" + str(i) : comb } outfile.write(json.dumps(case) + \"\\n\")"}
{"text_id": "9143", "text": "docstring: def prepare_input_ground_bicubic(img, scale=2, pad=0, upscaleimages=True, channels=1): img_ground = preprocess_image(img, scale = scale, pad=pad, channels=channels, upscale=False, crop_remains=True, img_type='ground') img_low = preprocess_image(img, scale = scale, pad=0, channels=channels, upscale= (not upscaleimages), crop_remains=True, img_type='input') img_bicubic = preprocess_image(img, scale = scale, pad=pad, channels=channels, upscale=True, crop_remains=True, img_type='bicubic') return img_ground, img_bicubic, img_low"}
{"text_id": "9144", "text": "docstring: def load_token(token_name): try: with open(token_name, 'r') as file: return str(file.readline()).strip() except EnvironmentError: print('Error loading access token from file')"}
{"text_id": "9145", "text": "docstring: def plot_bar_sbs_procedure_tt(df): subdata = df top=subdata.groupby([\"procedure\"])[[\"wait_time_50\"]].mean().reset_index().sort_values(by=['wait_time_50'], ascending=False).head(20)[\"procedure\"].tolist() subdata_top=subdata[subdata[\"procedure\"].isin(top)] chart1 = alt.Chart(subdata_top).mark_tick().encode( x=alt.X('mean(wait_time_50):Q',title=\"Wait Time (weeks)\"), y=alt.Y(\"procedure\", sort='-x',title=\"\"), color=alt.Color('year') ).properties( title=\"Waiting Times for 50 percent of Cases by Procedure\", width=200, height=300 ).interactive() top2=subdata.groupby([\"procedure\"])[[\"wait_time_90\"]].mean().reset_index().sort_values(by=['wait_time_90'], ascending=False).head(20)[\"procedure\"].tolist() subdata_top2=subdata[subdata[\"procedure\"].isin(top2)] chart2 = alt.Chart(subdata_top2).mark_tick().encode( x=alt.X('mean(wait_time_90):Q',title=\"Wait Time (weeks)\"), y=alt.Y(\"procedure\", sort='-x',title=\"\"), color=alt.Color('year') ).properties( title=\"Waiting Times for 90 percent of Cases by Procedure\", width=200, height=300 ).interactive() chart_sbs=alt.hconcat(chart1,chart2).configure_axis( labelFontSize=10, titleFontSize=10 ).to_html() return chart_sbs"}
{"text_id": "9146", "text": "docstring: def xpath(self, query: str) -> parsel.SelectorList: logger.debug('selecting content using xpath selector: %s', query) return self._selector.xpath(query)"}
{"text_id": "9147", "text": "docstring: def load_data(splits, visualize=False, verbose=False): scenes = [] for split in splits: scenes_file_path = \"splits/scenes_\" + split + \".txt\" with open(scenes_file_path) as f: for line in f: scenes.append(line.strip()) nb = {} for scene in scenes: nb[scene] = defaultdict(list) with open(SIM_DIR + 'connectivity/%s_connectivity.json' % scene) as f: data = json.load(f) for i,item in enumerate(data): if item['included']: for j,conn in enumerate(item['unobstructed']): if conn and data[j]['included']: nb[scene][item['image_id']].append(data[j]['image_id']) degree = [] dist = [] dataset = [] for scene in scenes: laser_path = DATA_DIR + scene + \"/laser_scans.json\" with open(laser_path) as jf: json_data = json.load(jf) scans = {} for item in json_data: scans[item['image_id']] = item for item in json_data: target_heading = [] target_range = [] for n_id in nb[scene][item['image_id']]: if n_id in scans: height_diff = item['position']['z'] - scans[n_id]['position']['z'] item_range = distance(item['position'], scans[n_id]['position']) if abs(height_diff) < ALLOWED_HEIGHT_DIFF and item_range < MAX_RANGE: target_heading.append(heading(item['position'], scans[n_id]['position'])) target_range.append(item_range) degree.append(len(target_heading)) if target_heading: dist += target_range item['laser'] = np.array(item['laser']) item['target_heading'] = np.array(target_heading) item['target_range'] = np.array(target_range) dataset.append(item) img = radial_occupancy(item['laser']) if visualize: target = radial_target(item['target_heading'], item['target_range']) visualize_scan(img, target) if verbose: print(\"Loaded %d scans from %d scenes\" % (len(dataset), len(scenes))) print(\"Average of %.1f targets per scan\" % np.average(np.array(degree))) print(\"\\nHistogram of range\\nBin\\tFreq\") freqs,bins = np.histogram(np.array(dist), bins=20, range=(0,5), density=True) for f,b in zip(freqs,bins): print('%.2fm\\t%.2f' % (b,f/np.sum(freqs))) random.shuffle(data) return dataset"}
{"text_id": "9148", "text": "docstring: def clamp(self): self.weight.data.clamp_(-1, 1)"}
{"text_id": "9149", "text": "docstring: def process_status(self, status): raise NotImplementedError"}
{"text_id": "9150", "text": "docstring: def ExpandNetworks(networks, project): expanded_networks = [] for network in networks: if network.startswith('https://'): expanded_networks.append(network) else: expanded_networks.append(COMPUTE_RESOURCE_URL.format(project, network)) return expanded_networks"}
{"text_id": "9151", "text": "docstring: def DoItemCollapsed(self, item): self.DeleteChildren(item)"}
{"text_id": "9152", "text": "docstring: def bfs(self, prune=None): queue = deque([(self, self.parent, None)]) while queue: item, parent, key = queue.popleft() yield item, parent, key if prune and prune(item, parent, key): continue if isinstance(item, Expression): for k, v in item.args.items(): nodes = ensure_list(v) for node in nodes: if isinstance(node, Expression): queue.append((node, item, k))"}
{"text_id": "9153", "text": "docstring: def read_config(self, tenant): runtime_config_path = RuntimeConfig.config_file_path( self.service, tenant ) self.logger.info( \"Reading runtime config '%s'\" % runtime_config_path ) try: with open(runtime_config_path, encoding='utf-8') as fh: self.config = json.load(fh) except Exception as e: self.logger.error( \"Could not load runtime config '%s':\\n%s\" % (runtime_config_path, e) ) raise e return self"}
{"text_id": "9154", "text": "docstring: def index_to_profile_token(self): if self._profile_index >= len(self._profiles): _LOGGER.warning( \"ONVIF Camera '%s' doesn't provide profile %d.\" \" Using the last profile.\", self._name, self._profile_index, ) self._profile_index = -1 _LOGGER.debug(\"Using profile index '%d'\", self._profile_index) return self._profiles[self._profile_index].token"}
{"text_id": "9155", "text": "docstring: async def _async_update_data(self) -> Dict[str, Device]: try: events = await self.client.fetch_events() except BadCredentialsException as exception: raise ConfigEntryAuthFailed() from exception except TooManyRequestsException as exception: raise UpdateFailed(\"Too many requests, try again later.\") from exception except MaintenanceException as exception: raise UpdateFailed(\"Server is down for maintenance.\") from exception except TimeoutError as exception: raise UpdateFailed(\"Failed to connect.\") from exception except (ServerDisconnectedError, NotAuthenticatedException): self.executions = {} try: await self.client.login() self.devices = await self._get_devices() except BadCredentialsException as exception: raise ConfigEntryAuthFailed() from exception except TooManyRequestsException as exception: raise UpdateFailed(\"Too many requests, try again later.\") from exception return self.devices except Exception as exception: _LOGGER.debug(exception) raise UpdateFailed(exception) from exception for event in events: _LOGGER.debug( \"%s/%s (device: %s, state: %s -> %s)\", event.name, event.exec_id, event.deviceurl, event.old_state, event.new_state, ) if event.name == EventName.DEVICE_AVAILABLE: self.devices[event.deviceurl].available = True elif event.name in [ EventName.DEVICE_UNAVAILABLE, EventName.DEVICE_DISABLED, ]: self.devices[event.deviceurl].available = False elif event.name in [ EventName.DEVICE_CREATED, EventName.DEVICE_UPDATED, ]: self.hass.async_create_task( self.hass.config_entries.async_reload(self._config_entry_id) ) return None elif event.name == EventName.DEVICE_REMOVED: base_device_url, *_ = event.deviceurl.split(\"#\") registry = await device_registry.async_get_registry(self.hass) if device := registry.async_get_device({(DOMAIN, base_device_url)}): registry.async_remove_device(device.id) del self.devices[event.deviceurl] elif event.name == EventName.DEVICE_STATE_CHANGED: for state in event.device_states: device = self.devices[event.deviceurl] if state.name not in device.states: device.states[state.name] = state device.states[state.name].value = self._get_state(state) elif event.name == EventName.EXECUTION_REGISTERED: if event.exec_id not in self.executions: self.executions[event.exec_id] = {} self.update_interval = timedelta(seconds=1) elif ( event.name == EventName.EXECUTION_STATE_CHANGED and event.exec_id in self.executions and event.new_state in [ExecutionState.COMPLETED, ExecutionState.FAILED] ): del self.executions[event.exec_id] if not self.executions: self.update_interval = self.original_update_interval return self.devices"}
{"text_id": "9156", "text": "docstring: def log_prob(self, x, context): raise NotImplementedError()"}
{"text_id": "9157", "text": "docstring: def updateTransformation(self, config: Config, pruneM: bool): setupA = sum([x.scatterMatrix for x in config.clusters]) - config.scatterMatrixAllData eigenvalues, eigenvectors = MatrixUtils.sortedEigSym(setupA, ascending=True) newM = self.determineMFromEigenvalues(eigenvalues) if pruneM else config.m return config.copy(transformation=eigenvectors, m=newM, eigenvalues=eigenvalues)"}
{"text_id": "9158", "text": "docstring: def send_request(self, path, method='GET'): time.sleep(self.request_interval_milliseconds) full_uri = self.api_root + '/' + path response = self.session.request( method, full_uri, timeout=self.request_timeout_milliseconds, headers=self.headers) return response"}
{"text_id": "9159", "text": "docstring: def serialized(method_name): def real_serialized(method): def wrapper(*args, **kwargs): service_queue = args[0].service_queue my_request_id = uuid.uuid4() service = None if len(args) > 0: last_arg = args[-1] if isinstance(last_arg, dict) and ('pool' in last_arg): service = last_arg if 'service' in kwargs: service = kwargs['service'] NOTE: The following block of code alters the state of a queue that other greenthreads are waiting behind. This code assumes it will not be preempted by another greenthread while running. It does not do I/O or call any other monkey-patched code which might cause a context switch. To avoid race conditions, DO NOT add logging to this code block. num_requests = len(service_queue) queue optimization if num_requests > 1 and method_name == 'create_member': cur_pool_id = service['pool']['id'] cur_index = num_requests - 1 do not attempt to replace the first entry (index 0) because it may already be in process. while cur_index > 0: (check_request, check_method, check_service) = \\ service_queue[cur_index] if check_service['pool']['id'] != cur_pool_id: cur_index -= 1 continue if check_method != 'create_member': break move this request up in the queue and return so that existing thread can handle it service_queue[cur_index] = \\ (check_request, check_method, service) return End of code block which assumes no preemption. req = (my_request_id, method_name, service) service_queue.append(req) reqs_ahead_of_us = request_index(service_queue, my_request_id) while reqs_ahead_of_us != 0: if reqs_ahead_of_us == 1: it is almost our turn. get ready waitsecs = .01 else: waitsecs = reqs_ahead_of_us * .5 if waitsecs > .01: LOG.debug('%s request %s is blocking' ' for %.2f secs - queue depth: %d' % (str(method_name), my_request_id, waitsecs, len(service_queue))) greenthread.sleep(waitsecs) reqs_ahead_of_us = request_index(service_queue, my_request_id) try: LOG.debug('%s request %s is running with queue depth: %d' % (str(method_name), my_request_id, len(service_queue))) start_time = time() result = method(*args, **kwargs) LOG.debug('%s request %s took %.5f secs' % (str(method_name), my_request_id, time() - start_time)) except: LOG.error('%s request %s FAILED' % (str(method_name), my_request_id)) raise finally: service_queue.pop(0) return result return wrapper return real_serialized"}
{"text_id": "9160", "text": "docstring: def update_if_finite_grads(): def incr_loss_scale(): new_loss_scale = self.current_loss_scale * self.multiplier return tf.group( _assign_if_finite(self.current_loss_scale, new_loss_scale), self.counter.assign(0)) return tf.cond( self.counter + 1 >= self.growth_steps, incr_loss_scale, lambda: _op_in_graph_mode(self.counter.assign_add(1)))"}
{"text_id": "9161", "text": "docstring: def load(self, url: str, encoding: str = \"iso8859-1\"): with open(url, \"r\", encoding=encoding) as f: reader = csv.reader(f, delimiter=\",\") if self.data is None: self.data = [] for row in reader: self.data.append(row)"}
{"text_id": "9162", "text": "docstring: def add_renren_login(config, consumer_key, consumer_secret, scope='', login_path='/login/renren', callback_path='/login/renren/callback', name='renren'): provider = RenrenProvider(name, consumer_key, consumer_secret, scope) config.add_route(provider.login_route, login_path) config.add_view(provider.login, route_name=provider.login_route, permission=NO_PERMISSION_REQUIRED) config.add_route(provider.callback_route, callback_path, use_global_views=True, factory=provider.callback) register_provider(config, name, provider)"}
{"text_id": "9163", "text": "docstring: def unavailabilities_get_with_http_info(self, **kwargs): \"\"\"Get all unavailabilities This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.unavailabilities_get_with_http_info(async_req=True) >>> result = thread.get() :param async_req bool :param int page: :param int length: :param str sort: :param str q: :param str fields: :param str _with: :return: UnavailabilityCollection If the method is called asynchronously, returns the request thread. \"\"\" all_params = ['page', 'length', 'sort', 'q', 'fields', '_with'] all_params.append('async_req') all_params.append('_return_http_data_only') all_params.append('_preload_content') all_params.append('_request_timeout') params = locals() for key, val in six.iteritems(params['kwargs']): if key not in all_params: raise TypeError( \"Got an unexpected keyword argument '%s'\" \" to method unavailabilities_get\" % key ) params[key] = val del params['kwargs'] collection_formats = {} path_params = {} query_params = [] if 'page' in params: query_params.append(('page', params['page'])) if 'length' in params: query_params.append(('length', params['length'])) if 'sort' in params: query_params.append(('sort', params['sort'])) if 'q' in params: query_params.append(('q', params['q'])) if 'fields' in params: query_params.append(('fields', params['fields'])) if '_with' in params: query_params.append(('with', params['_with'])) header_params = {} form_params = [] local_var_files = {} body_params = None header_params['Accept'] = self.api_client.select_header_accept( ['application/json']) auth_settings = [] return self.api_client.call_api( '/unavailabilities', 'GET', path_params, query_params, header_params, body=body_params, post_params=form_params, files=local_var_files, response_type='UnavailabilityCollection', auth_settings=auth_settings, async_req=params.get('async_req'), _return_http_data_only=params.get('_return_http_data_only'), _preload_content=params.get('_preload_content', True), _request_timeout=params.get('_request_timeout'), collection_formats=collection_formats)"}
{"text_id": "9164", "text": "docstring: def inactivate_required_services_for_redeploy(self, resources_to_process, dry_run): if dry_run: return for resource in resources_to_process: if self.terraform_thread.isAlive(): resource_base_classes = inspect.getmro(resource.__class__) if ECSTaskDefinitionResource in resource_base_classes: try: deregister_task_definition( Settings.AWS_ACCESS_KEY, Settings.AWS_SECRET_KEY, Settings.AWS_REGION, resource.get_input_attr('family'), ) except: pass elif ECSClusterResource in resource_base_classes: cluster_name = resource.get_input_attr('name') else: return for i in range(3): if self.terraform_thread.isAlive(): try: stop_all_tasks_in_a_cluster( cluster_name, Settings.AWS_ACCESS_KEY, Settings.AWS_SECRET_KEY, Settings.AWS_REGION ) except: pass time.sleep(20) else: return"}
{"text_id": "9165", "text": "docstring: def create_log(nickname=\"\", cmd=False, with_random=True): log_nickname = nickname if cmd: if len(sys.argv) >= 2: log_nickname = sys.argv[1] random = str(rand.random()) if with_random else \"\" print(\"log id:\", log_nickname + random) try: log_object = Tee(\"examples/log_\" + log_nickname + random + \"_log.txt\") except FileNotFoundError: log_object = Tee(\"log_\" + log_nickname + random + \"_log.txt\")"}
{"text_id": "9166", "text": "docstring: def check_wavefront(filename_or_hdulist, slice=0, ext=0, test='nearzero', comment=\"\"): if isinstance(filename_or_hdulist, str): hdulist = fits.open(filename_or_hdulist) filename = filename_or_hdulist elif isinstance(filename_or_hdulist, fits.HDUList): hdulist = filename_or_hdulist filename = 'input HDUlist' imstack = hdulist[ext].data im = imstack[slice,:,:] if test=='nearzero': return np.all(np.abs(im) < np.finfo(im.dtype).eps*10) elif test == 'is_real': cplx_im = imstack[1,:,:] * np.exp(1j*imstack[2,:,:]) return np.all( cplx_im.imag < np.finfo(im.dtype).eps*10)"}
{"text_id": "9167", "text": "docstring: def invert(self, solution, inverse_data): status = solution['status'] primal_vars = None dual_vars = None if status in s.SOLUTION_PRESENT: opt_val = solution['value'] primal_vars = {inverse_data[self.VAR_ID]: solution['primal']} else: if status == s.INFEASIBLE: opt_val = np.inf elif status == s.UNBOUNDED: opt_val = -np.inf else: opt_val = None return Solution(status, opt_val, primal_vars, dual_vars, {})"}
{"text_id": "9168", "text": "docstring: def generate_acr_attr_setter(role_name): def set_obj_attr(model, person): return { \"access_control_list_\": [{ \"ac_role_id\": AC_ROLES[model][role_name], \"person_id\": person.id }] } return set_obj_attr"}
{"text_id": "9169", "text": "docstring: def _yield_eval_comprehensions( generators: List[ast.comprehension], scope: Dict, generator_scope: Optional[Dict]=None): if generator_scope is None: generator_scope = {} if len(generators) <= 0: yield generator_scope else: comprehension = generators[0] if comprehension.is_async: raise SafeEvalException([f\"Async list comprehensions not \" f\"supported: {type(comprehension)}\"]) if isinstance(comprehension.target, ast.Name): targets: List[str] = [comprehension.target.id] elif isinstance(comprehension.target, ast.Tuple): targets = [i.id for i in comprehension.target.elts] else: raise SafeEvalException( [f\"Unsupported list comprehension target: \" f\"{type(comprehension.target)}\"]) for item in _eval(comprehension.iter, scope | generator_scope): for name in targets: generator_scope[name] = item eval_scope = scope | generator_scope for condition in comprehension.ifs: if not _eval(condition, eval_scope): break else: yield from _yield_eval_comprehensions( generators[1:], scope, generator_scope)"}
{"text_id": "9170", "text": "docstring: def SetProxyFromConf(): path = helperlib.CONFIG_SYSCONFDIR + '/' + helperlib.CONFIG_SYSCONFDIR_DSC + '/dsc.conf' txt, error = ReadFile(path) if error : return for l in txt.splitlines(): if l.startswith('PROXY'): info = l.split('=')[1].strip() if 'https' in info: os.environ['HTTPS_PROXY'] = info if 'http:' in info: os.environ['HTTP_PROXY'] = info return"}
{"text_id": "9171", "text": "docstring: def mean_distance(cnt, ref_point): y,x = np.hsplit(cnt,2) y0, x0 = ref_point mean_distance = np.mean(((y-y0)**2 + (x-x0)**2)**0.5) return mean_distance"}
{"text_id": "9172", "text": "docstring: def string_rep(obj, quotes=False): assert obj is not None, 'Input must not be None' quote = '\"' if quotes else '' if isinstance(obj, (tuple, list)): obj_str = ', '.join([string_rep(item) for item in obj]) else: obj_str = str(obj) return quote + obj_str + quote"}
{"text_id": "9173", "text": "docstring: def overlaps(self, range_object: 'Range') -> bool: if range_object is None: return False if not isinstance(range_object, Range): raise TypeError(\"Object to be compared must be of type 'Range'\") if not self._is_set: raise ValueError(\"Range is not set.\") if not range_object.check_set(): raise ValueError(\"Range object to be compared is not set.\") if (range_object.start_time() <= self.end_time()) and\\ (range_object.end_time() >= self.start_time()): return True return False"}
{"text_id": "9174", "text": "docstring: def load_data(self, dataset_name, data_loader=load_data_from_openml_or_ml_prepare, data_type='infer', output_type='numpy', K=None, **data_loader_kwargs): self.data = super(SkLearnTrainer, self).load_data(dataset_name=dataset_name, data_loader=data_loader, data_type=data_type, output_type=output_type, K=K, **data_loader_kwargs)"}
{"text_id": "9175", "text": "docstring: def saveEmbededMessage(self, contentId=False, json=False, useFileName=False, raw=False, customPath=None, customFilename=None): self.data.save(json, useFileName, raw, contentId, customPath, customFilename)"}
{"text_id": "9176", "text": "docstring: def read_way(stop1, stop2, sql_connection): try: cur = sql_connection.cursor() cur.execute(\"SELECT _id FROM stopsker WHERE Name_stop = ?\", [stop1]) Input = cur.fetchone() if Input is None: return [], [], [] else: id1 = int(Input[0]) cur.execute(\"SELECT _id FROM stopsker WHERE Name_stop = ?\", [stop2]) Input = cur.fetchone() if Input is None: return [], [], [] else: id2 = int(Input[0]) cur.execute(\"SELECT route, transfer, cords FROM way WHERE id1 = ? and id2 = ?\", [id1, id2]) Input = cur.fetchone() if Input is None: return [], [], [] way = list() for element in Input[0].split(): cur.execute(\"SELECT Name_stop, Cords FROM stopsker WHERE _id = ?\", [element]) info = cur.fetchone() cords = info[1].split() way.append(STOP( element, info[0], float(cords[0]), float(cords[1]) )) transfer = list() for elem in Input[1].split(\";\"): one_transfer = elem.split() cur.execute(\"SELECT Name_stop, Cords FROM stopsker WHERE _id = ?\", [one_transfer[0]]) info = cur.fetchone() cords = info[1].split() cur.execute(\"SELECT Name_route FROM routesker WHERE _id = ?\", [one_transfer[1]]) name_route = cur.fetchone()[0] transfer.append([ STOP( one_transfer[0], info[0], float(cords[0]), float(cords[1]) ), name_route]) way_cords = list() for item in (Input[2].split('\\n')): way_cords.append(item.split()) way_cords = [ [float(item[0]), float(item[1])] for item in way_cords] except Exception as exc: print({exc}) sql_connection.close() exit() else: return way_cords, way, transfer"}
{"text_id": "9177", "text": "docstring: def apply_diff(self, diff: Dict, revision: Dict) -> bool: logging.info(f\"Applying {diff['id']} for revision {revision['id']}...\") patch = self.get_raw_diff(str(diff['id'])) self.apply_diff_counter += 1 patch_file = f\"{self.apply_diff_counter}_{diff['id']}.patch\" with open(os.path.join(self.build_dir, patch_file), 'wt') as f: f.write(patch) upload_file(self.build_dir, patch_file) logging.debug(f'raw patch:\\n{patch}') proc = subprocess.run('git apply -', input=patch, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) if proc.returncode != 0: logging.info(proc.stdout) logging.error(proc.stderr) message = f\":bk-status-failed: Failed to apply [{patch_file}](artifact://{patch_file}).\\n\\n\" if self.revision_id != revision['id']: message += f\"**Attention! D{revision['id']} is one of the dependencies of the target \" \\ f\"revision D{self.revision_id}.**\\n\\n\" message += (f\"No testing is possible because we couldn't apply the patch.\\n\\n\" f\"---\\n\\n\" '### Troubleshooting\\n\\n' 'More information is available in the log of of *create branch* step. ' f\"All patches applied are available as *Artifacts*.\\n\\n\" f\":bulb: The patch may not apply if it includes only the most recent of \" f\"multiple local commits. Try to upload a patch with\\n\" f\"```shell\\n\" f\"arc diff `git merge-base HEAD origin` --update D{revision['id']}\\n\" f\"```\\n\\n\" f\"to include all local changes.\\n\\n\" '---\\n\\n' f\"If this case could have been handled better, please [create a bug]({feedback_url()}).\") annotate(message, style='error', context='patch_diff') return False self.commit(revision, diff) return True"}
{"text_id": "9178", "text": "docstring: def _ExpungeParts(self, cnxn, project_id): while True: issue_id_rows = self.services.issue.issue_tbl.Select( cnxn, cols=['id'], project_id=project_id, limit=1000) issue_ids = [row[0] for row in issue_id_rows] for issue_id in issue_ids: self.services.issue_star.ExpungeStars(cnxn, issue_id) self.services.issue.ExpungeIssues(cnxn, issue_ids) yield issue_ids break project_purge_functions = ( self.services.config.ExpungeConfig, self.services.template.ExpungeProjectTemplates, self.services.features.ExpungeSavedQueriesExecuteInProject, self.services.features.ExpungeFilterRules, self.services.issue.ExpungeFormerLocations, self.services.issue.ExpungeLocalIDCounters, self.services.features.ExpungeQuickEditHistory, self.services.project_star.ExpungeStars, self.services.project.ExpungeProject, ) for f in project_purge_functions: f(cnxn, project_id) yield project_id"}
{"text_id": "9179", "text": "docstring: def bump_spec( self, version: str = None, changelog_entry: str = None, bump_release: bool = False, ): cmd = [\"rpmdev-bumpspec\"] if version: cmd += [\"--new\", version] if changelog_entry: cmd += [\"--comment\", changelog_entry] if bump_release: cmd += [\"-r\"] cmd.append(str(self.absolute_specfile_path)) run_command(cmd)"}
{"text_id": "9180", "text": "docstring: def pop(self) -> T: return heapq.heappop(self.data)"}
{"text_id": "9181", "text": "docstring: def eraseToolMousePress(self, event: QGraphicsSceneMouseEvent, idx: int): m_strand = self._model_strand m_strand.strandSet().removeStrand(m_strand)"}
{"text_id": "9182", "text": "docstring: def reflect_kvs(self): for k in self.keys(): kv_root = self.get_kv_root(k=k) kv = KVStore.load_implementation(root=kv_root) kv.attach(root=kv_root) self.kvs[k] = kv"}
{"text_id": "9183", "text": "docstring: def to_dict(self): result = {} for attr, _ in six.iteritems(self.swagger_types): value = getattr(self, attr) if isinstance(value, list): result[attr] = list(map( lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x, value )) elif hasattr(value, \"to_dict\"): result[attr] = value.to_dict() elif isinstance(value, dict): result[attr] = dict(map( lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], \"to_dict\") else item, value.items() )) else: result[attr] = value if issubclass(CardBaseRequestCO, dict): for key, value in self.items(): result[key] = value return result"}
{"text_id": "9184", "text": "docstring: def _finalize_self(self): self.slug = os.path.dirname(self.filename).split('/')[-1] m = self.KEYPOINTS.search(self._data) assert m, \\ 'No keypoints found in %s' % self.filename self.keypoints = m.group(1)"}
{"text_id": "9185", "text": "docstring: def record_vistrail(what, vistrail): if not usage_report.recording: return from vistrails.core.vistrail.controller import VistrailController from vistrails.core.vistrail.pipeline import Pipeline from vistrails.core.vistrail.vistrail import Vistrail from vistrails.db.services.locator import XMLFileLocator if isinstance(vistrail, VistrailController): vistrail = vistrail.vistrail if what == 'save': saved_vistrails[id(vistrail)] = vistrail return elif what == 'close': i = id(vistrail) if i in saved_vistrails: del saved_vistrails[i] what = 'saved_close' else: return if isinstance(vistrail, Vistrail): upgrade_from = set() upgrade_to = set() nb_notes = 0 nb_paramexplorations = 0 for annotation in vistrail.action_annotations: if annotation.key == Vistrail.UPGRADE_ANNOTATION: upgrade_from.add(annotation.action_id) upgrade_to.add(int(annotation.value)) elif annotation.key == Vistrail.NOTES_ANNOTATION: nb_notes += 1 elif annotation.key == Vistrail.PARAMEXP_ANNOTATION: nb_paramexplorations += 1 nb_upgrades = len(upgrade_from - upgrade_to) if isinstance(vistrail.locator, XMLFileLocator): usage_report.note({'in_examples_dir': os.path.realpath(vistrail.locator._name).startswith( os.path.realpath(vistrails_examples_directory()))}) nb_modules = 0 nb_groups = 0 nb_abstractions = 0 for action in vistrail.actions: if action.id in upgrade_to or action.description == \"Upgrade\": continue for operation in action.operations: if operation.vtType == 'add' or operation.vtType == 'change': if operation.what == 'module': nb_modules += 1 if operation.data.is_group(): nb_groups += 1 elif operation.data.is_abstraction(): nb_abstractions += 1 usage_report.note(dict(use_vistrail=what, nb_versions=len(vistrail.actionMap), nb_tags=len(vistrail.get_tagMap()), nb_notes=nb_notes, nb_paramexplorations=nb_paramexplorations, nb_upgrades=nb_upgrades, nb_variables=len(vistrail.vistrail_variables), nb_modules=nb_modules, nb_groups=nb_groups, nb_abstractions=nb_abstractions)) for feature in features_for_vistrails.pop(id(vistrail), ()): usage_report.note({'feature_for_vistrail': feature}) elif isinstance(vistrail, Pipeline): usage_report.note(dict(use_workflow=what, nb_modules=len(vistrail.module_list))) else: raise TypeError"}
{"text_id": "9186", "text": "docstring: def build_empty_bbox_mask(bboxes): flag = [1 for _ in range(len(bboxes))] for i, bbox in enumerate(bboxes): if bbox == [0,0,0,0]: flag[i] = 0 return flag"}
{"text_id": "9187", "text": "docstring: def client(app, session): client = app.test_client(use_cookies=True) yield client"}
{"text_id": "9188", "text": "docstring: def success_probability_counter(self, population): children = [population[i] for i in range(0 + self.mu, self.mu + self.lambda_)] parents = [population[i] for i in range(0, self.mu)] num = sum(any(child > parent for parent in parents) for child in children) self.successful_individuals += num self.total_individuals += len(children)"}
{"text_id": "9189", "text": "docstring: def topic(): topic = get_random_string(string.ascii_letters, 10) logger.debug('Using Topic: %s', topic) return topic"}
{"text_id": "9190", "text": "docstring: def submit(workspace, cmd, create_job_info=True, **params): from klab import cluster, process params = dict((k, v) for k, v in list(params.items()) if v is not None) test_run = params.get('test_run', False) nstruct = params.get('nstruct') max_runtime = params.get('max_runtime', '6:00:00') max_memory = params.get('max_memory', '1G') if test_run: max_runtime = '0:30:00' if nstruct is None: raise TypeError(\"sumbit() requires the keyword argument 'nstruct' for production runs.\") qsub_command = 'qsub', '-h', '-cwd' qsub_command += '-o', workspace.log_dir qsub_command += '-e', workspace.log_dir qsub_command += '-t', '1-{0}'.format(nstruct), qsub_command += '-l', 'h_rt={0}'.format(max_runtime), qsub_command += '-l', 'mem_free={0}'.format(max_memory), qsub_command += '-b', 'y', qsub_command += '-N', params.get('job_name'), for param in cmd: qsub_command += param, status = process.check_output(qsub_command).decode('utf-8') status_pattern = re.compile(r'Your job-array (\\d+).[0-9:-]+ \\(\".*\"\\) has been submitted') status_match = status_pattern.match(status) if not status_match: print(status) sys.exit() job_id = status_match.group(1) if create_job_info: with open(workspace.job_info_path(job_id), 'w') as file: json.dump(params, file) qrls_command = 'qrls', job_id process.check_output(qrls_command) print(status, end=' ')"}
{"text_id": "9191", "text": "docstring: def distance_between_pairs(coreferents_distances, coreferents_list, i, j): if i == j: return 0 corefs_i = coreferents_list[i] corefs_j = coreferents_list[j] if not corefs_i or not corefs_j: coref_distance = MAX_COREF_DISTANCE for coref_i in corefs_i: for coref_j in corefs_j: if coref_j in coreferents_distances[coref_i]: coref_distance = min(coref_distance, coreferents_distances[coref_i][coref_j] + 1) + 1 here because distance between corferents should be also positive to distinguish from the same word return coref_distance"}
{"text_id": "9192", "text": "docstring: def name(self, name): name = name.strip() if not name: raise ValueError(\"Name required.\") for x in name: if x in _forbidden_in_named_range: raise ValueError(\"Character forbidden '%s' \" % x) step = \"\" for x in name: if x in string.ascii_letters and step in (\"\", \"A\"): step = \"A\" continue elif step in (\"A\", \"A1\") and x in string.digits: step = \"A1\" continue else: step = \"\" break if step == \"A1\": raise ValueError(\"Name of the type 'ABC123' is not allowed.\") try: body = self.document_body named_range = body.get_named_range(name) if named_range: named_range.delete() except Exception: pass self.set_attribute(\"table:name\", name)"}
{"text_id": "9193", "text": "docstring: def transpose_values(cls): cls.ENTITY = dict( zip(cls.ENTITY.values(), cls.ENTITY.keys()) ) cls.TYPE = dict(zip(cls.TYPE.values(), cls.TYPE.keys())) cls.NE = dict(zip(cls.NE.values(), cls.NE.keys())) cls.SLOT = dict(zip(cls.SLOT.values(), cls.SLOT.keys())) cls.RELATION = dict( zip(cls.RELATION.values(), cls.RELATION.keys()) ) cls.FEATURE = dict( zip(cls.FEATURE.values(), cls.FEATURE.keys()) )"}
{"text_id": "9194", "text": "docstring: def register_proto_function(collection_name, proto_type=None, to_proto=None, from_proto=None): if to_proto and not callable(to_proto): raise TypeError(\"to_proto must be callable.\") if from_proto and not callable(from_proto): raise TypeError(\"from_proto must be callable.\") _proto_function_registry.register((proto_type, to_proto, from_proto), collection_name)"}
{"text_id": "9195", "text": "docstring: def execute(self, inp): out = '' cmds = self.parse_exec_cmds(inp) for cmd in cmds: cmd_out = self.cmd_exec(cmd) out += '='*20 + '\\n\\n$ {}\\n{}\\n'.format(cmd, cmd_out) return out"}
{"text_id": "9196", "text": "docstring: def plot_heatmap( self, genes: Union[list, np.ndarray], save=None, show=True, transform: str = \"zscore\", nticks=10, cmap: str = \"YlGnBu\", width=10, height_per_gene=0.5, return_axs=False ): import seaborn as sns import matplotlib.pyplot as plt plt.ioff() gene_idx = self._idx_genes(genes) fig = plt.figure(figsize=(width, height_per_gene * len(gene_idx))) ax = fig.add_subplot(111) data = np.array([ self._continuous_model(idx=g, non_numeric=False) for i, g in enumerate(gene_idx) ]) idx_x_sorted = np.argsort(self._continuous_coords) data = data[:, idx_x_sorted] xcoord = self._continuous_coords[idx_x_sorted] if transform.lower() == \"log10\": data = np.nextafter(0, 1, out=data, where=data == 0) data = np.log(data) / np.log(10) elif transform.lower() == \"zscore\": mu = np.mean(data, axis=0) sd = np.std(data, axis=0) sd = np.nextafter(0, 1, out=sd, where=sd == 0) data = np.array([(x - mu[i]) / sd[i] for i, x in enumerate(data)]) elif transform.lower() == \"none\": pass else: raise ValueError(\"transform not recognized in plot_heatmap()\") sns.heatmap(data=data, cmap=cmap, ax=ax) xtick_pos = np.asarray(np.round(np.linspace( start=0, stop=data.shape[1] - 1, num=nticks, endpoint=True )), dtype=int) xtick_lab = [str(np.round(xcoord[np.argmin(np.abs(xcoord - xcoord[i]))], 2)) for i in xtick_pos] ax.set_xticks(xtick_pos) ax.set_xticklabels(xtick_lab) ax.set_xlabel(\"continuous\") plt.yticks(np.arange(len(genes)), genes, rotation='horizontal') ax.set_ylabel(\"genes\") if save is not None: plt.savefig(save + '_genes.png') if show: plt.show() plt.close(fig) if return_axs: return ax else: return"}
{"text_id": "9197", "text": "docstring: def construct_preview_pulse(vips, pulse, iteration): template_no = pulse['Template_no'] template_def = vips.template_defs[template_no - 1] template_idx = pulse['Template_identifier'].def_idx if template_idx >= vips.DRAG_INDEX_OFFSET: templ_x, _ = utils.template_def_to_points(vips, template_def, iteration) templ_y = vips.drag_templates[template_idx - vips.DRAG_INDEX_OFFSET] else: templ_x, templ_y = utils.template_def_to_points(vips, template_def, iteration) if len(templ_y) == 0: return 0, [] start_base, start_delta = pulse['Time'] time = start_base + start_delta * iteration abs_time = utils.get_absolute_time(vips, start_base, start_delta, iteration) p_amp, p_freq, p_phase = utils.get_amp_freq_phase(pulse, iteration) reset_time = -1 for (t, _, _) in vips.carrier_changes[pulse['Port'] - 1]: if t > abs_time: break reset_time = t p_phase = utils.phase_sync(p_freq, p_phase, abs_time - reset_time) if p_freq != 0 and pulse['Carrier'] != 0: carrier = np.cos(2 * np.pi * p_freq * templ_x + np.pi * p_phase) templ_y = templ_y * carrier wave = templ_y * p_amp return time, wave"}
{"text_id": "9198", "text": "docstring: def evaluate_options(options, option_eval_dct): options = list(options) option_names = tuple(sorted(option_eval_dct.keys())) for idx, option in enumerate(options): if _option_is_valid(option): name = _option_name(option) assert name in option_names options[idx] = option_eval_dct[name](option) return tuple(options)"}
{"text_id": "9199", "text": "docstring: def create_binary_buffer(init_or_size): if isinstance(init_or_size, int): init_or_size = bytearray(init_or_size) return create_initialised_buffer(init_or_size)"}
{"text_id": "9200", "text": "docstring: def solve(cellArr): if type(cellArr) is not list or type(cellArr[0]) is not list: print(\"please pass a 2D integer list to this function.\") return None rowLength = len(cellArr[0]) for row in cellArr: if len(row) != rowLength: print(\"all rows in the 2D integer list must be of equal length.\") return None puzzle = Block_Puzzle(cellArr) if not puzzle.is_solvable(): return None goal = puzzle.create_solved_puzzle() path = aStar(puzzle, goal) moves = [] for index in range(len(path)-1): moves.append(path[index].get_move(path[index+1])) return moves"}
{"text_id": "9201", "text": "docstring: def _create_masks(self) -> None: L = len(self.hidden_dims) D = self.n_in self.masks[0] = permutation(D) if self.random_order else np.arange(D) for l in range(L): low = self.masks[l].min() size = self.hidden_dims[l] self.masks[l + 1] = randint(low=low, high=D - 1, size=size) self.masks[L + 1] = self.masks[0] for i in range(len(self.masks) - 1): m = self.masks[i] m_next = self.masks[i + 1] M = torch.zeros(len(m_next), len(m)) for j in range(len(m_next)): M[j, :] = torch.from_numpy((m_next[j] >= m).astype(int)) self.mask_matrix.append(M) if self.gaussian: m = self.mask_matrix.pop(-1) self.mask_matrix.append(torch.cat((m, m), dim=0)) mask_iter = iter(self.mask_matrix) for module in self.model.modules(): if isinstance(module, MaskedLinear): module.initialise_mask(next(mask_iter))"}
{"text_id": "9202", "text": "docstring: def period(self, value): self._write_int('period', value)"}
{"text_id": "9203", "text": "docstring: def format_line(line_list): label = \"-1\" tmp_new = list() tmp_new.append(label) ct = 1 for element in line_list: element = \"{}:{}\".format(ct, element) tmp_new.append(element) ct += 1 return \" \".join(tmp_new)"}
{"text_id": "9204", "text": "docstring: def is_integer_image(image: TensorOrArray) -> bool: c = get_num_channels(image) if c == 1: return True return False"}
{"text_id": "9205", "text": "docstring: def wifi(self, args): if not self.adb_exist(1): return 0 self.find_device(1) for i in self.device_list: print \"====================\" print \" %s\" % i print \"====================\" p_mac = os.popen(\"adb shell ifconfig | grep HWaddr\") p_mac_read = p_mac.read() print p_mac_read mac_search = re.search(\"HWaddr (\\S*)\", p_mac_read) self.mac = \"\".join(mac_search.group(1).split(':')[-4:]) self.mpstate.mac = self.mac print \"mac:%s\" % self.mac p = os.popen(\"adb -s %s shell sed -n '56,60p' /etc/hostapd.conf\" % i) _line = p.read() self.mpstate.uiqueue.put(_line) while 1: p_wifi = os.popen(\"adb shell cat /etc/hostapd.conf | grep 'ssid='\") p_wifi_r = p_wifi.read() if \"device not found\" in p_wifi_r: print \"can not find device\" else: ssid_search = re.search(\"ssid=(\\w*)\", p_wifi_r) self.ssid = ssid_search.group(1) self.mpstate.ssid = self.ssid print \"ssid: %s\" % self.ssid break"}
{"text_id": "9206", "text": "docstring: def runserver(options): SERVICEDIR.chdir() easy.info(\"-- Changing to %s --\" % SERVICEDIR) easy.info(\"-- Running Service --\") pipe_output, file_name = tempfile.mkstemp() cmd = [\"cloudsearch_service\", \"--config\", \"conf.ini\"] p = subprocess.Popen(cmd, stdout=pipe_output, stderr=subprocess.STDOUT) output_fd = open(file_name) try: while not p.poll(): time.sleep(0.01) line = output_fd.readline() if line: sys.stdout.write(\"> %s\\n\" % line.strip()) sys.stdout.flush() except KeyboardInterrupt: easy.info(\"\\tCtrl-C caught, Exitting...\") p.send_signal(signal.SIGINT) time.sleep(1) p.kill() finally: output_fd.close() os.unlink(file_name)"}
{"text_id": "9207", "text": "docstring: def distance_vector(self, src_node_id, top_id, required_bandwidth): distance = {} parent = {} top_mgr = self.mgrs[\"top\"] cur_top = top_mgr.get_topology(top_id) for node_id in cur_top.get_node_ids(): distance[node_id] = math.inf distance[src_node_id] = 0 for i in range(1, cur_top.get_num_nodes()): for edge in cur_top.get_all_edges(): available_bandwidth = edge[\"bps_capacity\"] - edge[\"bps_reserved\"] try: temp = distance[edge[\"src_node_id\"]] + 1/available_bandwidth except ZeroDivisionError: continue if (distance[edge[\"dst_node_id\"]] > temp and available_bandwidth >= required_bandwidth): distance[edge[\"dst_node_id\"]] = temp parent[edge[\"dst_node_id\"]] = { \"dst_node_id\": edge[\"dst_node_id\"], \"dst_port\": edge[\"dst_port\"], \"src_node_id\": edge[\"src_node_id\"], \"src_port\": edge[\"src_port\"] } return { \"distance\": distance, \"parent\": parent }"}
{"text_id": "9208", "text": "docstring: def sync_queues(self): for r in self.rooms: songs = get_queue_songs(r.id) for song in songs: song_obj = create_song_object(r, song) if self.music_manager is not None: if not self.music_manager.is_downloaded(song_obj.id): res = self.music_manager.download_song(song_obj.id) print(\"Downlaoded on master: {}\".format(res)) if song_obj.id not in r.downloaded_songs: res = song_obj.download() message = res['message'] if \"already cached\" in message: print(\"Song downloaded already\") r.downloaded_songs.append(song_obj.id) else: print(\"Download to the slave: {}\".format(res))"}
{"text_id": "9209", "text": "docstring: def _vocab_to_id(self, sequence_vocabs, dict_map): vocab_id_list = \\ [dict_map.get(x, self.VOCAB_UNKNOWN) for x in sequence_vocabs] if not vocab_id_list: vocab_id_list.append(self.VOCAB_PADDING) return vocab_id_list"}
{"text_id": "9210", "text": "docstring: def gap_fill(input_array): input_blank = input_array.copy() input_array[input_array<0]=np.nan input_blank[input_blank>=0]=0 input_blank[input_blank<0]=1 structure = np.ones((3, 3), dtype=np.int) labelled, ncomponents = label(input_blank, structure) for the_label in np.unique(labelled): sum_value = [] indices = np.where(labelled==the_label) Xs = indices[0] if len(Xs) > np.shape(input_array)[0]*np.shape(input_array)[1]*0.5: continue Ys = indices[1] for ii in np.arange(0,len(Xs)): sum_value.append(surrounding_mean(Xs[ii],Ys[ii],input_array)) input_array[labelled == the_label] = np.nanmean(sum_value) return input_array"}
{"text_id": "9211", "text": "docstring: def play_match_end(): if not SOUND_SUPPORTED: return QtMultimedia.QSound.play(utils.load_resource('sounds/Match End_normalized.wav'))"}
{"text_id": "9212", "text": "docstring: def prepare(self, state): self.logger.debug('Preparing file for export') self.output_dir = self.runner.output_dir exists = os.path.exists(self.outpath) with hdf.File(self.outpath, 'a', libver='latest') as hf: hf.attrs.update(self.get_info()) if not exists: save_snapshot(self.outpath, snapshot=state, compression=self._compression) self.logger.debug('Preparation done')"}
{"text_id": "9213", "text": "docstring: def _init_nd_shape_and_axes(x, shape, axes): x = asarray(x) noshape = shape is None noaxes = axes is None if noaxes: axes = arange(x.ndim, dtype=intc) else: axes = atleast_1d(axes) if axes.size == 0: axes = axes.astype(intc) if not axes.ndim == 1: raise ValueError(\"when given, axes values must be a scalar or vector\") if not issubdtype(axes.dtype, integer): raise ValueError(\"when given, axes values must be integers\") axes = where(axes < 0, axes + x.ndim, axes) if axes.size != 0 and (axes.max() >= x.ndim or axes.min() < 0): raise ValueError(\"axes exceeds dimensionality of input\") if axes.size != 0 and unique(axes).shape != axes.shape: raise ValueError(\"all axes must be unique\") if not noshape: shape = atleast_1d(shape) elif isscalar(x): shape = array([], dtype=intc) elif noaxes: shape = array(x.shape, dtype=intc) else: shape = take(x.shape, axes) if shape.size == 0: shape = shape.astype(intc) if shape.ndim != 1: raise ValueError(\"when given, shape values must be a scalar or vector\") if not issubdtype(shape.dtype, integer): raise ValueError(\"when given, shape values must be integers\") if axes.shape != shape.shape: raise ValueError(\"when given, axes and shape arguments\" \" have to be of the same length\") shape = where(shape == -1, array(x.shape)[axes], shape) if shape.size != 0 and (shape < 1).any(): raise ValueError( \"invalid number of data points ({0}) specified\".format(shape)) return shape, axes"}
{"text_id": "9214", "text": "docstring: def split_by_sensor_id(list_id_value, state): if not list_id_value: avg_list = [[], []] next_state = state return (avg_list, next_state) n_list, cum_list = state avg_list, next_n_list, next_cum_list = list(), list(), list() list_id_value = np.array(list_id_value) for id in [0,1]: values = list_id_value[np.where( list_id_value[:,0]==id)][:,1] b = np.zeros(len(values)+1) b[0] = cum_list[id] b[1:] = values b = np.cumsum(b) n_array = np.arange(n_list[id], n_list[id]+len(b), 1) avg = b[1:]/np.rint(n_array[1:]) avg_list.append(avg) next_n_list.append(n_array[-1]) next_cum_list.append(b[-1]) next_state = (next_n_list, next_cum_list) return (avg_list, next_state)"}
{"text_id": "9215", "text": "docstring: def switch_child(wanted_object, object_type, **kwargs): child_list = None if object_type == \"function\": child_list = list(get_child_name_list(wanted_object, kwargs['xml_function_list'])) elif object_type == \"state\": child_list = list(get_child_name_list(wanted_object, kwargs['xml_state_list'])) elif object_type == \"Functional element\": child_list = list(get_child_name_list(wanted_object, kwargs['xml_fun_elem_list'])) for allocated_fun in wanted_object.allocated_function_list: for fun in kwargs['xml_function_list']: if fun.id == allocated_fun: child_list.append((fun.name, \"Function allocation\")) for allocated_state in wanted_object.allocated_state_list: for state in kwargs['xml_state_list']: if state.id == allocated_state: child_list.append((state.name, \"State allocation\")) if child_list: child_dict = {'title': f\"Child list for {wanted_object.name}:\", 'data': list(tuple(sorted(child_list))), 'columns': [\"Object's name\", \"Relationship's type\"]} return child_dict"}
{"text_id": "9216", "text": "docstring: def playagain(): if input(\"Would you like to play again (Yes/No)? \").lower().startswith(\"y\"): main() else: print(\"Leaving so soon, we will miss you\", \"\\U0001F97A\")"}
{"text_id": "9217", "text": "docstring: def wait_for_balance_update(start_balance, timeout): start_time = time.time() success = False while True and (time.time() < start_time + timeout): new_balance = get_lnurl_balance() if start_balance == new_balance: print(\"Balance: \" + str(start_balance) + \" (no changes)\") time.sleep(3) else: print( \"Balance: \" + str(start_balance) + \" | New Balance:\" + str(new_balance) ) success = True break return success"}
{"text_id": "9218", "text": "docstring: def heading(value): try: value = float(value) except ValueError: raise ArgumentTypeError(\"{} is not a number\".format(value)) if value < 0.0 or value > 360.0: raise ArgumentTypeError(\"Invalid heading\") return value"}
{"text_id": "9219", "text": "docstring: def calculate_scores(self): for bot_pair in self.interactions: self.interaction_scores[bot_pair] = [] for meeting in self.interactions[bot_pair]: meeting_scores = [0, 0] for turn in meeting: turn_scores = self.score_turn(turn) meeting_scores[0] += turn_scores[0] meeting_scores[1] += turn_scores[1] meeting_scores = tuple(meeting_scores) self.interaction_scores[bot_pair].append(meeting_scores) if bot_pair[0] == bot_pair[1]: self.bot_info_by_id[bot_pair[0]]['total']\\ += meeting_scores[0] else: for idx, bot_id in enumerate(bot_pair): self.bot_info_by_id[bot_id]['total']\\ += meeting_scores[idx]"}
{"text_id": "9220", "text": "docstring: def positions_process(self) -> None: pass"}
{"text_id": "9221", "text": "docstring: def delete_task(self, id, **kwargs): \"\"\"Delete a single tasks This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async=True >>> thread = api.delete_task(id, async=True) >>> result = thread.get() :param async bool :param str id: The unique identifer for an Object (i.e. User, Task, Project, Submission etc) (required) :return: None If the method is called asynchronously, returns the request thread. \"\"\" kwargs['_return_http_data_only'] = True if kwargs.get('async'): return self.delete_task_with_http_info(id, **kwargs) else: (data) = self.delete_task_with_http_info(id, **kwargs) return data"}
{"text_id": "9222", "text": "docstring: def dcm_2_nifti(input_folder, output_folder, verbose=True, naming_tags=['SeriesDescription'], folder_tags=['PatientID', 'StudyDate'], folder_mode='combine', prefix='', suffix='', write_header=False, header_suffix='_header', harden_orientation=True): if verbose: print 'Searching for dicom files...' found_files = grab_files_recursive(input_folder) if verbose: print 'Found', len(found_files), 'in directory. \\n' print 'Checking DICOM compatability...' dicom_files = [] for file in found_files: try: temp_dicom = pydicom.read_file(file) dicom_files += [[file, temp_dicom.data_element('SeriesInstanceUID').value]] except: continue if verbose: print 'Found', len(dicom_files), 'DICOM files in directory. \\n' print 'Counting volumes..' dicom_headers = [] unique_dicoms = defaultdict(list) for dicom_file in dicom_files: UID = dicom_file[1] unique_dicoms[UID] += [dicom_file[0]] if verbose: print 'Found', len(unique_dicoms.keys()), 'unique volumes \\n' print 'Saving out files from these volumes.' output_dict = {} output_filenames = [] for UID in unique_dicoms.keys(): try: current_files = unique_dicoms[UID] current_dicoms = [get_uncompressed_dicom(dcm) for dcm in unique_dicoms[UID]] dicom_instances = [x.data_element('InstanceNumber').value for x in current_dicoms] current_dicoms = [x for _,x in sorted(zip(dicom_instances,current_dicoms))] current_files = [x for _,x in sorted(zip(dicom_instances,current_files))] first_dicom, last_dicom = current_dicoms[0], current_dicoms[-1] print first_dicom.file_meta print first_dicom.file_meta.TransferSyntaxUID volume_label = '_'.join([first_dicom.data_element(tag).value for tag in naming_tags]).replace(\" \", \"\") volume_label = prefix + sanitize_filename(volume_label) + suffix + '.nii.gz' if verbose: print 'Saving...', volume_label except: print 'Could not read DICOM volume SeriesDescription. Skipping UID...', str(UID) continue try: output_affine = np.eye(4) image_position_patient = np.array(first_dicom.data_element('ImagePositionPatient').value).astype(float) image_orientation_patient = np.array(first_dicom.data_element('ImageOrientationPatient').value).astype(float) last_image_position_patient = np.array(last_dicom.data_element('ImagePositionPatient').value).astype(float) pixel_spacing_patient = np.array(first_dicom.data_element('PixelSpacing').value).astype(float) output_affine[0:3, 0] = pixel_spacing_patient[0] * image_orientation_patient[0:3] output_affine[0:3, 1] = pixel_spacing_patient[1] * image_orientation_patient[3:6] output_affine[0:3, 2] = (image_position_patient - last_image_position_patient) / (1 - len(current_dicoms)) output_affine[0:3, 3] = image_position_patient cr_flip = np.eye(4) cr_flip[0:2,0:2] = [[0,1],[1,0]] neg_flip = np.eye(4) neg_flip[0:2,0:2] = [[-1,0],[0,-1]] output_affine = np.matmul(neg_flip, np.matmul(output_affine, cr_flip)) output_shape = get_dicom_pixel_array(current_dicoms[0], current_files[0]).shape output_numpy = [] for i in xrange(len(current_dicoms)): try: output_numpy += [get_dicom_pixel_array(current_dicoms[i], current_files[i])] except: print 'Warning, error at slice', i output_numpy = np.stack(output_numpy, -1) if harden_orientation is not None: cx, cy, cz = np.argmax(np.abs(output_affine[0:3,0:3]), axis=0) output_numpy = np.transpose(output_numpy, (cx,cy,cz)) harden_matrix = np.eye(4) for dim, i in enumerate([cx,cy,cz]): harden_matrix[i,i] = 0 harden_matrix[dim, i] = 1 output_affine = np.matmul(output_affine, harden_matrix) flip_matrix = np.eye(4) for i in xrange(3): if output_affine[i,i] < 0: flip_matrix[i,i] = -1 output_numpy = np.flip(output_numpy, i) output_affine = np.matmul(output_affine, flip_matrix) specific_folder = output_folder for tag in folder_tags: if specific_folder == output_folder or folder_mode == 'recursive': specific_folder = os.path.join(specific_folder, sanitize_filename(first_dicom.data_element(tag).value)) elif folder_mode == 'combine': specific_folder = specific_folder + '_' + sanitize_filename(first_dicom.data_element(tag).value) if not os.path.exists(specific_folder): os.makedirs(specific_folder) output_filename = os.path.join(specific_folder, volume_label) if os.path.exists(output_filename) and output_filename in output_filenames: output_filename = replace_suffix(output_filename, '', '_copy') save_numpy_2_nifti(output_numpy, output_affine, output_filename) output_filenames += [output_filename] except: print 'Could not read DICOM at SeriesDescription...', volume_label return output_filenames"}
{"text_id": "9223", "text": "docstring: def path(key, *path): return ':'.join([key] + list(path))"}
{"text_id": "9224", "text": "docstring: def find_roots(a: int, b: int, c: int): if a == 0: x = -c / b print(f\"Solution is {x}\") return x d = b ** 2 - (4 * a * c) if d < 0: print(\"No solutions\") return None elif d == 0: x = (-b + sqrt(d)) / (2 * a) print(f\"Solution is {x}\") return x x1 = (-b + sqrt(d)) / (2 * a) x2 = (-b - sqrt(d)) / (2 * a) print(f\"Solutions are {x1} and {x2}\") return x1, x2"}
{"text_id": "9225", "text": "docstring: def target_and_mask(self): seq_mask = tf.sequence_mask([len(sent_tokens) for sent_tokens in self.tokens], constants.MAX_TOKENS) for sent_tokens, sentence_mask in zip(self.tokens, tf.unstack(seq_mask)): sentence_mask = tf.cast(sentence_mask, tf.float32) sentence_mask = tf.expand_dims(sentence_mask, 1) sentence_mask = sentence_mask * tf.transpose(sentence_mask) sentence_length = min(len(sent_tokens), constants.MAX_TOKENS) sentence_distances = np.zeros((constants.MAX_TOKENS, constants.MAX_TOKENS), dtype=np.float32) for i in range(sentence_length): for j in range(i, sentence_length): i_j_distance = j - i sentence_distances[i, j] = i_j_distance sentence_distances[j, i] = i_j_distance yield tf.constant(sentence_distances, dtype=tf.float32), sentence_mask"}
{"text_id": "9226", "text": "docstring: def load_mask(self, image_id): info = self.image_info[image_id] mask_image_path = info['path'].replace(\"images\", \"masks\") mask = cv2.imread(mask_image_path) mask = (np.max(mask, axis=2) if len(mask.shape) > 2 else mask).reshape((128,128,1)) return mask, np.array([1,])"}
{"text_id": "9227", "text": "docstring: def label_to_colors( img, colormap=px.colors.qualitative.Light24, alpha=128, color_class_offset=0, labels_contiguous=False, no_map_zero=False, ): colormap = [ tuple([fromhex(h[s : s + 2]) for s in range(0, len(h), 2)]) for h in [c.replace(\"#\", \"\") for c in colormap] ] if type(alpha) is not type(list()): alpha = [alpha] cm_alpha = list(zip(colormap, itertools.cycle(alpha))) cimg = np.zeros(img.shape + (3,), dtype=\"uint8\") alpha = np.zeros(img.shape + (1,), dtype=\"uint8\") if labels_contiguous: labels = range(img.min(), img.max() + 1) else: labels = set(img.flatten()) for c in labels: if (c == 0) and no_map_zero: continue cimg[img == c], alpha[img == c] = cm_alpha[ (c + color_class_offset) % len(colormap) ] return np.concatenate((cimg, alpha), axis=len(cimg.shape) - 1)"}
{"text_id": "9228", "text": "docstring: async def _landing_url(self, responses: SourceResponses) -> URL: api_url = await self._api_url() board_id = parse_qs(urlparse(str(responses.api_url)).query)[\"rapidViewId\"][0] return URL(f\"{api_url}/secure/RapidBoard.jspa?rapidView={board_id}&view=reporting&chart=velocityChart\")"}
{"text_id": "9229", "text": "docstring: def type_(): __get_method = _get(raw_data, method, {}) assigned_type = _get(__get_method, \"type\") if assigned_type: if not isinstance(assigned_type, dict): return assigned_type return list(iterkeys(assigned_type))[0] assigned_type = _get(raw_data, \"type\") if isinstance(assigned_type, dict): return list(iterkeys(assigned_type))[0] return assigned_type"}
{"text_id": "9230", "text": "docstring: def to_14C_explicit(self, start_values_14C, Fa_func, decay_rate=0.0001209681): srm_14C = self.model.to_14C_explicit('lamda_14C', 'Fa_14C') par_set_14C = {k:v for k, v in self.parameter_dict.items()} par_set_14C['lamda_14C'] = decay_rate nr_pools = self.nr_pools start_values_14C_cb = np.ones(nr_pools*2) start_values_14C_cb[:nr_pools] = self.start_values start_values_14C_cb[nr_pools:] = start_values_14C times_14C = self.times func_set_14C = {k:v for k,v in self.func_set.items()} function_string = 'Fa_14C(' + srm_14C.time_symbol.name + ')' func_set_14C[function_string] = Fa_func smr_14C = SmoothModelRun( srm_14C, par_set_14C, start_values_14C_cb, times_14C, func_set_14C, ) return smr_14C"}
{"text_id": "9231", "text": "docstring: def wrap_with_color(code): def inner(text, bold=False): c = code if os.environ.get('DISABLE_COLORS') or os.environ.get('FABRIC_DISABLE_COLORS'): return text if bold: c = \"1;%s\" % c return \"\\033[%sm%s\\033[0m\" % (c, text) return inner"}
{"text_id": "9232", "text": "docstring: def absolute_date(resolution, start, end): if resolution not in ('minutes' 'hours' 'days' 'weeks' 'months' 'years'): raise ValueError('Invalid date resolution: %s' % resolution) payload = {resolution: {'start': start, 'end': end}} return payload"}
{"text_id": "9233", "text": "docstring: def listen(self, protocolFactory): return defer.execute(self._reactor.listenTCP, self._port, protocolFactory, backlog=self._backlog, interface=self._interface)"}
{"text_id": "9234", "text": "docstring: def convert_from_latlon_to_utm(points=None, latitudes=None, longitudes=None, false_easting=None, false_northing=None): old_geo = Geo_reference() utm_points = [] if points == None: assert len(latitudes) == len(longitudes) points = map(None, latitudes, longitudes) for point in points: zone, easting, northing = redfearn(float(point[0]), float(point[1]), false_easting=false_easting, false_northing=false_northing) new_geo = Geo_reference(zone) old_geo.reconcile_zones(new_geo) utm_points.append([easting, northing]) return utm_points, old_geo.get_zone()"}
{"text_id": "9235", "text": "docstring: def add_done_callback(self, fn): if self.done(): return fn(self) self._callbacks.append(fn)"}
{"text_id": "9236", "text": "docstring: def gather_text_annotations(response): doc = response.text_annotations[1:] gathered_text = Layout() for i, text_comp in enumerate(doc): points = _cvt_GCV_vertices_to_points(text_comp.bounding_poly.vertices) gathered_text.append( TextBlock( block = Quadrilateral(points), text = text_comp.description, id = i ) ) return gathered_text"}
{"text_id": "9237", "text": "docstring: def insert_sql(cls): names = [] auto_increment_field_name = '' for k, sql_columns_by_type in cls.sql_columns().items(): for sql_column in sql_columns_by_type: if( sql_column.sql_column_type !=Sql.ColumnType.AUTO_INCREMENT ): names.append(sql_column._name) insert_sql = ( cls._database_engine.insert.value.format( cls.__name__.lower() ,', '.join(names) ,( (str( cls._database_engine.value_place_holder.value)+',') * len(names) ) ) ) insert_sql = re.sub(',\\s*\\)', ' )', insert_sql) return insert_sql"}
{"text_id": "9238", "text": "docstring: def mask_and_mean_loss(input_tensor, binary_tensor, axis=None): return mean_on_masked(mask_loss(input_tensor, binary_tensor), binary_tensor, axis=axis)"}
{"text_id": "9239", "text": "docstring: def parseXML(self, xml): paramInput = self.getInputSpecification(xml=xml)() paramInput.parseNode(xml) return paramInput"}
{"text_id": "9240", "text": "docstring: def run(self, limit=None, shuffle=True) -> pd.DataFrame: if limit is None: limit = len(self.behave) if shuffle: indices = self.behave.sample(n=limit, replace=False).index.tolist() else: indices = self.behave.index.tolist()[:limit] self.df_scores = pd.DataFrame(columns=['behave_index', 'user_id', 'nDCG - baseline', 'nDCG - tfidf', 'nDCG%5 - baseline', 'nDCG%5 - tfidf', 'nDCG%10 - baseline', 'nDCG%10 - tfidf']) for i in tqdm(indices, desc='Running TF-IDF on users', total=limit): labels = self.labels.iloc[i] user_id = self.behave.iloc[i][\"user_id\"] user_vector = self.get_user_vector(i, csr=True) user_ranking = self.get_user_ranking(i, user_vector, labels) impression_index = self.behave.iloc[i]['impressions'] true_score = [np.array(labels)] rank_base = pd.DataFrame(impression_index, columns=['impression_ind']) rank_base[\"labels\"] = np.array(labels) rank_base.sort_values(by=['impression_ind'], ascending=False, inplace=True) pred_baseline = [np.array(rank_base[\"labels\"])] pred_tfidf = [np.array(user_ranking[\"labels\"])] score_baseline = ndcg_score(true_score, pred_baseline) score_tfidf = ndcg_score(true_score, pred_tfidf) score_baseline5 = ndcg_score(true_score, pred_baseline, k=5) score_tfidf5 = ndcg_score(true_score, pred_tfidf, k=5) score_baseline10 = ndcg_score(true_score, pred_baseline, k=10) score_tfidf10 = ndcg_score(true_score, pred_tfidf, k=10) self.df_scores.loc[i] = [i, user_id, score_baseline, score_tfidf, score_baseline5, score_tfidf5, score_baseline10, score_tfidf10] self.df_scores.to_csv(f'{self.result_path}/tfidf_scores_{self.filename}.csv', index=False) return self.df_scores"}
{"text_id": "9241", "text": "docstring: def build_graph(self): js_obj = json.loads(self._config_text) filter_array = js_obj.get('filters') for filt in filter_array: module_path = filt['module_path'] class_name = filt['class_name'] instance_name = filt['instance_name'] config = filt['config'] filter_instance = self._graph_mgr.filter_factory(module_path, class_name, instance_name, config) self._graph_mgr.add_filter(filter_instance) conn_array = js_obj.get('pin_connections') for conn in conn_array: source_filter = conn['source_filter'] source_pin = conn['source_pin'] target_filter = conn['target_filter'] target_pin = conn['target_pin'] self._graph_mgr.connect_pins(source_filter, source_pin, target_filter, target_pin)"}
{"text_id": "9242", "text": "docstring: def fork_node_link(self, node_relation, auth, save=True): try: node = self.node_relations.get(is_node_link=True, id=node_relation.id).child except NodeRelation.DoesNotExist: raise ValueError('Node link {0} not in list'.format(node_relation._id)) forked = node.fork_node(auth) if forked is None: raise ValueError('Could not fork node') relation = NodeRelation.objects.get( parent=self, child=node, is_node_link=True ) relation.child = forked relation.save() if hasattr(self, 'add_log'): self.add_log( NodeLog.NODE_LINK_FORKED, params={ 'parent_node': self.parent_id, 'node': self._id, 'pointer': { 'id': node._id, 'url': node.url, 'title': node.title, 'category': node.category, }, }, auth=auth, save=False, ) if save: self.save() return forked"}
{"text_id": "9243", "text": "docstring: def mkurl(self, endpoint: Union[str, Callable], *args, **kwargs) -> str: return self.router.mkurl(endpoint, *args, **kwargs)"}
{"text_id": "9244", "text": "docstring: def check_analysis_access(user, analysis_id): if analysis_id not in Analysis.get_public() + user.shared_analyses + \\ user.private_analyses: raise HTTPError(403, \"Analysis access denied to %s\" % (analysis_id))"}
{"text_id": "9245", "text": "docstring: def generate(self, x, max_length=10000, stop_threshold=-0.2): h = self.encoder(x) B, T, _ = h.size() alpha = F.one_hot(torch.zeros(B, dtype=torch.long, device=x.device), T).float() c = torch.zeros(B, self.input_size, device=x.device) attn_hx = ( torch.zeros(B, self.attn_rnn_size, device=x.device), torch.zeros(B, self.attn_rnn_size, device=x.device), ) rnn1_hx = ( torch.zeros(B, self.decoder_rnn_size, device=x.device), torch.zeros(B, self.decoder_rnn_size, device=x.device), ) rnn2_hx = ( torch.zeros(B, self.decoder_rnn_size, device=x.device), torch.zeros(B, self.decoder_rnn_size, device=x.device), ) go_frame = torch.zeros(B, self.n_mels, device=x.device) ys, alphas = [], [] for t in range(0, max_length, self.reduction_factor): y = ys[-1][:, :, -1] if t > 0 else go_frame y, alpha, c, attn_hx, rnn1_hx, rnn2_hx = self.decoder_cell( h, y, alpha, c, attn_hx, rnn1_hx, rnn2_hx ) if torch.all(y[:, :, -1] > stop_threshold): break ys.append(y) alphas.append(alpha) ys = torch.cat(ys, dim=-1) alphas = torch.stack(alphas, dim=2) return ys, alphas"}
{"text_id": "9246", "text": "docstring: def init_meta(self): super().init_meta() self.meta['decker'] = dict(ext=0, card='HIERARCH ESO INS OPTI3 NAME')"}
{"text_id": "9247", "text": "docstring: def buildFromWindow(self, window): self.layout = window.layout() self.folders = window.folders() self.active = window.get_view_index(window.active_view()) for i in range(window.num_groups()): sViews = window.views_in_group(i) activeSView = window.active_view_in_group(i) group = dict() group['active'] = sViews.index(activeSView) if sViews else 0 group['views'] = [] for sView in sViews: fileExistsOnDisk = sView.file_name() if not fileExistsOnDisk: continue view = dict() view['file'] = sView.file_name() view['visible'] = (sView.visible_region().a, sView.visible_region().b) view['selection'] = (sView.sel()[0].a, sView.sel()[0].b) view['read_only'] = sView.is_read_only() group['views'].append(view) self.groups.append(group)"}
{"text_id": "9248", "text": "docstring: def on_mouse_press(f): window.on_mouse_pressed = f"}
{"text_id": "9249", "text": "docstring: def validate_response(response): if response.error_code != MoveItErrorCodes.SUCCESS: raise RosError(response.error_code.human_readable, int(response.error_code))"}
{"text_id": "9250", "text": "docstring: def cli(command, name, rename, description, quality_status, junit_file, file, filter, external_url, external_url_label, lock): rest_crud.set_current_command() if not command: print(\"command is mandatory. Please see neoload tests-results --help\") return rest_crud.set_current_sub_command(command) if name == \"cur\": name = user_data.get_meta(meta_key) is_id = tools.is_id(name) if command == \"ls\": tools.ls(name, is_id, __resolver, filter, ['project', 'status', 'author', 'sort']) return __id = tools.get_id(name, __resolver, is_id) if command == \"use\": tools.use(__id, meta_key, __resolver) return if not __id: __id = user_data.get_meta_required(meta_key) system_exit = {'message': '', 'code': 0} if command == \"summary\": system_exit = summary(__id) elif command == \"junitsla\": junit(__id, junit_file) elif command == \"delete\": delete(__id) user_data.set_meta(meta_key, None) else: json_data = load_from_file(file) if file else create_json(rename, description, quality_status, external_url, external_url_label, lock) print_compatibility_warning_for_old_nlw(json_data) if command == \"put\": put(__id, json_data) elif command == \"patch\": patch(__id, json_data) if command != \"delete\": user_data.set_meta(meta_key, __id) tools.system_exit(system_exit)"}
{"text_id": "9251", "text": "docstring: def build_job_script(self, statement): tmpfilename = get_temp_filename(dir=self.work_dir, clear=True) tmpfilename = tmpfilename + \".sh\" expanded_statement, cleanup_funcs = self.expand_statement(statement) with open(tmpfilename, \"w\") as tmpfile: tmpfile.write(\"#!/bin/bash -eu\\n\") if not self.ignore_pipe_errors: tmpfile.write(\"set -o pipefail\\n\") os.chmod(tmpfilename, stat.S_IRWXG | stat.S_IRWXU) tmpfile.write(\"\\ncd {}\\n\".format(self.work_dir)) if self.output_directories is not None: for outdir in self.output_directories: if outdir: tmpfile.write(\"\\nmkdir -p {}\\n\".format(outdir)) tmpfile.write(\"umask 002\\n\") cluster_tmpdir = get_params()[\"cluster_tmpdir\"] if self.run_on_cluster and cluster_tmpdir: tmpdir = cluster_tmpdir tmpfile.write(\"TMPDIR=`mktemp -d -p {}`\\n\".format(tmpdir)) tmpfile.write(\"export TMPDIR\\n\") else: tmpdir = get_temp_dir(dir=get_params()[\"tmpdir\"], clear=True) tmpfile.write(\"mkdir -p {}\\n\".format(tmpdir)) tmpfile.write(\"export TMPDIR={}\\n\".format(tmpdir)) cleanup_funcs.append( (\"clean_temp\", \"{{ rm -rf {}; }}\".format(tmpdir))) cleanup_funcs.append((\"info\", \"{ echo 'benchmark'; hostname; times; }\")) for cleanup_func, cleanup_code in cleanup_funcs: tmpfile.write(\"\\n{}() {}\\n\".format(cleanup_func, cleanup_code)) tmpfile.write(\"\\nclean_all() {{ {}; }}\\n\".format( \"; \".join([x[0] for x in cleanup_funcs]))) tmpfile.write(\"\\ntrap clean_all EXIT\\n\\n\") if self.job_memory not in(\"unlimited\", \"etc\") and \\ self.options.get(\"cluster_memory_ulimit\", False): requested_memory_kb = max( 1000, int(math.ceil( iotools.human2bytes(self.job_memory) / 1024 * self.job_threads))) tmpfile.write(\"set +e\\n\") tmpfile.write(\"ulimit -v {} > /dev/null \\n\".format( requested_memory_kb)) tmpfile.write(\"ulimit -m {} > /dev/null \\n\".format( requested_memory_kb)) tmpfile.write(\"ulimit -H -v > /dev/null \\n\") tmpfile.write(\"set -e\\n\") if self.shellfile: tmpfile.write(\"mkdir -p $(dirname \\\"{}\\\")\\n\".format( self.shellfile)) tmpfile.write( 'echo \"%s : START -> %s\" >> %s\\n' % (self.job_name, tmpfilename, self.shellfile)) tmpfile.write(\"set | sed 's/^/%s : /' >> %s\\n\" % (self.job_name, self.shellfile)) tmpfile.write(\"pwd | sed 's/^/%s : /' >> %s\\n\" % (self.job_name, self.shellfile)) tmpfile.write(\"hostname | sed 's/^/%s: /' >> %s\\n\" % (self.job_name, self.shellfile)) if get_params()['os'] == 'Linux': tmpfile.write(\"cat /proc/meminfo | sed 's/^/%s: /' >> %s\\n\" % (self.job_name, self.shellfile)) elif get_params()['os'] == 'Darwin': tmpfile.write(\"vm_stat | sed 's/^/%s: /' >> %s\\n\" % (self.job_name, self.shellfile)) tmpfile.write( 'echo \"%s : END -> %s\" >> %s\\n' % (self.job_name, tmpfilename, self.shellfile)) tmpfile.write(\"ulimit | sed 's/^/%s: /' >> %s\\n\" % (self.job_name, self.shellfile)) job_path = os.path.abspath(tmpfilename) tmpfile.write(expanded_statement) tmpfile.write(\"\\n\\n\") tmpfile.close() return statement, job_path"}
{"text_id": "9252", "text": "docstring: def _mmc_loop(self, rounds, temp=298.15, verbose=True): current_round = 0 while current_round < rounds: modifiable = list(filter( lambda p: p.parameter_type is not MMCParameterType.STATIC_VALUE, self.current_parameters)) chosen_parameter = random.choice(modifiable) if chosen_parameter.parameter_type is MMCParameterType.UNIFORM_DIST: chosen_parameter.randomise_proposed_value() else: chosen_parameter.randomise_proposed_value() proposed_parameters = [ p.current_value if p.proposed_value is None else p.proposed_value for p in self.current_parameters] model = self.specification(*proposed_parameters) model.pack_new_sequences(self.sequences) proposed_energy = self.eval_function(model) if verbose: sys.stdout.write( '\\rRound: {}, Current energy: {}, Proposed energy: {} ' '(best {}), {}. ' .format(current_round, float_f(self.current_energy), float_f(proposed_energy), float_f( self.best_energy), \"ACCEPTED\" if self.check_move( proposed_energy, self.current_energy, t=temp) else \"DECLINED\" )) sys.stdout.flush() if self.check_move(proposed_energy, self.current_energy, t=temp): for p in self.current_parameters: p.accept_proposed_value() self.current_energy = proposed_energy if self.current_energy < self.best_energy: self.best_energy = copy.deepcopy(self.current_energy) self.best_parameters = copy.deepcopy( self.current_parameters) self.best_model = model else: for p in self.current_parameters: p.reject_proposed_value() current_round += 1 return"}
{"text_id": "9253", "text": "docstring: def vt_ipv4_bytes(self, str): ipv4_sp = str.split(\".\") ipv4_dec = [] for i in range(len(ipv4_sp)): ipv4_dec.append(int(ipv4_sp[i])) return ipv4_dec"}
{"text_id": "9254", "text": "docstring: def _on_toggle(self, change): name = change['owner'].key show = change['new'] if show: EphyviewerConfigurator.show(self, name) else: EphyviewerConfigurator.hide(self, name)"}
{"text_id": "9255", "text": "docstring: def etl_tracks(): with xa(DATABASE_URL) as session: n = 0 for f in session.query(Float).order_by(Float.id): prev_lon = 0 ls = [] ps = [] np = 0 for p in f.points: if p.lon != -999 and p.lat != -99: np += 1 lat, lon = float(p.lat), float(p.lon) if (prev_lon < -90 and lon > 90) or (prev_lon > 90 and lon < -90): if len(ps)==1: ps = ps + ps ls += [ps] ps = [] ps.append('%.6f %.6f' % (lon, lat)) prev_lon = lon if len(ps) == 1: ps = ps + ps ls += [ps] ls = 'MULTILINESTRING(%s)' % ','.join(['(%s)' % ','.join(ps) for ps in ls]) print '%d: Float %ld %d points' % (n, f.id, np) f.track = ls n += 1 if n % 100 == 0: session.commit()"}
{"text_id": "9256", "text": "docstring: def sum(self): metricSummarizers = {} for metric, config in self.config.metrics.items(): metricSummarizers[metric] = config.instanceSummarizers() values = {} for metric, series in self.timeseries.items(): sums = {} for summarizer in metricSummarizers[metric]: sums[summarizer.name] = summarizer.calculate(series, self.parameters) values[metric] = sums return values"}
{"text_id": "9257", "text": "docstring: def text(update: Update, context: callbackcontext): chat = update.effective_chat if not update.message: return entities = update.message.parse_entities([MessageEntity.MENTION]) if not (chat and isMessageFromAGroup(chat.type) and DictHasElems(entities)): return mention = getMentions(entities, MessageEntity.MENTION) telegramUserId = shouldProcessImage(mention, context.bot_data, context.chat_data) message = update.effective_message if not(telegramUserId and message and validMessageLength(message.text, mention)): return context.bot.sendChatAction( chat_id = update.effective_chat.id, action = ChatAction.UPLOAD_PHOTO) userProfilePic = context.bot.getUserProfilePhotos(telegramUserId, limit = 1) resultImage = processImage(userProfilePic, message.text, mention) if(resultImage): update.message.reply_photo(photo=resultImage,) else: context.bot.sendMessage( chat_id = update.effective_chat.id, text = (\"Imagine this is the profile \" + f\"picture of {mention} with the text \" + \"from the message I replied (?) Sorry\" + \"but that user privacy settings \" + \"doesn't allow me to use his \" + \"profile picture \ud83d\ude05\"), reply_to_message_id = message.message_id)"}
{"text_id": "9258", "text": "docstring: def ssm_sendcommand(instance_id, doc_name, params): try: response = ssm_client.send_command( InstanceIds=[instance_id], DocumentName=str(doc_name), Comment='helmHelperLambda triggered this', Parameters=params ) return response['Command'] except Exception as err: logger.error('SSM Send Command error - \"{type}\": \"{message}\"'.format(type=type(err), message=str(err))) return err"}
{"text_id": "9259", "text": "docstring: def _compute_random_hashes(self): if not os.path.exists(self.src_path) or os.path.isdir(self.src_path) or self.maintype == 'image': return self.random_hashes = [] if self.size < 64: self.block_length = self.size else: if self.size < 128: self.block_length = random.randint(16, self.size) else: self.block_length = random.randint(16, 128) for i in range(random.randint(3, 6)): start_pos = random.randint(0, self.size - self.block_length) with open(self.src_path, 'rb') as f: f.seek(start_pos) hashed = hashlib.sha256(f.read(self.block_length)).hexdigest() self.random_hashes.append((start_pos, hashed)) time.sleep(random.uniform(0.1, 0.5))"}
{"text_id": "9260", "text": "docstring: def count(self,code,vendor_id=0): c=0 for a in self.avp: if a.code==code and a.vendor_id==vendor_id: c += 1 return c"}
{"text_id": "9261", "text": "docstring: def download_osm(left,bottom,right,top): from urllib import urlopen fp = urlopen( \"http://api.openstreetmap.org/api/0.6/map?bbox=%f,%f,%f,%f\"%(left,bottom,right,top) ) return fp"}
{"text_id": "9262", "text": "docstring: def insert_unary( self, request: Union[compute.InsertRegionUrlMapRequest, dict] = None, *, project: str = None, region: str = None, url_map_resource: compute.UrlMap = None, retry: OptionalRetry = gapic_v1.method.DEFAULT, timeout: float = None, metadata: Sequence[Tuple[str, str]] = (), ) -> compute.Operation: has_flattened_params = any([project, region, url_map_resource]) if request is not None and has_flattened_params: raise ValueError( \"If the `request` argument is set, then none of \" \"the individual field arguments should be set.\" ) if not isinstance(request, compute.InsertRegionUrlMapRequest): request = compute.InsertRegionUrlMapRequest(request) if project is not None: request.project = project if region is not None: request.region = region if url_map_resource is not None: request.url_map_resource = url_map_resource rpc = self._transport._wrapped_methods[self._transport.insert] response = rpc(request, retry=retry, timeout=timeout, metadata=metadata,) return response"}
{"text_id": "9263", "text": "docstring: def parse_biothings_post_res(self): new_res = {} for _res in self.response: if not isinstance(_res, dict): continue if _res.get('notfound'): if _res['query'] in new_res: continue new_res[_res['query']] = {} else: if self.api[:4] in ['semm', 'cord']: transformed_json = _res else: transformed_json = Transformer(_res, self.mapping).transform() if _res['query'] not in new_res: new_res[_res['query']] = transformed_json else: if isinstance(transformed_json, dict): for k, v in transformed_json.items(): if k in [\"@context\", \"@type\"]: new_res[_res['query']][k] = v else: if k not in new_res[_res['query']]: new_res[_res['query']][k] = [] if isinstance(v, list): new_res[_res['query']][k] += v else: new_res[_res['query']][k].append(v) return dict(new_res)"}
{"text_id": "9264", "text": "docstring: def loss(loss_func, program=None, **kwargs): if program == None: program = paddle.static.default_main_program() func_parameters = {} for item in kwargs.items(): if isinstance(item[1], str): func_parameters.setdefault(item[0], program.global_block().var(item[1])) else: func_parameters.setdefault(item[0], item[1]) loss = loss_func(**func_parameters) return loss"}
{"text_id": "9265", "text": "docstring: def emulate(self, pc): while pc: opcode = self.Triton.getConcreteMemoryAreaValue(pc, 16) instruction = Instruction() instruction.setOpcode(opcode) instruction.setAddress(pc) self.assertTrue(self.Triton.processing(instruction)) pc = self.Triton.getConcreteRegisterValue(self.Triton.registers.rip) return"}
{"text_id": "9266", "text": "docstring: def start(self): while not self.done: channels = self.read() if channels: self.send(channels) time.sleep(self.period)"}
{"text_id": "9267", "text": "docstring: def build(self) -> pd.DataFrame: self.run_step(\"qc\") self.run_step(\"nc\") self.run_step(\"td\") self.run_step(\"ma\") self.run_step(\"dr\") return self.d_regulation"}
{"text_id": "9268", "text": "docstring: def feedback_vertex_set(self, value_only=False, solver=None, verbose=0, constraint_generation=True, *, integrality_tolerance=1e-3): if not constraint_generation and not self.is_directed(): raise ValueError(\"the only implementation available for \" \"undirected graphs is with constraint_generation \" \"set to True\") if ((not self.is_directed() and self.is_forest()) or ( self.is_directed() and self.is_directed_acyclic())): if value_only: return 0 return [] from sage.numerical.mip import MixedIntegerLinearProgram if constraint_generation: p = MixedIntegerLinearProgram(constraint_generation=True, maximization=False, solver=solver) b = p.new_variable(binary=True) p.set_objective(p.sum(b[v] for v in self)) while True: p.solve(log=verbose) b_val = p.get_values(b, convert=bool, tolerance=integrality_tolerance) h = self.subgraph([v for v in self if not b_val[v]]) if self.is_directed(): isok, certificate = h.is_directed_acyclic(certificate=True) else: isok, certificate = h.is_forest(certificate=True) if isok: if value_only: return Integer(self.order() - h.order()) else: return [v for v in self if b_val[v]] while not isok: p.add_constraint(p.sum(b[v] for v in certificate), min=1) if verbose: print(\"Adding a constraint on circuit: \", certificate) h.delete_vertices(certificate) if self.is_directed(): isok, certificate = h.is_directed_acyclic(certificate=True) else: isok, certificate = h.is_forest(certificate=True) else: p = MixedIntegerLinearProgram(maximization=False, solver=solver) b = p.new_variable(binary=True) d = p.new_variable(integer=True, nonnegative=True) n = self.order() for u,v in self.edge_iterator(labels=None): p.add_constraint(d[u] - d[v] + n * (b[u] + b[v]), min=1) for u in self: p.add_constraint(d[u], max=n) p.set_objective(p.sum(b[v] for v in self)) p.solve(log=verbose) b_sol = p.get_values(b, convert=bool, tolerance=integrality_tolerance) if value_only: return Integer(sum(1 for v in self if b_sol[v])) else: return [v for v in self if b_sol[v]]"}
{"text_id": "9269", "text": "docstring: def search(self, startCoord=None): if self.total_dims == 0: return {} if self.resume: startCoord = self.search_opts.get('start_coord') if not isinstance(startCoord,list): err('%s argument \"%s\" must be a list of coordinate indices' % (self.__class__.__name__,'start_coord')) if not startCoord: startCoord = self.__findLastCoord() best_coord,best_perf,search_time,runs = self.searchBestCoord(startCoord) corr_transfer = self.MAXFLOAT if isinstance(best_perf,tuple): corr_transfer = best_perf[1] best_perf = best_perf[0] if best_coord == None: err ('the search cannot find a valid set of performance parameters. ' + 'the search time limit might be too short, or the performance parameter ' + 'constraints might prune out the entire search space.') else: self.best_coord_info = '%s=%s, cost=%e, transfer_time=%e, inputs=%s, search_space=%1.3e, search_time=%.2f, runs=%d' \\ % (best_coord, self.coordToPerfParams(best_coord), best_perf, corr_transfer, str(self.input_params), \\ self.space_size, search_time, runs) info('----- begin summary -----') info(' best coordinate: %s' % self.best_coord_info) info('----- end summary -----') if not Globals().extern: best_perf_cost = self.getPerfCost(best_coord) best_perf_params = self.coordToPerfParams(best_coord) else: best_perf_cost=0 best_perf_params=Globals().config return (best_perf_params, best_perf_cost)"}
{"text_id": "9270", "text": "docstring: def m3b_bots_targeting_3bins(fn='M3b_sampled_mentions.60000.csv', ofn='M3b-bots-targeting-3bins.pdf', nbins=3): fn = get_data_file(fn, subfolder='consensus1') ofn = get_output_file(ofn) df = pd.read_csv(fn) df = df.loc[~df.is_via_word_in_tweet_text] df = df[['from_raw_id', 'from_bot_score', 'to_followers_count']].copy() df = df.groupby('from_raw_id').mean() df = df.sort_values('from_bot_score', ascending=True) df = df.reset_index(drop=True) df = assign_segregated_groups(df, nbins) gpb = df.groupby('gn') d_mean = gpb.to_followers_count.mean() d_std = gpb.to_followers_count.std() d_size = gpb.size() x = np.array(range(3)) y1 = d_mean.values / 1e6 y1err = d_std / d_size.apply(np.sqrt) / 1e6 fig, ax = plt.subplots(figsize=FIGSIZE) ax.errorbar(x, y1, yerr=y1err, fmt='--o', capsize=4) ax.set_xticks(x) ax.set_xticklabels(['Lower Third', 'Middle Third', 'Top Third']) ax.set_xlabel('Bot Score Percentile of Mentioning Accounts') ax.set_ylabel('Followers of Mentioned Accounts') ax.set_xlim([-0.5, 2.5]) ax.set_ylim([6.55, 8.15]) ax.text( 0, 1.02, r'$\\times10^6$', transform=ax.transAxes, horizontalalignment='left') iax = plt.axes([.54, .24, .4, .32]) df.loc[:, 'to_followers_count'] = np.log10(df.to_followers_count) data = [] for name, gp in df.groupby('gn'): data.append(gp['to_followers_count'].values) iax.violinplot( data, positions=x + 0.15, vert=True, widths=0.8, showmeans=False, showextrema=False, showmedians=True, points=100, ) iax.set_xticks(x + 0.15) iax.set_xticklabels([]) iax.set_yticks([0, 4, 8]) iax.yaxis.set_major_formatter( mpl.ticker.FuncFormatter(lambda x, y: r'$10^%d$' % x)) plt.tight_layout() plt.savefig(ofn)"}
{"text_id": "9271", "text": "docstring: def output_piecewise(self, expr, paren): self.open_paren(paren) self.write('Piecewise [') comma = False for piece in getattr(expr, u'piece', []): if comma: self.write(',') else: comma = True self.write('Case ') self.output_expr(child_i(piece, 2), True) self.write(' ') self.output_expr(child_i(piece, 1), True) self.write('] ') if hasattr(expr, u'otherwise'): self.write('(Just ') self.output_expr(child_i(expr.otherwise, 1), True) self.write(')') else: self.write('Nothing') self.close_paren(paren) return"}
{"text_id": "9272", "text": "docstring: def do_find(lookup, term): space = defaultdict(list) for name in lookup.keys(): space[name].append(name) try: iter_lookup = lookup.iteritems() except AttributeError: iter_lookup = lookup.items() for name, definition in iter_lookup: for keyword in definition['keywords']: space[keyword].append(name) space[definition['category']].append(name) matches = fnmatch.filter(space.keys(), term) results = set() for match in matches: results.update(space[match]) return [(r, translate(lookup, r)) for r in results]"}
{"text_id": "9273", "text": "docstring: def frequency(seq, useall=False, calpc=False): length = len(seq) if calpc: freqs = {} else: base_counts = {} if useall: seqset = set(seq) else: seqset = (\"A\", \"T\", \"G\", \"C\") for letter in seqset: num = seq.count(letter) if calpc: freq = round(num/length, 2) freqs[letter] = freq else: base_counts[letter] = num if calpc: return freqs else: return base_counts"}
{"text_id": "9274", "text": "docstring: def mkQApp(name=None): global QAPP def onPaletteChange(palette): color = palette.base().color().name() app = QtWidgets.QApplication.instance() app.setProperty('darkMode', color.lower() != \"#ffffff\") QAPP = QtGui.QApplication.instance() if QAPP is None: qtVersionCompare = tuple(map(int, QtVersion.split(\".\"))) if qtVersionCompare > (6, 0): pass elif qtVersionCompare > (5, 14): os.environ[\"QT_ENABLE_HIGHDPI_SCALING\"] = \"1\" QtGui.QApplication.setHighDpiScaleFactorRoundingPolicy(QtCore.Qt.HighDpiScaleFactorRoundingPolicy.PassThrough) else: QtGui.QApplication.setAttribute(QtCore.Qt.AA_EnableHighDpiScaling) QtGui.QApplication.setAttribute(QtCore.Qt.AA_UseHighDpiPixmaps) QAPP = QtGui.QApplication(sys.argv or [\"pyqtgraph\"]) QAPP.paletteChanged.connect(onPaletteChange) QAPP.paletteChanged.emit(QAPP.palette()) if name is not None: QAPP.setApplicationName(name) return QAPP"}
{"text_id": "9275", "text": "docstring: def plotCutoffs(cut_S,ax,p): lines.drawCutoffHoriz(ax=ax,y=float(cut_S.values[0]), cl=cutPalette.ugColors[cut_S.name], lb=\"{0} {1}% Threshold: {2}\".format(cut_S.name,round(p*100,3), round(float(cut_S.values[0]),1)),ls=\"--\",lw=2)"}
{"text_id": "9276", "text": "docstring: def add_ou_noise_to_action( action: t.Tensor, noise_param: Dict[str, Any] = None, ratio=1.0, reset=False ): global DEFAULT_OU_GEN if reset: DEFAULT_OU_GEN = None if DEFAULT_OU_GEN is None: DEFAULT_OU_GEN = OrnsteinUhlenbeckNoiseGen(action.shape, **noise_param) DEFAULT_OU_GEN.reset() return action + DEFAULT_OU_GEN(action.device) * ratio"}
{"text_id": "9277", "text": "docstring: def activate_ports(module, port_path, ports, stdout, stderr): activate_c = 0 for port in ports: if not query_port(module, port_path, port): module.fail_json(msg=\"Failed to activate %s, port(s) not present\" % (port), stdout=stdout, stderr=stderr) if query_port(module, port_path, port, state=\"active\"): continue rc, out, err = module.run_command(\"%s activate %s\" % (port_path, port)) stdout += out stderr += err if not query_port(module, port_path, port, state=\"active\"): module.fail_json(msg=\"Failed to activate %s: %s\" % (port, err), stdout=stdout, stderr=stderr) activate_c += 1 if activate_c > 0: module.exit_json(changed=True, msg=\"Activated %s port(s)\" % (activate_c), stdout=stdout, stderr=stderr) module.exit_json(changed=False, msg=\"Port(s) already active\", stdout=stdout, stderr=stderr)"}
{"text_id": "9278", "text": "docstring: def _log_name(self): if self.name: return '%s:%s' % (self.user.unixname, self.name) else: return self.user.unixname"}
{"text_id": "9279", "text": "docstring: def evidence_weight_wrapper(X, y, signs=1, folds=5, lb_penalty=-5, ub_penalty=5, num_penalty=11, l_auto_weight=True, X_test=None): Weights=None if l_auto_weight: Weights=FindWeight.auto_weight(y) R_w, accuracy = FindWeight.evidence_weight(X, y, signs=signs, folds=folds, lb_penalty=lb_penalty, ub_penalty=ub_penalty, num_penalty=num_penalty, Weights=Weights) if X_test is None: y_pred=R_prob=None else: y_pred, R_prob = FindWeight.score(X_test, R_w, Weights=Weights) return (R_w, accuracy, y_pred, R_prob)"}
{"text_id": "9280", "text": "docstring: def perturb_blur_iter_full(self, X_nat, y, c_trg): if self.rand: X = X_nat.clone().detach_() + torch.tensor(np.random.uniform(-self.epsilon, self.epsilon, X_nat.shape).astype('float32')).to(self.device) else: X = X_nat.clone().detach_() ks_gauss = 11 ks_avg = 3 sig = 1 blur_type = 1 for i in range(self.k): if blur_type == 1: preproc = smoothing.GaussianSmoothing2D(sigma=sig, channels=3, kernel_size=ks_gauss).to(self.device) elif blur_type == 2: preproc = smoothing.AverageSmoothing2D(channels=3, kernel_size=ks_avg).to(self.device) X.requires_grad = True output, feats = self.model.forward_blur(X, c_trg, preproc) if self.feat: output = feats[self.feat] self.model.zero_grad() loss = self.loss_fn(output, y) loss.backward() grad = X.grad X_adv = X + self.a * grad.sign() eta = torch.clamp(X_adv - X_nat, min=-self.epsilon, max=self.epsilon) X = torch.clamp(X_nat + eta, min=-1, max=1).detach_() if blur_type == 1: sig += 0.5 if sig >= 3.2: blur_type = 2 sig = 1 if blur_type == 2: ks_avg += 2 if ks_avg >= 11: blur_type = 1 ks_avg = 3 self.model.zero_grad() return X, X - X_nat"}
{"text_id": "9281", "text": "docstring: def create_rgb_vrt(outname, infiles, overviews, overview_resampling): print(outname) pols = [re.search('[hv]{2}', os.path.basename(f)).group() for f in infiles] if pols[1] in ['vv', 'hh']: infiles.reverse() pols.reverse() ov = str(overviews) for x in ['[', ']', ',']: ov = ov.replace(x, '') gdalbuildvrt(src=infiles, dst=outname, options={'separate': True}) tree = etree.parse(outname) root = tree.getroot() srs = tree.find('SRS').text geotrans = tree.find('GeoTransform').text bands = tree.findall('VRTRasterBand') new_band = etree.SubElement(root, 'VRTRasterBand', attrib={'dataType': 'Float32', 'band': '3', 'subClass': 'VRTDerivedRasterBand'}) new_band.append(deepcopy(bands[0].find('NoDataValue'))) pxfun_type = etree.SubElement(new_band, 'PixelFunctionType') pxfun_type.text = 'mul' new_band.append(deepcopy(bands[0].find('ComplexSource'))) new_band.append(deepcopy(bands[1].find('ComplexSource'))) src = new_band.findall('ComplexSource')[1] fname = src.find('SourceFilename') fname_old = fname.text nodata = src.find('NODATA').text src_attr = src.find('SourceProperties').attrib fname.text = etree.CDATA(\"\"\" <VRTDataset rasterXSize=\"{rasterxsize}\" rasterYSize=\"{rasterysize}\"> <SRS dataAxisToSRSAxisMapping=\"1,2\">{srs}</SRS> <GeoTransform>{geotrans}</GeoTransform> <VRTRasterBand dataType=\"{dtype}\" band=\"1\" subClass=\"VRTDerivedRasterBand\"> <PixelFunctionType>{px_fun}</PixelFunctionType> <ComplexSource> <SourceFilename relativeToVRT=\"1\">{fname}</SourceFilename> <SourceBand>1</SourceBand> <SourceProperties RasterXSize=\"{rasterxsize}\" RasterYSize=\"{rasterysize}\" DataType=\"{dtype}\" BlockXSize=\"{blockxsize}\" BlockYSize=\"{blockysize}\"/> <SrcRect xOff=\"0\" yOff=\"0\" xSize=\"{rasterxsize}\" ySize=\"{rasterysize}\"/> <DstRect xOff=\"0\" yOff=\"0\" xSize=\"{rasterxsize}\" ySize=\"{rasterysize}\"/> <NODATA>{nodata}</NODATA> </ComplexSource> </VRTRasterBand> <OverviewList resampling=\"{ov_resampling}\">{ov}</OverviewList> </VRTDataset> \"\"\".format(rasterxsize=src_attr['RasterXSize'], rasterysize=src_attr['RasterYSize'], srs=srs, geotrans=geotrans, dtype=src_attr['DataType'], px_fun='inv', fname=fname_old, blockxsize=src_attr['BlockXSize'], blockysize=src_attr['BlockYSize'], nodata=nodata, ov_resampling=overview_resampling.lower(), ov=ov)) bands = tree.findall('VRTRasterBand') for band, col in zip(bands, ['Red', 'Green', 'Blue']): color = etree.Element('ColorInterp') color.text = col band.insert(0, color) for i, band in enumerate(bands): if i in [0, 1]: band.remove(band.find('NoDataValue')) ovr = etree.SubElement(root, 'OverviewList', attrib={'resampling': overview_resampling.lower()}) ovr.text = ov etree.indent(root) tree.write(outname, pretty_print=True, xml_declaration=False, encoding='utf-8')"}
{"text_id": "9282", "text": "docstring: def did_move(self): self.rect = self.surf.get_rect().move(self.x_coord, self.y_coord) parent_height = self.parent.get_height() parent_width = self.parent.get_width() if self.y_coord < 0: self.y_coord = 0 if self.y_coord > parent_height - self.height: self.y_coord = parent_height - self.height if self.x_coord < 0: self.x_coord = 0 if self.x_coord > parent_width - self.width: self.x_coord = parent_width - self.width"}
{"text_id": "9283", "text": "docstring: def validate_self(self) -> bool: return True"}
{"text_id": "9284", "text": "docstring: def save_as_image(mapfile, imagefile=None, type=RASTER_FORMAT_PNG, pix_width=1000, pix_height=0, pix_32_bit=False): return Map.open(mapfile).image_file(imagefile=imagefile, type=type, pix_width=pix_width, pix_height=pix_height, pix_32_bit=pix_32_bit)"}
{"text_id": "9285", "text": "docstring: def post_mock_response_votes(status_code, content): test_mock = mock.Mock() test_mock.status_code = status_code test_mock.content = content def mock_response(api_url, headers, json, timeout, proxies): return test_mock return mock_response"}
{"text_id": "9286", "text": "docstring: def draw_text(self, surface, text, size, x, y, color): font = pg.font.Font(pg.font.match_font('cambria'), size) text_surface = font.render(text, True, color) text_rect = text_surface.get_rect() text_rect.midtop = (x, y) surface.blit(text_surface, text_rect)"}
{"text_id": "9287", "text": "docstring: def pie_chart(labels, values, title): fig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent')]) fig.update_layout(title={'text':title, 'y':0.9,'x':.61, 'xanchor': 'right'}) fig.show()"}
{"text_id": "9288", "text": "docstring: def wws2ts(C): Ct = zero_tensor((3,3,3,3)) for a in range(3): for b in range(6): inds_a = skew_inds[a] inds_b = mandel[b] mult_a = skew_mults[a] mult_b = mandel_mults[b] for ord_a, f in zip(((0,1),(1,0)),(1,-1)): for ord_b in ((0,1),(1,0)): ind = tuple([inds_a[aa] for aa in ord_a] + [inds_b[bb] for bb in ord_b]) Ct[ind] = C[a,b] / (mult_a * mult_b) * f return Ct"}
{"text_id": "9289", "text": "docstring: def load_preset(preset_files: Sequence[Path]) -> Dict[str, str]: preset: Dict[str, str] = {} for fork_file in preset_files: yaml = YAML(typ='base') fork_preset: dict = yaml.load(fork_file) if fork_preset is None: continue if not set(fork_preset.keys()).isdisjoint(preset.keys()): duplicates = set(fork_preset.keys()).intersection(set(preset.keys())) raise Exception(f\"duplicate config var(s) in preset files: {', '.join(duplicates)}\") preset.update(fork_preset) assert preset != {} return parse_config_vars(preset)"}
{"text_id": "9290", "text": "docstring: def never_report_collisions(geom1, geom2): return False"}
{"text_id": "9291", "text": "docstring: def on_enter_window(self, event): self.last_moused = (-1, \"\") event.Skip()"}
{"text_id": "9292", "text": "docstring: def create_user_consent(self, user_consent_id, request): return self.start().uri('/api/user/consent') \\ .url_segment(user_consent_id) \\ .body_handler(JSONBodyHandler(request)) \\ .post() \\ .go()"}
{"text_id": "9293", "text": "docstring: def debater_abnormal_round_speaks(debater, round_number): team = debater.team() had_noshow = NoShow.objects.filter(round_number=round_number, no_show_team=team) if had_bye(team, round_number) or (had_noshow and had_noshow.first().lenient_late): return avg_deb_speaks(debater) elif had_noshow: return MINIMUM_DEBATER_SPEAKS"}
{"text_id": "9294", "text": "docstring: def parse_html_parcela(cls, parsed_html, x=None, y=None, picture=None): description = parsed_html.find(id='ctl00_Contenido_tblInmueble') descriptive_data = dict() descriptive_data[u'Longitud'] = x descriptive_data[u'Latitud'] = y descriptive_data[u'Gr\u00e1ficoParcela'] = picture descriptive_data[u'Construcciones'] = [] fields = description.find_all('div') for field in fields: field_header = field.find('span') for field_name in cls.description_field_names: if field_name in field_header.text: field_value = field.find('label', {\"class\": \"control-label black text-left\"}) descriptive_data[field_name] = field_value.text.strip() if field_header.text == u'Referencia catastral': descriptive_data[field_name] = descriptive_data[field_name].split(' ')[0] descriptive_data[field_name] = descriptive_data[field_name].split('\\xa0')[0] elif field_header.text == u'Localizaci\u00f3n': descriptive_data[field_name] = field_value.encode_contents().decode('utf-8').replace('<br/>',config['separator']).replace('<br>', config['separator']) fields = parsed_html.find(id='ctl00_Contenido_tblFinca').find_all('div') for field in fields: field_header = field.find('span') for field_name in cls.gsurface_field_names: if field_name in field_header.text: field_value = field.find('label', {\"class\": \"control-label black text-left\"}) descriptive_data[field_name] = field_value.text.strip() constructions_table = parsed_html.find(id='ctl00_Contenido_tblLocales') if constructions_table is None: constructions = [] else: constructions = constructions_table.find_all('tr') header = True for construction in constructions: if header: header = False continue columns = construction.find_all('span') descriptive_data[u'Construcciones'].append( dict(uso=columns[0].text, escalera=columns[1].text, planta=columns[2].text, puerta=columns[3].text, superficie=columns[4].text, tipo=columns[5].text, fecha=columns[6].text)) descriptive_data[u'Gr\u00e1ficoParcela']=picture cadaster_entry = CadasterEntryHTML(descriptive_data) return cadaster_entry"}
{"text_id": "9295", "text": "docstring: async def cmd_add_deck(message : discord.Message, args : str, isDM : bool): if not args: await message.channel.send(\":x: Please provide a link to the deck file generated by `make-deck`!\") return callingBGuild = botState.guildsDB.getGuild(message.guild.id) async with botState.httpClient.get(args) as resp: deckMeta = await resp.json() now = datetime.utcnow() callingBGuild.decks[deckMeta[\"deck_name\"].lower()] = {\"meta_url\": args, \"creator\": message.author.id, \"last_update\": now.timestamp(), \"plays\": 0, \"expansion_names\" : list(deckMeta[\"expansions\"].keys())} await message.channel.send(\"Deck added!\")"}
{"text_id": "9296", "text": "docstring: def train_loader(config): device = torch.device('cuda' if torch.cuda.is_available() and config['cuda_enabled'] else 'cpu') N_u = config['num_datadriven'] N_f = config['num_collocation'] N_inp = config['NUM_INPUTS'] N_out = config['NUM_OUTPUTS'] N_init = config['NUM_INIT'] data = np.genfromtxt( config['datadir'] + config['datafile'], delimiter=',', dtype=np.float32) N_data = data.shape[0] lb = np.min(data[:, :N_inp], axis=0) ub = np.max(data[:, :N_inp], axis=0) idx_init = np.array([]) for col in config['COLS_INIT']: vals = np.unique(data[:, col]) for val in vals[:min(N_init, vals.shape[0])]: idx_val = [data[:, col] == val] idx_init = np.hstack([idx_init, idx_val]) idx_non_init = np.random.choice(np.delete(np.arange(N_data), idx_init), max(N_u - idx_init.shape[0], 0), replace=False) idx_train = np.hstack([idx_init, idx_non_init]) idx_validation = np.random.choice(np.delete(np.arange(N_data), idx_train), N_u, axis=0) inp_u = data[idx_train, :N_inp] out_u = data[idx_train, -N_out:] inp_f = np.vstack([lb + (ub-lb)*lhs(N_inp, N_f) if N_f > 0 else np.array([]).reshape(0, N_inp), inp_u]) val_inp = data[idx_validation, :N_inp] val_out = data[idx_validation, -N_out:] if N_init > 1: out_u += config['noise'] * \\ np.std(out_u) * np.random.randn(*out_u.shape) return [torch.from_numpy(x).float().to(device) for x in [inp_u, out_u, inp_f, val_inp, val_out, lb, ub]]"}
{"text_id": "9297", "text": "docstring: def analyze_class_attribute_access(itype: Instance, name: str, context: Context, is_lvalue: bool, builtin_type: Callable[[str], Instance], not_ready_callback: Callable[[str, Context], None], msg: MessageBuilder, original_type: Type = None) -> Type: node = itype.type.get(name) if not node: if itype.type.fallback_to_any: return AnyType() return None is_decorated = isinstance(node.node, Decorator) is_method = is_decorated or isinstance(node.node, FuncDef) if is_lvalue: if is_method: msg.cant_assign_to_method(context) if isinstance(node.node, TypeInfo): msg.fail(messages.CANNOT_ASSIGN_TO_TYPE, context) if itype.type.is_enum and not (is_lvalue or is_decorated or is_method): return itype t = node.type if t: if isinstance(t, PartialType): return handle_partial_attribute_type(t, is_lvalue, msg, node.node) is_classmethod = is_decorated and cast(Decorator, node.node).func.is_class return add_class_tvars(t, itype, is_classmethod, builtin_type, original_type) elif isinstance(node.node, Var): not_ready_callback(name, context) return AnyType() if isinstance(node.node, TypeInfo): return type_object_type(node.node, builtin_type) if isinstance(node.node, MypyFile): return builtin_type('builtins.module') if is_decorated: return AnyType() else: return function_type(cast(FuncBase, node.node), builtin_type('builtins.function'))"}
{"text_id": "9298", "text": "docstring: def _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights=None): metric_results = [] for metric_name, metric_fn in metrics_dict.items(): with K.name_scope(metric_name): metric_result = training_utils.call_metric_function( metric_fn, y_true, y_pred, weights=weights, mask=mask) metric_results.append(metric_result) return metric_results"}
{"text_id": "9299", "text": "docstring: def VarListCopy(DestinationList, DesitnationStart, SourceList, SourceStart, NumToCopy=0): pass"}
{"text_id": "9300", "text": "docstring: def save(self, *args, **kwargs): if not self.cart_item_name: self.cart_itame_name = self.product.product_name self.calculate_total() super(CartItem, self).save(*args, **kwargs)"}
{"text_id": "9301", "text": "docstring: def _get_collaborators_with_given_access( self, access_levels: List[int] ) -> List[str]: all_members = None if hasattr(self.gitlab_repo, \"members_all\"): all_members = self.gitlab_repo.members_all.list(all=True) else: all_members = self.gitlab_repo.members.all(all=True) response = [] for member in all_members: if isinstance(member, dict): access_level = member[\"access_level\"] username = member[\"username\"] else: access_level = member.access_level username = member.username if access_level in access_levels: response.append(username) return response"}
{"text_id": "9302", "text": "docstring: def load(self): if not os.path.exists(self.store_path): return {} issues = yaml.safe_load(open(self.store_path)) if issues and IssuesManager.SUMMARY_OUT_ISSUES_ROOT in issues: return issues return {}"}
{"text_id": "9303", "text": "docstring: def verify_image_pixel_shift_random(self): href_random = self.driver.find_element_by_xpath(\"//a[@href = '/shifting_content/image?mode=random&pixel_shift=100']\") href_random.click() current_url = self.driver.current_url expected_url = \"http://the-internet.herokuapp.com/shifting_content/image?mode=random&pixel_shift=100\" assert current_url == expected_url, \"Shifting of image elements happens by 100 pixels and randomly\""}
{"text_id": "9304", "text": "docstring: def remove_active(self, time): for server in self.active_list: server.stop_all() logging.warning('Spot server on market %s fails at time %d. Because spot price %f is larger than bid %f' % (self.name, time, self.spot_price[min(time / MINUTE_INTERVAL, TRACE_LENGTH - 1)], self.bid)) server.created_time = None server.spot_fail = False server.Lb = None server.Es = None server.bid = None self.free_list.append(server) del self.active_list[:]"}
{"text_id": "9305", "text": "docstring: def increment_qualification_score(self, name, worker_id, notify=False): result = self.get_current_qualification_score(name, worker_id) current_score = result['score'] or 0 new_score = current_score + 1 qtype_id = result['qtype']['id'] self.assign_qualification(qtype_id, worker_id, new_score, notify) return { 'qtype': result['qtype'], 'score': new_score }"}
{"text_id": "9306", "text": "docstring: def send_request(self, request, check_asserts=True): try: response = self.rest_client.infer(self.model, request) except proteus.ConnectionError: pytest.fail( \"Connection to the proteus server ended without response!\", False ) num_inputs = len(request.inputs) gold_response_output = [ -1, 0.9937100410461426, 268, 79.875, 156, 169.06874084472656, ] if check_asserts: assert not response.error, response.error_msg assert response.id == \"\" assert response.model_name == \"facedetect\" assert len(response.outputs) == num_inputs for index, output in enumerate(response.outputs): assert output.name == \"input\" + str(index) assert output.datatype == Datatype.FP32 assert output.parameters == {} num_boxes = int(len(output.data) / 6) assert output.shape == [6, num_boxes] assert len(output.data) == len(gold_response_output) np.testing.assert_almost_equal(gold_response_output, output.data, 2) return response"}
{"text_id": "9307", "text": "docstring: def whatweb(self): opts = '-a 3 --open-timeout 10 -t 20 --color=never --read-timeout 10' opts += f\" --user-agent '{self.useragent}'\" if self.opts['web_user'] and self.opts['web_pass']: opts += f\" -u {self.opts['web_user']}:{self.opts['web_pass']}\" if self.opts['cookies']: opts += f\" --cookie '{self.cookies}'\" if self.opts['proxy']: opts += f\" --proxy '{self.opts['proxy']}'\" if self.opts['proxy_user'] and self.opts['proxy_pass']: opts += f\" --proxy user {self.opts['proxy_user']}:\" opts += f\"{self.opts['proxy_pass']}\" opts += f' {self.target}' self._run_tool('whatweb', opts) return"}
{"text_id": "9308", "text": "docstring: def _merge(self, filenames, outfile, threads=0, regions=None, info_rules=None): check_dependency(\"bcftools\") self._index(filenames) command = [\"bcftools\", \"merge\"] command += filenames command += [\"-o\", outfile] command += [\"-O\", \"z\"] command += [\"-0\"] command += [\"-i\", \"CSQ:join\"] command += [\"--force-samples\"] if threads: command += [\"--threads\", threads] if regions: command += [\"-r\", regions] process = subprocess.run(command, check=True) self.vcf = VCF(outfile) infodict = { \"ID\": \"bcftools-merge\", \"Number\": \"1\", \"Type\": \"Merge\", \"Description\": \"Merging of cohort samples into single vcf. \" + \"Commandline: {}\".format(\" \".join(process.args)) } self.vcf.add_info_to_header(infodict) self._index([outfile])"}
{"text_id": "9309", "text": "docstring: def _concatenate_name(self, index): return self.path + str(index) + \".json\""}
{"text_id": "9310", "text": "docstring: def _PullDockerImages(api, os_versions): api.docker.login(server='gcr.io', project='chops-public-images-prod') for os_version in sorted(os_versions): image = _IMAGE_TEMPLATE % os_version try: api.docker.pull(image) except api.step.StepFailure: raise api.step.InfraFailure( 'Image %s does not exist in the container registry.' % image)"}
{"text_id": "9311", "text": "docstring: def exec_np(x): return IsValid.exec_num(x)"}
{"text_id": "9312", "text": "docstring: def generateTelemetry(self) -> SensorData: self.newSensor = SensorData(sensorType=self.sensorType) if self.useRandomizer: self.newSensor.setValue(random.uniform(self.minVal,self.maxVal)) else: self.newSensor.setValue(self.dataSet.getDataEntry(self.dataSetIndex)) self.dataSetIndex = self.dataSetIndex+1 if(self.dataSetIndex>self.dataSet.getDataEntries().size): self.dataSetIndex=0 self.latestSensorData = self.newSensor return self.latestSensorData"}
{"text_id": "9313", "text": "docstring: def load_systems(release): pass"}
{"text_id": "9314", "text": "docstring: def cb(pkt): logging.debug(\"at cb(pkt), sniffing callback\") global clients_APs, APs if args.maximum: if args.noupdate: if len(clients_APs) > int(args.maximum): return else: if len(clients_APs) > int(args.maximum): with lock: clients_APs = [] APs = [] if pkt.haslayer(Dot11): if pkt.addr1 and pkt.addr2: pkt.addr1 = pkt.addr1.lower() pkt.addr2 = pkt.addr2.lower() if args.accesspoint: if args.accesspoint not in [pkt.addr1, pkt.addr2]: return if pkt.haslayer(Dot11Beacon) or pkt.haslayer(Dot11ProbeResp): APs_add(clients_APs, APs, pkt, args.channel, args.world) if noise_filter(args.skip, pkt.addr1, pkt.addr2): return if pkt.type in [1, 2]: clients_APs_add(clients_APs, pkt.addr1, pkt.addr2)"}
{"text_id": "9315", "text": "docstring: def currency(self): return self.get_attribute(\"office:currency\")"}
{"text_id": "9316", "text": "docstring: def begin_create_or_update( self, resource_group_name, resource_provider_name, resource_provider, **kwargs ): polling = kwargs.pop('polling', True) cls = kwargs.pop('cls', None) lro_delay = kwargs.pop( 'polling_interval', self._config.polling_interval ) cont_token = kwargs.pop('continuation_token', None) if cont_token is None: raw_result = self._create_or_update_initial( resource_group_name=resource_group_name, resource_provider_name=resource_provider_name, resource_provider=resource_provider, cls=lambda x,y,z: x, **kwargs ) kwargs.pop('error_map', None) kwargs.pop('content_type', None) def get_long_running_output(pipeline_response): deserialized = self._deserialize('CustomRPManifest', pipeline_response) if cls: return cls(pipeline_response, deserialized, {}) return deserialized path_format_arguments = { 'subscriptionId': self._serialize.url(\"self._config.subscription_id\", self._config.subscription_id, 'str'), 'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'), 'resourceProviderName': self._serialize.url(\"resource_provider_name\", resource_provider_name, 'str', max_length=64, min_length=3), } if polling is True: polling_method = ARMPolling(lro_delay, path_format_arguments=path_format_arguments, **kwargs) elif polling is False: polling_method = NoPolling() else: polling_method = polling if cont_token: return LROPoller.from_continuation_token( polling_method=polling_method, continuation_token=cont_token, client=self._client, deserialization_callback=get_long_running_output ) else: return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"text_id": "9317", "text": "docstring: def transform(self, corpus): text = [] for sentence in corpus: text.append(\" \".join(sentence.astype(str))) df = container.DataFrame(text, generate_metadata=True) for column_index in range(df.shape[1]): col_dict = dict(df.metadata.query((metadata_base.ALL_ELEMENTS, column_index))) col_dict['structural_type'] = type(1.0) col_dict['name'] = 'fastlvm_' + str(column_index) col_dict['semantic_types'] = ('http://schema.org/Text', 'https://metadata.datadrivendiscovery.org/types/Attribute') df.metadata = df.metadata.update((metadata_base.ALL_ELEMENTS, column_index), col_dict) return df"}
{"text_id": "9318", "text": "docstring: def calibrate_absolute_gain(self): self.df_preamp_gain = pd.read_csv(os.path.join(get_git_root(os.getcwd()), 'FEBDAQMULTx2/data_analysis/preamp_calib_data/caen_measurement.csv')) x_meas = np.array(self.df_preamp_gain['High Gain']) y_meas = np.array(self.df_preamp_gain['Gain (ch/pC)']) ius = InterpolatedUnivariateSpline(x_meas, y_meas) adc_per_charge = ius(self.preamp_gain) return (self.gainfitp[0]/1.6e-19)/(adc_per_charge/1e-12)"}
{"text_id": "9319", "text": "docstring: def find_git_dir(directory): directory = os.path.abspath(directory) if not os.path.exists(directory): return \"\" for _ in range(10): path = os.path.join(directory, \".git\") if os.path.exists(path): return directory if directory == \"/\": return \"\" directory = os.path.abspath(os.path.join(directory, os.pardir)) return \"\""}
{"text_id": "9320", "text": "docstring: def disconnect(): child = execute_command(\"windscribe disconnect\") match = child.expect( [ \"DISCONNECTED\", \"Service communication error\", \"Windscribe is not running\", EOF, ] ) if match == 3: logger.error(UNSUPPORTED_VERSION) raise UnsupportedVersionException(UNSUPPORTED_VERSION) elif match == 2: logger.error(WINDSCRIBE_NOT_RUNNING) raise WindscribeNotRunningException(WINDSCRIBE_NOT_RUNNING) elif match == 1: logger.error(NOT_CONNECTED_TO_INTERNET) raise ConnectionError(NOT_CONNECTED_TO_INTERNET) else: child.wait() logger.info(\"Disconnected.\")"}
{"text_id": "9321", "text": "docstring: def update_op( image_id_batched, groundtruth_boxes_batched, groundtruth_classes_batched, groundtruth_is_crowd_batched, num_gt_boxes_per_image, detection_boxes_batched, detection_scores_batched, detection_classes_batched, num_det_boxes_per_image): for (image_id, gt_box, gt_class, gt_is_crowd, num_gt_box, det_box, det_score, det_class, num_det_box) in zip( image_id_batched, groundtruth_boxes_batched, groundtruth_classes_batched, groundtruth_is_crowd_batched, num_gt_boxes_per_image, detection_boxes_batched, detection_scores_batched, detection_classes_batched, num_det_boxes_per_image): self.add_single_ground_truth_image_info( image_id, {'groundtruth_boxes': gt_box[:num_gt_box], 'groundtruth_classes': gt_class[:num_gt_box], 'groundtruth_is_crowd': gt_is_crowd[:num_gt_box]}) self.add_single_detected_image_info( image_id, {'detection_boxes': det_box[:num_det_box], 'detection_scores': det_score[:num_det_box], 'detection_classes': det_class[:num_det_box]})"}
{"text_id": "9322", "text": "docstring: def ClearEmbeddingIndices(self): try: self.embedding_indices = None except Exception as ex: template = \"An exception of type {0} occurred in [GloVeEmbeddingLayer.ClearEmbeddingIndices]. Arguments:\\n{1!r}\" message = template.format(type(ex).__name__, ex.args) print(message)"}
{"text_id": "9323", "text": "docstring: async def update(self): while True: payload = { 'left_motor_speed': self.left_motor.speed, 'left_motor_duty_cycle': self.left_motor.duty_cycle_ms, 'right_motor_speed': self.right_motor.speed, 'right_motor_duty_cycle': self.right_motor.duty_cycle_ms, 'throttle': self.throttle, 'turn_speed': self.turn_speed, 'trim': self.trim, } self.publish('auv.update', payload) await asyncio.sleep(1 / self.update_frequency)"}
{"text_id": "9324", "text": "docstring: def action_btn_new_project_pressed(self, *args): if self.popup: return False if self.designer.project_manager.current_project.saved: self._show_new_dialog() return True _confirm_dlg_save = ConfirmationDialogSave( 'Your project is not saved.\\nWhat would you like to do?' ) def save_and_open(*args): self.action_btn_save_pressed() self._show_new_dialog() _confirm_dlg_save.bind( on_save=save_and_open, on_cancel=self.close_popup, on_dont_save=self._show_new_dialog, ) self.popup = Popup( title='New', content=_confirm_dlg_save, size_hint=(None, None), size=('300pt', '150pt'), auto_dismiss=False) self.popup.open() return True"}
{"text_id": "9325", "text": "docstring: def docker_ip(): docker_client = get_docker_client() containers = get_docker_container_dicts(docker_client) container_status = get_docker_container_status(docker_client) docker_host = docker_client.host try: if container_status[POSTGIS_CONTAINER]: postgis_container = containers[POSTGIS_CONTAINER] postgis_port = postgis_container['Ports'][0]['PublicPort'] print('\\nPostGIS/Database:') print(' Host: {0}'.format(docker_host)) print(' Port: {0}'.format(postgis_port)) else: print('PostGIS/Database: Not Running.') except KeyError: print('PostGIS/Database: Not Installed.') except: raise try: if container_status[GEOSERVER_CONTAINER]: geoserver_container = containers[GEOSERVER_CONTAINER] geoserver_port = geoserver_container['Ports'][0]['PublicPort'] print('\\nGeoServer:') print(' Host: {0}'.format(docker_host)) print(' Port: {0}'.format(geoserver_port)) print(' Endpoint: http://{0}:{1}/geoserver/rest'.format(docker_host, geoserver_port)) else: print('GeoServer: Not Running.') except KeyError: print('GeoServer: Not Installed.') except: raise try: if container_status[N52WPS_CONTAINER]: n52wps_container = containers[N52WPS_CONTAINER] n52wps_port = n52wps_container['Ports'][0]['PublicPort'] print('\\n52 North WPS:') print(' Host: {0}'.format(docker_host)) print(' Port: {0}'.format(n52wps_port)) print(' Endpoint: http://{0}:{1}/wps/WebProcessingService\\n'.format(docker_host, n52wps_port)) else: print('52 North WPS: Not Running.') except KeyError: print('52 North WPS: Not Installed.') except: raise"}
{"text_id": "9326", "text": "docstring: def from_xml_node(cls, node): return cls(name=node.find(\"Name\").text, bit_position=int(node.find(\"Bitposition\").text), bit_size=int(node.find(\"Bitsize\").text), factor=float(node.find(\"Factor\").text), offset=float(node.find(\"Offset\").text), min_value=float(node.find(\"Minimum\").text), max_value=float(node.find(\"Maximum\").text))"}
{"text_id": "9327", "text": "docstring: def generateRules( self, r=3, parsetags=True, limit=0, offset=0 ): if not limit: limit = float(\"inf\") counter = 0 tokenCounter = 0 rulesCounter = 0 self.logger.output( \"Here you see progress at generating rules in the following \" \"format:\\n\" \"{number of tokens of sentences}/{number of rules} {relation}%\\n\" \"\\\"relation\\\" is the number of rules divided by number of tokens.\" ) while True: try: sen = self.reader.nextSentence() tokenCounter += len(sen[\"sentence\"]) counter += 1 if counter < offset: raise ContinueException if counter > limit: raise StopIteration self.logger.write( f\"Process sentence #{counter}. Here is it:\\n\" ) self.logger.logjson(sen) for rule in self.processSentence( sentence=sen[\"sentence\"], text=self.reader.getAttr(sen, \"text\"), r=r, parseTags=parsetags ): self.logger.write(\"Generated rule:\\n\") self.logger.logjson(rule) rulesCounter += 1 self.logger.output( f\"{tokenCounter}/{rulesCounter}\\t\" f\"{rulesCounter/tokenCounter}%\", rewritable=True ) yield rule except (EOFError): break except (ContinueException, TokenizationError, TaggingError): continue"}
{"text_id": "9328", "text": "docstring: async def convert(self, ctx: Context, argument: str) -> str: if argument == \"*\" or argument == \"**\": return argument argument = argument.lower() if argument in EXTENSIONS: return argument elif (qualified_arg := f\"{exts.__name__}.{argument}\") in EXTENSIONS: return qualified_arg matches = [] for ext in EXTENSIONS: if argument == unqualify(ext): matches.append(ext) if len(matches) > 1: matches.sort() names = \"\\n\".join(matches) raise BadArgument( f\":x: `{argument}` is an ambiguous extension name. \" f\"Please use one of the following fully-qualified names.```\\n{names}```\" ) elif matches: return matches[0] else: if check_module(argument): return argument raise BadArgument(f\":x: Could not find the extension `{argument}`.\")"}
{"text_id": "9329", "text": "docstring: def _export_file(self, file_path, *args, **kwargs): raise NotImplementedError('_export_file function for \"{}\" is not implemented!'.format(self))"}
{"text_id": "9330", "text": "docstring: def image_create(self,image_format,width_pixels,height_pixels,stride_bytes,image_handle): _k4a.VERIFY(self.k4a.k4a_image_create(image_format,width_pixels,height_pixels,stride_bytes,image_handle),\"Create image failed!\")"}
{"text_id": "9331", "text": "docstring: def tag_property_name(self) -> typing.Optional[builtins.str]: result = self._values.get(\"tag_property_name\") return result"}
{"text_id": "9332", "text": "docstring: def __run_workers(self): self.__fire = True if len(self.tasks) < self.parallelism: log_msg = 'Fewer tasks than parallelism; spawning fewer workers.' logger.info(log_msg) for i in xrange(len(self.tasks)): self.__workers.append( multiprocessing.Process(target=self.__enslave, name='worker_%s' % i)) else: for i in xrange(self.parallelism): self.__workers.append( multiprocessing.Process(target=self.__enslave, name='worker_%s' % i)) log_msg = 'Initialized {} workers.'.format(len(self.__workers)) logger.info(log_msg) for (_worker_nb, _worker) in enumerate(self.__workers): log_msg = 'Starting worker {}'.format(_worker_nb) logger.debug(log_msg) _worker.start()"}
{"text_id": "9333", "text": "docstring: def E_logsoftev_EdgeData(self, Data, LP): N = Data.nNodeTotal E = Data.nEdgeTotal ElogLamA = np.zeros(self.K) ElogLamB = np.zeros(self.K) E_logsoftev_EdgeLik = np.zeros( (Data.nEdgeTotal, self.K) ) E_logsoftev_EdgeEps = np.zeros( Data.nEdgeTotal ) ElogEps1 = np.log(self.epsilon) ElogEps0 = np.log(1-self.epsilon) for k in xrange(self.K): ElogLamA[k] = self.comp[k].ElogLamA ElogLamB[k] = self.comp[k].ElogLamB E_logsoftev_EdgeLik[Data.ind1,:] = ElogLamA E_logsoftev_EdgeLik[Data.ind0,:] = ElogLamB E_logsoftev_EdgeEps[Data.ind1] = ElogEps1 E_logsoftev_EdgeEps[Data.ind0] = ElogEps0 return (E_logsoftev_EdgeLik, E_logsoftev_EdgeEps)"}
{"text_id": "9334", "text": "docstring: def testMakePromises_Anon(self): self.SetUpPromises('Priority:High') self.mox.ReplayAll() backendsearchpipeline.BackendSearchPipeline( self.mr, self.services, 100, ['proj'], None, []) self.mox.VerifyAll()"}
{"text_id": "9335", "text": "docstring: def gen_eve_wf(y2000_file,sta_file,src_root=\"day_data\",tar_root=\"eve_wf\",tb = -20,te = 60,shift_hour=0): if not os.path.exists(tar_root): os.mkdir(tar_root) base_path = os.path.abspath(\"./\") sta_dict = load_sta(sta_file) phs_dict = load_y2000(y2000_file) cut_eve_wf(phs_dict,sta_dict,src_root,tar_root,tb,te,shift_hour=shift_hour) write_eve_wf(phs_dict,tar_root)"}
{"text_id": "9336", "text": "docstring: def try_two_simple_requests(): requests_dot_post() requests_dot_post()"}
{"text_id": "9337", "text": "docstring: def user_logs_dir(app_name, app_author, version=None, use_virtualenv=True, create=True): return _get_folder(True, _FolderTypes.logs, app_name, app_author, version, False, use_virtualenv, create)[0]"}
{"text_id": "9338", "text": "docstring: def solve(self, *args, timeout=None, all_solutions=False, **kwargs): log = logging.getLogger(__name__) if timeout and not self.support_timeout: if not self.support_all: raise ValueError('Timeout not supported') all_solutions = True solver_args = self.args( *args, timeout=timeout, all_solutions=all_solutions, **kwargs ) timeout = None if self.support_timeout else timeout try: log.debug('Running solver with arguments {}'.format(solver_args)) process = Process(solver_args).run(timeout=timeout) out = process.stdout_data err = process.stderr_data except CalledProcessError as err: log.exception(err.stderr) raise RuntimeError(err.stderr) from err return out, err"}
{"text_id": "9339", "text": "docstring: def parse_report_file(report_file): report = '' for c in report_file.chunks(): report += c.decode() return json.loads(report)"}
{"text_id": "9340", "text": "docstring: def square(array, x, y, size, mag): x1 = x - size y1 = y - size x2 = x + size y2 = y + size div = 4.0 l = len(array) if x1 >= 0: a = array[x1, y] else: a = 0.0 div -= 1.0 if y1 >= 0: b = array[x, y1] else: b = 0.0 div -= 1.0 if x2 < l: c = array[x2, y] else: c = 0.0 div -= 1.0 if y2 < l: d = array[x, y2] else: d = 0.0 div -= 1.0 if div: array[x, y] = (a + b + c + d) / div + scaled_random(mag)"}
{"text_id": "9341", "text": "docstring: def parse(content) -> tuple: if isinstance(content, str) or isinstance(content, Path): if isfile(str(content)): tree = ET.parse(content) root = tree.getroot() else: try: tree = ET.fromstring(content) root = tree.getroot() except: raise(ValueError( \"Content was identified as string, but could not be parsed. Please, verify.\")) elif isinstance(content, ET.ElementTree): try: tree = content root = tree.getroot() except: raise(ValueError( \"Content was identified as ElementTree object, but could not get its root. Please, verify.\")) elif isinstance(content, ET.Element): tree = None root = content else: raise(ValueError(\"Content is not file, string or xml tree. Please, verify.\")) return tree, root"}
{"text_id": "9342", "text": "docstring: def cronbach_alpha(scores, key): n_scales = key.shape[1] alpha = [0.0] * n_scales for i in range(n_scales): inds = np.where(key[:, i] != 0)[0] signs = key[inds, i] X = np.dot(scores[:, inds], np.diag(signs)) k = len(signs) total = np.sum(X, 1) tot_var = np.var(total) item_var = np.sum(np.var(X, 0)) a = (float(k) / (k - 1)) * (1 - np.divide(item_var, tot_var)) alpha[i] = a return np.round(alpha, 3)"}
{"text_id": "9343", "text": "docstring: def open(self): self.set_nodelay(True) logging.info(self.request.host) client = Client(client_websocket=self) ClientWebSocketHandler.clients.add(client) logging.info('[%d] New client' % (id(client),))"}
{"text_id": "9344", "text": "docstring: def on_message(self, callback): self.mqtt_connection.on_message(callback)"}
{"text_id": "9345", "text": "docstring: def group_user_del_bsd(group, user): assert group_check(group), \"Group does not exist: %s\" % (group) if group_user_check(group, user): group_for_user = run(\"getent group | egrep -v '^%s:' | grep '%s' | awk -F':' '{print $1}' | grep -v %s; true\" % (group, user, user)).splitlines() if group_for_user: sudo(\"pw usermod -G '%s' '%s'\" % (\",\".join(group_for_user), user)) else: sudo(\"pw usermod -G '' '%s'\" % (user))"}
{"text_id": "9346", "text": "docstring: def is_ordered(self) -> bool: return all((site.is_ordered for site in self))"}
{"text_id": "9347", "text": "docstring: def istokoper(toks, off): if off >= len(toks): return False return opers.get(toks[off][0]) is not None"}
{"text_id": "9348", "text": "docstring: def _validate_filters(filters: Iterable[ValidFilter]) -> list[FilterExpression]: def validate(filterr): if isinstance(filterr, list): if all(isinstance(obj, str) for obj in filterr): filterr = FilterExpression(filterr, special=\"strlist\") elif all(isinstance(obj, Stock) for obj in filterr): filterr = FilterExpression(filterr, special=\"stocklist\") elif isinstance(filterr, dict): if all( ((isinstance(k, str), isinstance(v, Stock)) == (True, True)) for k, v in filterr.items() ): filterr = FilterExpression(filterr, special=\"dict\") if not isinstance(filterr, FilterExpression): raise TypeError(f\"Invalid filter {filterr}\") return filterr return [validate(filterr) for filterr in filters]"}
{"text_id": "9349", "text": "docstring: def fraction_predicted_classes(ruleset: MIDSRuleSet, test_dataframe, target_attributes: List[TargetAttr] ) -> Tuple[float, Dict[TargetAttr, float]]: if type(ruleset) != MIDSRuleSet: raise Exception(\"Type of ruleset must be IDSRuleSet\") warnings.warn( \"Ugly conversion to string to deal with numerical attributes.\" \" Clean this up (look at Survived in Titanic).\") values_in_data_per_target_attribute: Dict[TargetAttr, Set[TargetVal]] = {} predicted_values_per_target_attribute: Dict[TargetAttr, Set[TargetVal]] = {} for target_attr in target_attributes: values_as_str: List[str] = [str(val) for val in test_dataframe[target_attr].values] values_in_data_per_target_attribute[target_attr] = set(values_as_str) predicted_values_per_target_attribute[target_attr] = set() target_attribute_set: Set[TargetAttr] = set(target_attributes) for rule in ruleset.ruleset: consequent: Consequent = rule.car.consequent for literal in consequent.get_literals(): predicted_attribute: TargetAttr = literal.get_attribute() predicted_value: TargetVal = literal.get_value() if predicted_attribute in target_attribute_set: predicted_value_str = str(predicted_value) predicted_values: Set[TargetVal] = predicted_values_per_target_attribute[predicted_attribute] if predicted_value_str in values_in_data_per_target_attribute[predicted_attribute]: predicted_values.add(predicted_value_str) frac_predicted_classes_per_target_attr: Dict[TargetAttr, float] = {} avg_frac_predicted_classes: float = 0 for target_attr in values_in_data_per_target_attribute.keys(): values_occuring_in_data = values_in_data_per_target_attribute[target_attr] predicted_values = predicted_values_per_target_attribute[target_attr] domain_size_in_test_data = len(values_occuring_in_data) nb_of_predicted_values = len(predicted_values) frac_classes: float = nb_of_predicted_values / domain_size_in_test_data frac_predicted_classes_per_target_attr[target_attr] = frac_classes avg_frac_predicted_classes += frac_classes nb_of_target_attrs = len(target_attributes) avg_frac_predicted_classes = avg_frac_predicted_classes / nb_of_target_attrs if not is_valid_fraction(avg_frac_predicted_classes): raise Exception(\"Avg fraction predicted classes examples is not within [0,1]: \" + str(avg_frac_predicted_classes)) return avg_frac_predicted_classes, frac_predicted_classes_per_target_attr"}
{"text_id": "9350", "text": "docstring: def flatten_corr_matrix(self, corr_matrix): rows = corr_matrix.index.values.tolist() columns = corr_matrix.columns corr_flatten = pd.DataFrame( [(r, c, corr_matrix[r][c]) for c in columns for r in rows], columns=[\"x\", \"y\", \"corr\"], ) return corr_flatten"}
{"text_id": "9351", "text": "docstring: def _get_tag_dict(self): return at_tags"}
{"text_id": "9352", "text": "docstring: def SaveMapFrame(frame, map_id, output_file, hdr=None, compress=True, overwrite=False): if frame.type != core.G3FrameType.Map: return if frame['Id'] != map_id and frame.type == core.G3FrameType.Map: raise ValueError(f\"Frame Id {frame['Id']} != to map_id:{map_id}\") core.log_debug(f\"Matched Frame.type:{frame.type} and map_id:{frame['Id']}\") T = frame['T'] Q = frame.get('Q', None) U = frame.get('U', None) W = frame.get('Wpol', frame.get('Wunpol', None)) save_skymap_fits( output_file, T=T, Q=Q, U=U, W=W, overwrite=overwrite, compress=compress, hdr=hdr, )"}
{"text_id": "9353", "text": "docstring: def cross_val_rmse(model, X, y, cv=5, random_state=None, model_name=None, verbose=False): X = np.array(X) y = np.array(y) kf = KFold(n_splits=cv, shuffle=False, random_state=random_state) fold_indices = kf.split(X) if verbose: print(f\"Starting {cv}-fold cross validation\") rmse_list = [] for i, indices in enumerate(fold_indices): train_indices = indices[0] val_indices = indices[1] timer_start = time.perf_counter_ns() model.fit(X[train_indices], y[train_indices]) train_time = time.perf_counter_ns() - timer_start y_pred = model.predict(X) train_rmse = np.sqrt(mean_squared_error(y[train_indices], y_pred[train_indices])) val_rmse = np.sqrt(mean_squared_error(y[val_indices], y_pred[val_indices])) if verbose: print(f\"fit {i}\\ttrain RMSE: {train_rmse:.3f}\\t val RMSE: {val_rmse:.3f}\\t train time: {train_time * 1e-9:.2f} s\") rmse_list.append([train_rmse, val_rmse]) index = ['train', 'val'] if model_name is not None: for i in range(len(index)): index[i] = f\"{model_name} {index[i]}\" df = pd.DataFrame(np.array(rmse_list).T, index=index, columns=[\"fold \" + str(i) for i in range(cv)]) df_mean = df.mean(axis=1) df_std = df.std(axis=1) df['mean'] = df_mean df['std'] = df_std return df"}
{"text_id": "9354", "text": "docstring: def flip(self): self.__move_up = not self.__move_up self.__img = pygame.transform.rotate(self.__img, 180)"}
{"text_id": "9355", "text": "docstring: def channels(): if request.method == \"GET\": return jsonify({\"channels\": channels_list}) elif request.method == \"POST\": print(channels_list) if len(request.json.keys()) != 1 or list(request.json.keys())[0] != \"chName\": print(list(request.json.keys())[0]) return jsonify({\"status\": \"400\", \"message\": \"Bad request\"}), 400 channel_name = request.json.get(\"chName\") if channel_name in channels_list: return jsonify({\"valid\": False}), 409 channels_list.append(channel_name) messages_memory[channel_name] = [] return jsonify({\"valid\": True}) return 405"}
{"text_id": "9356", "text": "docstring: def reverse_lookup_keys_paginated(self, modelobj, manager=None, max_results=None, continuation=None): if manager is None: manager = modelobj.manager return manager.index_keys_page( self.model_cls, self.index_name, modelobj.key, max_results=max_results, continuation=continuation)"}
{"text_id": "9357", "text": "docstring: def expandString(self, s): if self.compressMethod == 0: return self.zlib.decompress(s) elif self.compressMethod == 1: return self.lzma.decompress(s) elif self.compressMethod == 2: return self.bz2.decompress(s)"}
{"text_id": "9358", "text": "docstring: def count_unique_items(items): unique_items = [] unique_item_counts = [] for item in items: if item in unique_items: unique_item_counts[unique_items.index(item)] +=1 else: unique_items.append(item) unique_item_counts.append(1) return zip(unique_items, unique_item_counts)"}
{"text_id": "9359", "text": "docstring: def uniform_map(cls, length, rate, num_loci=None): return cls([0, length], [rate, 0], num_loci=num_loci)"}
{"text_id": "9360", "text": "docstring: def WebCoverageService(url, version=None, xml=None, cookies=None, timeout=30, auth=None, headers=None): if not auth: auth = Authentication() if version is None: if xml is None: reader = wcsBase.WCSCapabilitiesReader(auth=auth, headers=headers) request = reader.capabilities_url(url) xml = openURL( request, cookies=cookies, timeout=timeout, auth=auth, headers=headers).read() capabilities = etree.etree.fromstring(xml) version = capabilities.get('version') del capabilities clean_url = clean_ows_url(url) if version == '1.0.0': return wcs100.WebCoverageService_1_0_0.__new__( wcs100.WebCoverageService_1_0_0, clean_url, xml, cookies, auth=auth, headers=headers) elif version == '1.1.0': return wcs110.WebCoverageService_1_1_0.__new__( wcs110.WebCoverageService_1_1_0, url, xml, cookies, auth=auth, headers=headers) elif version == '1.1.1': return wcs111.WebCoverageService_1_1_1.__new__( wcs111.WebCoverageService_1_1_1, url, xml, cookies, auth=auth, headers=headers) elif version == '2.0.0': return wcs200.WebCoverageService_2_0_0.__new__( wcs200.WebCoverageService_2_0_0, url, xml, cookies, auth=auth, headers=headers) elif version == '2.0.1': return wcs201.WebCoverageService_2_0_1.__new__( wcs201.WebCoverageService_2_0_1, url, xml, cookies, auth=auth, headers=headers)"}
{"text_id": "9361", "text": "docstring: def format_output(output, format='yaml'): output_format = format.lower() try: return supported_formats[output_format](output) except KeyError: raise exc.HTTPUnsupported(\"The format(%s) is unsupported.\" % output_format)"}
{"text_id": "9362", "text": "docstring: def add(self, layer): if not isinstance(layer, base_layer.Layer): raise TypeError('The added layer must be ' 'an instance of class Layer. ' 'Found: ' + str(layer)) self.built = False if not self._layers: set_inputs = False if not isinstance(layer, InputLayer): first_layer = layer if isinstance(layer, (Model, Sequential)): if not layer.layers: raise ValueError('Cannot add an empty model ' 'to a `Sequential` model.') first_layer = layer.layers[0] while isinstance(first_layer, (Model, Sequential)): first_layer = first_layer.layers[0] batch_shape = first_layer._batch_input_shape dtype = first_layer.dtype if hasattr(first_layer, '_batch_input_shape'): batch_shape = first_layer._batch_input_shape dtype = first_layer.dtype x = Input( batch_shape=batch_shape, dtype=dtype, name=layer.name + '_input') layer(x) set_inputs = True else: batch_shape = None else: assert len(layer._inbound_nodes[-1].output_tensors) == 1 set_inputs = True if set_inputs: if len(layer._inbound_nodes[-1].output_tensors) != 1: raise ValueError('All layers in a Sequential model ' 'should have a single output tensor. ' 'For multi-output layers, ' 'use the functional API.') self.outputs = [layer._inbound_nodes[-1].output_tensors[0]] self.inputs = layer_utils.get_source_inputs(self.outputs[0]) elif self.outputs: output_tensor = layer(self.outputs[0]) if isinstance(output_tensor, list): raise TypeError('All layers in a Sequential model ' 'should have a single output tensor. ' 'For multi-output layers, ' 'use the functional API.') self.outputs = [output_tensor] if self.inputs: self.build() else: self._layers.append(layer)"}
{"text_id": "9363", "text": "docstring: def run_worker(worker_url, context, idx): printdbg('run_worker {} starting'.format(idx)) socket = context.socket(zmq.REQ) socket.identity = (u\"Worker-%d\" % (idx)).encode('ascii') socket.connect(worker_url) yield from socket.send(b\"READY\") try: while True: address, empty, request = yield from socket.recv_multipart() msg = request.decode('ascii') print('{} received request \"{}\"'.format( socket.identity.decode('ascii'), msg )) msg = '{}-OK'.format(msg).encode('ascii') yield from socket.send_multipart([address, b'', msg]) print('{} sent reply \"{}\"'.format( socket.identity.decode('ascii'), msg )) except asyncio.CancelledError: printdbg('worker {} cancelled.') return"}
{"text_id": "9364", "text": "docstring: def _get_length_range(self): for rng in LOG_LENGTHS: if LOG_LENGTHS[rng][0] <= self.length <= LOG_LENGTHS[rng][1]: return rng"}
{"text_id": "9365", "text": "docstring: def capstderr(self): return ''.join(content for (prefix, content) in self.get_sections('Captured stderr'))"}
{"text_id": "9366", "text": "docstring: def load_minigraph_with_golden_empty_input(duthost): initial_config = get_running_config(duthost) empty_input = {} reload_minigraph_with_golden_config(duthost, empty_input) current_config = get_running_config(duthost) pytest_assert(initial_config == current_config, \"Running config differs.\")"}
{"text_id": "9367", "text": "docstring: def add_forward_pump(self, wl, power, wl_bandwidth=0, mode_shape_parameters=None, label=''): self.channels.add_forward_pump(wl, wl_bandwidth, power, mode_shape_parameters, label)"}
{"text_id": "9368", "text": "docstring: def clear_cache(self): for node in self.node_list: if hasattr(node, 'clear_cache'): node.clear_cache()"}
{"text_id": "9369", "text": "docstring: def _get_artifact_context(run, file_type): sha1sum = None image_file = False log_file = False config_file = False if request.path == '/image': image_file = True if file_type == 'before': sha1sum = run.ref_image elif file_type == 'diff': sha1sum = run.diff_image elif file_type == 'after': sha1sum = run.image else: abort(400) elif request.path == '/log': log_file = True if file_type == 'before': sha1sum = run.ref_log elif file_type == 'diff': sha1sum = run.diff_log elif file_type == 'after': sha1sum = run.log else: abort(400) elif request.path == '/config': config_file = True if file_type == 'before': sha1sum = run.ref_config elif file_type == 'after': sha1sum = run.config else: abort(400) return image_file, log_file, config_file, sha1sum"}
{"text_id": "9370", "text": "docstring: def updatecase(self,attr,old,new): self.loadbutton.button_type='danger' self.loadbutton.label = 'Loading' self.case = new cases = create_case_list() times = create_time_list(self.case) self.selectcase.options = list(cases) self.selecttime.options = list(times) self.selecttime.value = times[0]"}
{"text_id": "9371", "text": "docstring: def check_measurement_quality( classifier: Classifier, run_id: int, db_name: Optional[str] = None, db_folder: Optional[str] = None, ) -> bool: if db_name is None: db_name, db_folder = nt.get_database() elif db_folder is None: _, db_folder = nt.get_database() quality = classifier.predict(run_id, db_name, db_folder=db_folder) return any(quality)"}
{"text_id": "9372", "text": "docstring: def __update_to_approved_imeis(self, imeis): imei_status = 'whitelist' imei_delta_status = 'update' updated_imeis = [] for imei in imeis: if ApprovedImeis.exists(imei): imei_ = ApprovedImeis.get_imei(imei) imei_.status = imei_status if imei_.exported: imei_.delta_status = imei_delta_status updated_imeis.append(imei_) ApprovedImeis.bulk_insert_imeis(updated_imeis)"}
{"text_id": "9373", "text": "docstring: async def emulated_roku_stop(event): LOGGER.debug(\"Stopping emulated_roku %s\", self.roku_usn) self._unsub_stop_listener = None await self._api_server.close()"}
{"text_id": "9374", "text": "docstring: def nancumprod(a, axis=None, dtype=None, out=None): a = _replace_nan(a, 1, out=out) return cumprod(a, axis=axis, dtype=dtype, out=out)"}
{"text_id": "9375", "text": "docstring: def inbound_value_filter(value): if hasattr(value, \"value\"): value = value.value if isinstance(value, list): if not value: return None if len(value) == 1: return value[0] elif isinstance(value, datetime.datetime): return datetime_to_seconds(value) return value"}
{"text_id": "9376", "text": "docstring: def from_dict(cls, config_dict: Dict, **kwargs): return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False) config = cls(**config_dict) if hasattr(config, \"pruned_heads\"): config.pruned_heads = dict((int(key), value) for key, value in config.pruned_heads.items()) to_remove = [] for key, value in kwargs.items(): if hasattr(config, key): setattr(config, key, value) to_remove.append(key) for key in to_remove: kwargs.pop(key, None) logger.info(\"Model config %s\", str(config)) if return_unused_kwargs: return config, kwargs else: return config"}
{"text_id": "9377", "text": "docstring: def create_inception_graph(): with tf.Session() as sess: model_filename = os.path.join( FLAGS.model_dir, 'classify_image_graph_def.pb') with gfile.FastGFile(model_filename, 'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) bottleneck_tensor, jpeg_data_tensor, resized_input_tensor = ( tf.import_graph_def(graph_def, name='', return_elements=[ BOTTLENECK_TENSOR_NAME, JPEG_DATA_TENSOR_NAME, RESIZED_INPUT_TENSOR_NAME])) return sess.graph, bottleneck_tensor, jpeg_data_tensor, resized_input_tensor"}
{"text_id": "9378", "text": "docstring: def toggle_dropdown(self, sender): if self.dropdown_presented: self.superview.remove_subview(self.touch_panel) self.superview.remove_subview(self.list_view) self.dropdown_presented = False self.update_interval = 0.0 else: self.list_view.flex = 'T' self.list_view.border_width = 1 self.list_view.border_color = '#eee' self.list_view.background_color = '#fff' self.dropdown_presented = True self.update_interval = 0.3 self.superview.add_subview(self.touch_panel) self.superview.add_subview(self.list_view) pass"}
{"text_id": "9379", "text": "docstring: def _concat_singles(dici): unlucky_guys = dici[\"all_male\"].loc[dici[\"all_male\"].index.isin(dici[\"lucky_guys\"].index) == False, :] unhappy_girls = dici[\"all_female\"].loc[dici[\"all_female\"].index.isin(dici[\"happy_girls\"].index) == False, :] girls = pd.concat((dici[\"happy_girls\"], unhappy_girls), axis = 0) guys = pd.concat((dici[\"lucky_guys\"], unlucky_guys), axis = 0) assert( len(unlucky_guys) + len(dici[\"lucky_guys\"]) == len(dici[\"all_male\"]) ), \"Error in concating guys\" assert( len(unhappy_girls) + len(dici[\"happy_girls\"]) == len(dici[\"all_female\"]) ), \"Error in concating girls\" out_dict = {\"girls\" : girls, \"guys\": guys} return out_dict"}
{"text_id": "9380", "text": "docstring: def alter_template(template_name, network, content, user): try: ip_version = IP_VERSION.IPv4[0] if network == IP_VERSION.IPv4[ 1] else IP_VERSION.IPv6[0] path = \"%s%s/%s%s\" % (PATH_ACL, ip_version, PATH_ACL_TEMPLATES, template_name) chdir(PATH_TYPES.TEMPLATE, ip_version, path) Git.synchronization() File.write(template_name, content) Git.commit(template_name, \"%s alterou o arquivo %s\" % (user.get_username(), template_name)) Git.push() logger.info(\"%s alterou no GIT o arquivo: %s\" % (user.get_username(), (path + template_name))) except (GITCommandError, FileError, Exception), e: logger.error(\"Erro quando o usu\u00e1rio %s tentou atualizar o arquivo: %s no Git\" % ( user.get_username(), (path + template_name))) logger.error(e) raise GITCommandError(e)"}
{"text_id": "9381", "text": "docstring: def _get_datetime_str(self): return datetime.datetime.now().strftime('%Y-%m-%d // %H:%M:%S')"}
{"text_id": "9382", "text": "docstring: def execute( query): cmd = dict() cmd[\"name_\"] = \"\" cmd[\"type_\"] = \"Database.execute\" s = comm.send_and_receive_socket(cmd) comm.send_string(s, query) msg = comm.recv_string(s) s.close() if msg != \"Success!\": raise Exception(msg)"}
{"text_id": "9383", "text": "docstring: def validate_tagged_tree(tree, ctx): if not isinstance(ctx, Context): ctx = Context(ctx) def walker(node): tag_name = tagged.get_tag(node) if tag_name is not None: tag = ctx.type_index.get_asdftype_from_yaml_tag(tag_name) if tag is not None: tag.validate(node) return treeutil.walk(tree, walker)"}
{"text_id": "9384", "text": "docstring: def wait_for(self, css_selector, timeout=None): from selenium.webdriver.common.by import By from selenium.webdriver.support import expected_conditions as ec if timeout is None: timeout = self.default_timeout self.wait_until( ec.presence_of_element_located((By.CSS_SELECTOR, css_selector)), timeout)"}
{"text_id": "9385", "text": "docstring: def pattern(effect): return { \"MISMATCHED\": \"0.0.0.dev0\", \"MISSING\": \"MISSING\", \"KEY_ERROR\": \"UNKNOWN\", \"NONE\": \"UNKNOWN\", }[effect]"}
{"text_id": "9386", "text": "docstring: def runTxLoop(self): while True: if not self.txbuf: if not self.txque: break self.txbuf = self.txque.popleft() self.fire('sock:tx:pop') sent = self.send(self.txbuf) self.txbuf = self.txbuf[sent:] if self.txbuf: break if not self.txbuf and not self.txque: return False return True"}
{"text_id": "9387", "text": "docstring: def add_new_detection(self): mindist = 4 ww = np.where(self._fitregions == 1) xfit = self.specx.copy()[ww] yfit = self.specdata.copy()[ww] from scipy.optimize import curve_fit npix = len(xfit) if npix <= 3: return ampl = np.max(yfit) mean = sum(xfit * yfit) / sum(yfit) sigma = sum(yfit * (xfit - mean) ** 2) / sum(yfit) gaus = lambda x, a, x0, sigma: a * np.exp(-(x - x0) ** 2 / (2 * sigma ** 2)) popt, pcov = curve_fit(gaus, xfit, yfit, p0=[ampl, mean, sigma]) print(ampl, mean, sigma, popt[1]) new_detn = popt[1] cls_line = np.min(np.abs(self._detns - new_detn)) if cls_line > mindist: detns = np.append(self._detns, new_detn) arsrt = np.argsort(detns) self._detns = detns[arsrt] self._detnsy = self.get_ann_ypos() self._lineids = np.append(self._lineids, 0)[arsrt] self._lineflg = np.append(self._lineflg, 0)[arsrt] else: self.update_infobox(\"New detection is <{0:d} pixels of a detection - ignoring\".format(mindist)) self._fitregions = np.zeros_like(self._fitregions) return"}
{"text_id": "9388", "text": "docstring: def update(self, steps, current, count, field = None): if not field == None: if self.particles: for particle in self.particles: self.w.delete(particle) self.particles = [] for aggregate in field.aggregates: for x in aggregate.particles: for y in aggregate.particles[x]: x1, y1 = self._map_coords(x, y) x2, y2 = self._map_coords(x + 1, y + 1) self.particles.append(self.w.create_rectangle(x1, y1, x2, y2, fill = 'black')) self.w.delete(self.text) count_aggregated = field.countAggregated() if not field == None else None text = self._status_string(steps, current, count, count_aggregated) self.text = self.w.create_text(25, 0, anchor= NW, text = text) self.master.update() time.sleep(self.delay)"}
{"text_id": "9389", "text": "docstring: def load_browserstack_credentials(): credentials = bstack_creds() if not credentials: username = os.environ.get(\"BSTACK_USERNAME\") access_key = os.environ.get(\"BSTACK_ACCESS_KEY\") if all([username, access_key]): return (username, access_key) return False return credentials"}
{"text_id": "9390", "text": "docstring: def _detail_dictionary(worker: AbstractWorker, my_dict: Tuple) -> Dict: pieces = {} for key, value in my_dict: detailed_key = serde._detail(worker, key) try: detailed_key = detailed_key.decode(\"utf-8\") except AttributeError: pass detailed_value = serde._detail(worker, value) try: detailed_value = detailed_value.decode(\"utf-8\") except AttributeError: pass pieces[detailed_key] = detailed_value return pieces"}
{"text_id": "9391", "text": "docstring: def all(self, **kwargs): valid_keys = ['limit', 'last_id', 'sort_order', 'user_id'] faxes = self.client.get('/outbound/faxes', kwargs, valid_keys) return [OutboundFax(self.client, fax) for fax in faxes]"}
{"text_id": "9392", "text": "docstring: def mkdir_p(dir): try: pathlib.Path(dir).mkdir(parents=True) except FileExistsError: pass"}
{"text_id": "9393", "text": "docstring: def inputs(name, batch_size, num_epochs): if not num_epochs: num_epochs = None filename = os.path.join(FLAGS.train_dir, 'data', '{}_{}.tfrecords'.format(name, records.tfrecord_name())) with tf.name_scope('input'): filename_queue = tf.train.string_input_producer( [filename], num_epochs=num_epochs) image, label = read_and_decode(filename_queue) images, sparse_labels = tf.train.shuffle_batch( [image, label], batch_size=batch_size, num_threads=8, capacity=1000 + 3 * batch_size, min_after_dequeue=1000) return images, sparse_labels"}
{"text_id": "9394", "text": "docstring: def clean_corrupted_imgs(self, accepted_formats = [\".jpg\", \".png\", \".jpeg\"]): imgs_path = os.path.join(self.output_dir, self.current_participant.replace(\" \", \"_\")) for filename in os.listdir(imgs_path): img_path = os.path.join(imgs_path, filename) photo_name, extension = os.path.splitext(filename) if(not extension.lower() in accepted_formats): os.remove(img_path) continue try: _ = Image.open(img_path) except: os.remove(img_path)"}
{"text_id": "9395", "text": "docstring: def status(self): if self.mode == 0: return \"impossible\" if self.mode == 'halt': return \"finished drawing\" if self.mode == 'drive': return \"drive {:.2f} cm\".format(self.delta_pos) if self.mode == 'rotate': return \"rotate {:.2f} degrees\".format(rad_to_deg(self.delta_angle))"}
{"text_id": "9396", "text": "docstring: def load_guess_group(self, update_spins=True): charge_density_file = None for ff in self.restart_file_list: if \"rho.sxb\" in ff.split(\"/\")[-1]: charge_density_file = ff wave_function_file = None for ff in self.restart_file_list: if \"waves.sxb\" in ff.split(\"/\")[-1]: wave_function_file = ff self.input.sphinx.initialGuess.setdefault(\"waves\", Group()) self.input.sphinx.initialGuess.waves.setdefault(\"lcao\", Group()) self.input.sphinx.initialGuess.waves.setdefault(\"pawBasis\", True) if wave_function_file is not None: self.input.sphinx.initialGuess.setdefault(\"exchange\", Group()) self.input.sphinx.initialGuess.exchange.setdefault( \"file\", '\"' + wave_function_file + '\"' ) if charge_density_file is None: self.input.sphinx.initialGuess.setdefault(\"rho\", Group({\"atomicOrbitals\": True})) else: self.input.sphinx.initialGuess.setdefault( \"rho\", Group({\"file\": '\"' + charge_density_file + '\"'}) ) if self._spin_enabled: if any( [ True if isinstance(spin, list) or isinstance(spin, np.ndarray) else False for spin in self.structure.get_initial_magnetic_moments() ] ): raise ValueError(\"SPHInX only supports collinear spins.\") else: rho = self.input.sphinx.initialGuess.rho rho.get(\"atomicSpin\", create=True) if update_spins: rho.atomicSpin.clear() if len(rho.atomicSpin) == 0: for spin in self.structure.get_initial_magnetic_moments()[ self.id_pyi_to_spx ]: rho[\"atomicSpin\"].append( Group({ \"label\": '\"spin_' + str(spin) + '\"', \"spin\": str(spin) }) ) if \"noWavesStorage\" not in self.input.sphinx.initialGuess: self.input.sphinx.initialGuess[\"noWavesStorage\"] = \\ not self.input[\"WriteWaves\"]"}
{"text_id": "9397", "text": "docstring: def layout(): settings_menu_items = [\"Unused\", [\"&Change Theme\", \"General Settings\"]] exit_menu_items = [\"Unused\", [\"&Exit\"]] about_menu_items = [\"Unused\", [\"&About\"]] return [ [ sg.ButtonMenu(\"Settings\", settings_menu_items, key=\"-SETTINGS-MENU-\", size=(9,1)), sg.Text(\"\", pad=(98,0)), sg.Button(\"About\", pad=(0,0)), sg.Button(\"Exit\", pad=(0,0)) ], [sg.HSeparator()], [ sg.Text(\"\\n\\nThis is a custom made GUI for the lms menu.\\n\"+ \"\\nPlease note that this is a beta test and not the\\n\"+ \"final product, thus it is subject to constant change\\n\\n\", font=\"Calibri 9\") ], [sg.HSeparator()], [ sg.Column([[sg.Button(\"Assignments\")]]), sg.VSeperator(pad=(100,0)), sg.Column([[sg.Button(\"Reviews\")]]) ] ]"}
{"text_id": "9398", "text": "docstring: def check_message_decodes(encoded, decoded): event_buffer = EventStreamBuffer() event_buffer.add_data(encoded) messages = list(event_buffer) assert len(messages) == 1 assert_message_equal(messages[0], decoded)"}
{"text_id": "9399", "text": "docstring: def to_dict(self) -> Dict: _dict = {} if hasattr(self, 'requests') and self.requests is not None: _dict['requests'] = self.requests.to_dict() if hasattr(self, 'limits') and self.limits is not None: _dict['limits'] = self.limits.to_dict() return _dict"}
{"text_id": "9400", "text": "docstring: def generate_annotation_xml( annotations: List[List[tuple]], img_prop: List[Any], element_types: List[str], ) -> Any: root = et.Element(\"annotation\") et.SubElement(root, \"folder\").text = os.path.basename( os.path.abspath(config.GENERATED_ANNOTATION_DIR) ) et.SubElement(root, \"filename\").text = f\"{img_prop[-1]}.png\" et.SubElement(root, \"path\").text = ( os.path.abspath(config.GENERATED_ANNOTATION_DIR) + f\"\\\\{img_prop[-1]}.xml\" ) source = et.SubElement(root, \"source\") et.SubElement(source, \"database\").text = \"Unknown\" size = et.SubElement(root, \"size\") et.SubElement(size, \"height\").text = str(img_prop[0][0]) et.SubElement(size, \"width\").text = str(img_prop[0][1]) et.SubElement(size, \"depth\").text = str(img_prop[0][2]) et.SubElement(root, \"segmented\").text = \"0\" for index, element_type in enumerate(element_types): element = et.SubElement(root, \"object\") et.SubElement(element, \"name\").text = os.path.basename(element_type) et.SubElement(element, \"pose\").text = \"Unspecified\" et.SubElement(element, \"truncated\").text = \"0\" et.SubElement(element, \"difficult\").text = \"0\" bndbox = et.SubElement(element, \"bndbox\") et.SubElement(bndbox, \"xmin\").text = str(annotations[index][0][0]) et.SubElement(bndbox, \"ymin\").text = str(annotations[index][0][1]) et.SubElement(bndbox, \"xmax\").text = str(annotations[index][1][0]) et.SubElement(bndbox, \"ymax\").text = str(annotations[index][1][1]) tree = et.ElementTree(root) return tree"}
{"text_id": "9401", "text": "docstring: def sleep(self, seconds=0): return self.server.sleep(seconds)"}
{"text_id": "9402", "text": "docstring: def numberOf(factor, uid): if uid is None: return executeSQL(\"select count(*) from (select distinct %s from \\ temp) as t;\", False, factor) else: return executeSQL(\"select count(*) from (select distinct %s from \\ temp where uid=%d) as t\", False, factor, uid)"}
{"text_id": "9403", "text": "docstring: def finetune_function(self, pl_module, epoch: int, optimizer, opt_idx: int): if epoch == 0: self.unfreeze_and_add_param_group( pl_module.backbone, optimizer, 0.1, train_bn=self.train_bn, initial_denom_lr=self.initial_denom_lr )"}
{"text_id": "9404", "text": "docstring: def gateway_get_client_token(gateway_name: str): gateway, gateway_params = get_payment_gateway(gateway_name) return gateway.get_client_token(**gateway_params)"}
{"text_id": "9405", "text": "docstring: def RequestArgsGetHeader(args, kwargs, header, default=None): if 'headers' in kwargs: return kwargs['headers'].get(header, default) elif len(args) > 3: return args[3].get(header, default) else: return default"}
{"text_id": "9406", "text": "docstring: def is_in_sound_radius(self): return is_in_radius(self, self.target_player, self.sound_radius)"}
{"text_id": "9407", "text": "docstring: def __IndexEnglishArticles(self): self.__original_articles = [] for root, _, files in os.walk(self.__original_root): for name in files: if not name == '.DS_Store' and re.search(r'\\/en$', root): self.__original_articles.append(Article(os.path.dirname(root))) return self.__original_articles"}
{"text_id": "9408", "text": "docstring: def installPip(printOut=False): cmd = ( \"py -3 pipinstaller\\\\resources\\get-pip.py --user\" if sys.platform == \"win32\" else \"python3 pipinstaller/resources/get-pip.py --user\" ) process = subprocess.Popen( cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE ) if not printOut: process.wait() else: while process.poll() is None: for line in process.stdout: print(line.decode(), end=\"\") noError = True for line in process.stderr: if line.lower().startswith(b\"deprecation\"): break print(\"Err: {}\".format(line.decode()), end=\"\") noError = ( False ) if not noError: print(\"Something went wrong in installation, see information above.\") return noError"}
{"text_id": "9409", "text": "docstring: def analyze(problem, Y, M=4, print_to_console=False, seed=None): if seed: np.random.seed(seed) D = problem['num_vars'] if Y.size % (D) == 0: N = int(Y.size / D) else: print(\"\"\" Error: Number of samples in model output file must be a multiple of D, where D is the number of parameters in your parameter file. \"\"\") exit() omega = np.zeros([D]) omega[0] = math.floor((N - 1) / (2 * M)) m = math.floor(omega[0] / (2 * M)) if m >= (D - 1): omega[1:] = np.floor(np.linspace(1, m, D - 1)) else: omega[1:] = np.arange(D - 1) % m + 1 if print_to_console: print(\"Parameter First Total\") Si = ResultDict((k, [None] * D) for k in ['S1', 'ST']) Si['names'] = problem['names'] for i in range(D): l = np.arange(i * N, (i + 1) * N) Si['S1'][i] = compute_first_order(Y[l], N, M, omega[0]) Si['ST'][i] = compute_total_order(Y[l], N, omega[0]) if print_to_console: print(\"%s %f %f\" % (problem['names'][i], Si['S1'][i], Si['ST'][i])) return Si"}
{"text_id": "9410", "text": "docstring: def go_to_relative( self, xyt_position, use_map=False, close_loop=False, smooth=False, wait=True, ): if self._done: self._done = False self._robot.base.go_to_relative( xyt_position, use_map=use_map, close_loop=close_loop, smooth=smooth, wait=wait ) self._done = True"}
{"text_id": "9411", "text": "docstring: def init_http_handlers(redis_host: str, redis_port: int, redis_pass: str) -> None: if not len(redis_host) or (redis_port <= 0 or redis_port > 65535) or not len(redis_pass): raise ValueError(\"Invalid Redis Details\") try: HttpHandlers.redis_client = redis.Redis(host=redis_host, port=redis_port, password=redis_pass, db=0) except Exception as ex: raise ConnectionError(str(ex))"}
{"text_id": "9412", "text": "docstring: def logic_StepEdge(session): @event.listens_for(session,'before_flush') def history_and_tc(session,flush_context,instances): def update_TC(stepedge): start=stepedge.step_id end=stepedge.dependency_id ail_StepTC=aliased(StepTC) start_in_tc=session.query( StepTC.step_id ).filter( StepTC.tc_id == start ) in_end_tc=session.query( StepTC.tc_id ).filter( StepTC.step_id == end ) tcs=in_end_tc.all() tcs.append(end) steps=start_in_tc.all() steps.append(start) for step in steps: for tc in tcs: if not session.query(StepTC).filter(StepTC.step_id==step,StepTC.tc_id==tc).first(): stc=StepTC(step_id=step,tc_id=tc) session.add(stc) problem is here? #session.add(StepTC(step_id=step,tc_id=tc)) def delete_TC(stepedge): this is going to be really slow for large deletes? suspects,trusty,new,delete=getSusTru(stepedge,session) for obj in session.deleted: if type(obj) is StepEdge: session.add(StepEdgeVersion(step_id=obj.step_id,dependency_id=obj.dependency_id,added=False)) delete_TC(obj) elif type(obj) is StepEdgeVersion: session.expunge(obj) raise AttributeError('StepEdgeVersion is write only!') for obj in session.new: if type(obj) is StepEdge: #print('wtf m8!') this is being called waaay too much session.add(StepEdgeVersion(step_id=obj.step_id,dependency_id=obj.dependency_id,added=True)) update_TC(obj) @event.listens_for(session,'before_attach') def check_for_cycles(session,instance): monumentally slow for repeated adds and high edge counts if type(instance) is StepEdge: since this is now called at before_attach the edge cannot find itself! edges=session.query(StepEdge.step_id,StepEdge.dependency_id).all() edge_tuple=(instance.step_id,instance.dependency_id) if edge_tuple in edges: raise ValueError('Edge already exists!') edges.append(edge_tuple) def makeDepDict(edges): depDict={} for step,dep in edges: depDict[step]=set((dep,)) for step,dep in edges: depDict[step].add(dep) return depDict def topoSort(depDict,revDepDict): starts=[node for node in depDict.keys() if node not in revDepDict.keys()] L=[] while starts: node=starts.pop() L.append(node) discards=set() try: for m in depDict[node]: revDepDict[m].discard(node) discards.add(m) if not revDepDict[m]: starts.append(m) depDict[node].difference_update(discards) except KeyError: pass check=set() (check.update(v) for v in depDict.values()) (check.update(v) for v in revDepDict.values()) if check: raise TypeError('NOT ACYCLIC!!') else: return L depDict=makeDepDict(edges) if not len(edges): session.flush() FIXME nasty problem with my implementation of transitive closure return False def cycle(start,node): if node==start: return True #deps=edges[:,1][edges[:,0]==node] this seems to be the culpret for the slow down try: if start in depDict[node]: return True else: for n in depDict[node]: if cycle(start,n): return True return False except KeyError: return False ValueError('[!] Edge %s -> %s would add cycle! Not adding!'%(instance.step_id,instance.dependency_id)) if session.query(StepTC.tc_id).filter(StepTC.step_id==instance.dependency_id, StepTC.tc_id==instance.step_id).first(): raise ValueError('[!] Edge %s -> %s would add cycle! Not adding!'%(instance.step_id,instance.dependency_id)) if cycle(instance.step_id,instance.dependency_id): I don't want to roll back the whole session here do it?! #XXX apparently delete autoflushes!??! this will trigger history_table_delete printD('missed one!') raise ValueError('[!] Edge %s -> %s would add cycle! Not adding!'%(instance.step_id,instance.dependency_id)) else: session.flush()"}
{"text_id": "9413", "text": "docstring: def path_properties(self): def path_metric(pth, req): return [ { 'metric-type': 'SNR-bandwidth', 'accumulative-value': round(mean(pth[-1].snr), 2) }, { 'metric-type': 'SNR-0.1nm', 'accumulative-value': round(mean(pth[-1].snr + lin2db(req.baud_rate / 12.5e9)), 2) }, { 'metric-type': 'OSNR-bandwidth', 'accumulative-value': round(mean(pth[-1].osnr_ase), 2) }, { 'metric-type': 'OSNR-0.1nm', 'accumulative-value': round(mean(pth[-1].osnr_ase_01nm), 2) }, { 'metric-type': 'reference_power', 'accumulative-value': req.power }, { 'metric-type': 'path_bandwidth', 'accumulative-value': req.path_bandwidth } ] if self.path_request.bidir: path_properties = { 'path-metric': path_metric(self.computed_path, self.path_request), 'z-a-path-metric': path_metric(self.reversed_computed_path, self.path_request), 'path-route-objects': self.detailed_path_json } else: path_properties = { 'path-metric': path_metric(self.computed_path, self.path_request), 'path-route-objects': self.detailed_path_json } return path_properties"}
{"text_id": "9414", "text": "docstring: def emit(self, record): color = log.LOG_LEVEL_COLOR[record.levelname] msg = self.format(record) msg = '<font color={}> {} </font><br />'.format(color, msg) self.sender.signal.emit(self.widget, msg)"}
{"text_id": "9415", "text": "docstring: def detect_local_minima(grid_obj): import scipy.ndimage.filters as filters import scipy.ndimage.morphology as morphology import numpy as np arr = grid_obj.pot_repeat neighborhood = morphology.generate_binary_structure(len(arr.shape), 2) local_min = (filters.minimum_filter(arr, footprint=neighborhood) == arr) we create the mask of the background background = (arr == 0) a little technicality: we must erode the background in order to successfully subtract it from local_min, otherwise a line will appear along the background border (artifact of the local minimum filter) http://www.scipy.org/doc/api_docs/SciPy.ndimage.morphology.htmlbinary_erosion eroded_background = morphology.binary_erosion( background, structure=neighborhood, border_value=1) we obtain the final mask, containing only peaks, by removing the background from the local_min mask detected_minima = local_min - eroded_background return np.where(detected_minima)"}
{"text_id": "9416", "text": "docstring: def preexec(): os.setgid(gid) try: os.setgroups(gids) except Exception as e: print('Failed to set groups %s' % e, file=sys.stderr) os.setuid(uid) if chdir: _try_setcwd(home)"}
{"text_id": "9417", "text": "docstring: def _get_corners(input_image): rows, cols = input_image.shape[0:2] return np.float32([[0, 0], [cols - 1, 0], [0, rows - 1], [cols - 1, rows - 1]])"}
{"text_id": "9418", "text": "docstring: def provision_tokenizer(): return word_tokenize"}
{"text_id": "9419", "text": "docstring: def ParsePatchSet(patchset): patches = [] ps_key = patchset.key splitted = engine_utils.SplitPatch(patchset.data) if not splitted: return [] first_id, last_id = models.Patch.allocate_ids(len(splitted), parent=ps_key) ids = range(first_id, last_id + 1) for filename, text in splitted: key = ndb.Key(models.Patch, ids.pop(0), parent=ps_key) patches.append(models.Patch( patchset_key=patchset.key, text=utils.to_dbtext(text), filename=filename, key=key)) return patches"}
{"text_id": "9420", "text": "docstring: def sanitize_filename(name, max_length=0): cleaned_filename = unicodedata.normalize('NFKD', name) if name else '' stripped_filename = ''.join(c for c in cleaned_filename if c in VALID_FILENAME_CHARS).replace(' ', '_') return stripped_filename[:max_length] if max_length > 0 else stripped_filename"}
{"text_id": "9421", "text": "docstring: def upload_relaminarisation_time(ti_obj: ti.TimeIntegration, data_id, max_ke_eps=0.2) -> RelamTimeData: return {'t_relam': get_relaminarisation_time(ti_obj.T, ti_obj.max_ke, max_ke_eps)}"}
{"text_id": "9422", "text": "docstring: def encode(self, input: Tensor) -> List[Tensor]: result = self.encoder(input) result = torch.flatten(result, start_dim=1) mu = self.fc_mu(result) mu = mu.view(mu.size()[0], 4, self.latent_dim) log_var = self.fc_var(result) log_var = log_var.view(log_var.size()[0], 4, self.latent_dim) return [mu, log_var]"}
{"text_id": "9423", "text": "docstring: def ball_reset(): global ball_x, ball_y, ball_move, BALL_SPD_x, BALL_SPD_y BALL_SPD_x = 0 BALL_SPD_y = 0 ball_x, ball_y = cen_x - 3, cen_y - 3 ball_move = False"}
{"text_id": "9424", "text": "docstring: def _grad_ell_over_covars(self, component_index, conditional_ll, kernel_products, sample_vars, normal_samples): raise NotImplementedError()"}
{"text_id": "9425", "text": "docstring: def ingest_files(self): if self.new_sms is not None: for sms in self.new_sms.smsfile: smsfile = SMSFile(sms) smsfile.insert_to_db()"}
{"text_id": "9426", "text": "docstring: def new_ex(self, c, name, independents, dependents): session = self.getSession(c) dataset = session.newDataset(name, independents, dependents, extended=True) c['dataset'] = dataset.name c['datasetObj'] = dataset c['filepos'] = 0 c['commentpos'] = 0 c['writing'] = True return c['path'], c['dataset']"}
{"text_id": "9427", "text": "docstring: def create_or_update_trigger_type_db(trigger_type): assert isinstance(trigger_type, dict) trigger_type_api = TriggerTypeAPI(**trigger_type) trigger_type_api.validate() trigger_type_api = TriggerTypeAPI.to_model(trigger_type_api) ref = ResourceReference.to_string_reference(name=trigger_type_api.name, pack=trigger_type_api.pack) existing_trigger_type_db = get_trigger_type_db(ref) if existing_trigger_type_db: is_update = True else: is_update = False if is_update: trigger_type_api.id = existing_trigger_type_db.id try: trigger_type_db = TriggerType.add_or_update(trigger_type_api) except StackStormDBObjectConflictError: trigger_type_db = get_trigger_type_db(ref) is_update = True extra = {'trigger_type_db': trigger_type_db} if is_update: LOG.audit('TriggerType updated. TriggerType.id=%s' % (trigger_type_db.id), extra=extra) else: LOG.audit('TriggerType created. TriggerType.id=%s' % (trigger_type_db.id), extra=extra) return trigger_type_db"}
{"text_id": "9428", "text": "docstring: def plot_mainfold(self, method): plot_mainfold(method, self.data, self.target_name)"}
{"text_id": "9429", "text": "docstring: def config(): dir = os.path.dirname(os.path.abspath(__file__)) config_path = os.path.join(dir, \"secrets\", \"config.json.secret\") if os.path.isfile(config_path): with open(config_path, \"rU\") as f: config = json.load(f) return config else: raise FileNotFoundError( \"No session login credentials found at {}. Please add file\" \" to complete live tests.\".format(config_path) )"}
{"text_id": "9430", "text": "docstring: def caption(path: str, uid: str, captions: List[str]) -> None: template = meme.load_template(uid) image = meme.caption(template, captions) image.save(filename=path)"}
{"text_id": "9431", "text": "docstring: def _prod(x): if len(x) == 0: return 1 return functools.reduce(operator.mul, x)"}
{"text_id": "9432", "text": "docstring: def delete_subscription(self, subscription_id, enterprise_id, **kwargs): kwargs['_return_http_data_only'] = True if kwargs.get('async_req'): return self.delete_subscription_with_http_info(subscription_id, enterprise_id, **kwargs) else: (data) = self.delete_subscription_with_http_info(subscription_id, enterprise_id, **kwargs) return data"}
{"text_id": "9433", "text": "docstring: def validation_error_to_list_errors(exception): errors = exception.normalized_messages() return list(_iter_errors_dict(errors))"}
{"text_id": "9434", "text": "docstring: def __check_share_community_with_user(self, this_community, this_user, this_privilege): if __debug__: assert isinstance(this_community, Community) assert isinstance(this_user, User) assert this_privilege >= PrivilegeCodes.OWNER and this_privilege <= PrivilegeCodes.VIEW if not self.user.is_active: raise PermissionDenied(\"Requesting user is not active\") if not this_user.is_active: raise PermissionDenied(\"Grantee user is not active\") if not self.owns_community(this_community): raise PermissionDenied(\"User must own community\")"}
{"text_id": "9435", "text": "docstring: def from_flag(cls, flagname, flag_values, other_flag_values=None): first_module = flag_values.FindModuleDefiningFlag( flagname, default='<unknown>') if other_flag_values is None: second_module = _helpers.GetCallingModule() else: second_module = other_flag_values.FindModuleDefiningFlag( flagname, default='<unknown>') flag_summary = flag_values[flagname].help msg = (\"The flag '%s' is defined twice. First from %s, Second from %s. \" \"Description from first occurrence: %s\") % ( flagname, first_module, second_module, flag_summary) return cls(msg)"}
{"text_id": "9436", "text": "docstring: def extract_hash_info(password_hash_389:str) -> bytes: if password_hash_389[0] == \"{\": password_hash_389 = password_hash_389[15:] hash_bytes = base64.b64decode(password_hash_389) iteration_bytes = hash_bytes[0:4] iterations = int(socket.htonl(struct.unpack(\"I\",iteration_bytes)[0])) assert iterations < 1000000 salt = hash_bytes[4:68] target_key = hash_bytes[68:] return iterations, salt, target_key"}
{"text_id": "9437", "text": "docstring: def predict(self, X_test): pred_test = np.array( [self.majority(self.compute_distances_neighbors(sample)) for sample in X_test]) return pred_test"}
{"text_id": "9438", "text": "docstring: def watch_node(self, job): logger.debug(\"received watch_node job for: %s\" % job.data) if \"node\" not in job.data or \"status\" not in job.data or \\ \"pod\" not in job.data: logger.warn(\"invalid or incomplete attributes received\") return self.extend_hello(90.0) dn = \"topology/pod-%s/node-%s/sys\" % (job.data[\"pod\"], job.data[\"node\"]) jobs = [] if job.data[\"status\"] == \"active\": logger.debug(\"%s active, pulling all current endpoints\" % dn) tsc = \"epmMacEp,epmIpEp,epmRsMacEpToIpEpAtt\" self.session = ept_utils.refresh_session(self.fabric) if self.session is None: logger.warn(\"failed to refresh apic session\") return opts = ept_utils.build_query_filters(queryTarget=\"subtree\", targetSubtreeClass=tsc) url = \"/api/mo/%s.json%s\" % (dn,opts) js = ept_utils.get(self.session, url) if js is None: logger.warn(\"get_dn: %s returned None\" % url) return logger.debug(\"received %s objects from node refresh\" % len(js)) for obj in js: classname = obj.keys()[0] key = ept_utils.parse_epm_event(classname,obj,self.overlay_vnid) if key is not None: jkey = { \"type\": key[\"type\"], \"addr\": key[\"addr\"], \"vnid\": key[\"vnid\"] } if classname==\"epmRsMacEpToIpEpAtt\": jkey[\"addr\"]=key[\"ip\"] jobs.append(EPJob(\"ep_analyze\",jkey,ts=key[\"ts\"],data=key)) else: logger.warn(\"failed to parse epm event: %s\" % obj) elif job.data[\"status\"] == \"inactive\": event_ts = time.time() logger.debug(\"%s inactive, removing current endpoints from db\"%dn) db_key = { \"fabric\": self.fabric, \"node\": job.data[\"node\"], \"events.0.status\": { \"$ne\": \"deleted\"} } with self.app.app_context(): db = self.app.mongo.db for h in db.ep_history.find(db_key,{\"events\":{\"$slice\":1}}): delete_jobs = ep_get_delete_jobs(h, self.overlay_vnid, event_ts=event_ts) if delete_jobs is None: logger.warn(\"failed to get delete_jobs for %s\" % h) else: jobs+= delete_jobs else: logger.warn(\"invalid status received: %s\" % job.data[\"status\"]) return self.requeue_job(jobs) return"}
{"text_id": "9439", "text": "docstring: def NETRemotePush(self): try: cur_buf = self.cur_buf except KeyError: Vim.ErrorMsg('Not a netranger buffer') return if not self.cur_buf.is_remote: Vim.ErrorMsg('Not a remote directory') else: Rclone.sync(cur_buf.wd, Rclone.SyncDirection.UP)"}
{"text_id": "9440", "text": "docstring: def cavity_parameter_estimator_2(l, rc1, rc2, lam=1064 * 10 ** (-9)): g1 = 1 - l / rc1 g2 = 1 - l / rc2 z1 = -l * (rc2 + l) / (rc2 + rc1 + 2 * l) z2 = z1 + l if z2 != (z1 + l): raise ValueError( \"Math is wrong, I knew it! \\n l = {}, z1 = {}, z2 = {}\".format(l, z1, z2)) else: print(\"Math is right once again, boring.\") z0 = np.sqrt(- l * (rc1 + l) * (rc2 + l) * (rc1 + rc2 + l) / (rc2 + rc1 + 2 * l)**2) w0 = np.sqrt(lam * z0 / np.pi) return g1, g2, z1, z2, w0"}
{"text_id": "9441", "text": "docstring: def showGraphs(gTimeline, gDPS, tSkill) : pl.plot(gTimeline, gDPS) pl.show() pl.bar(range(len(tSkill[0])), tSkill[2]) pl.xticks([ i + 0.5 for i in range(len(tSkill[0])) ], tSkill[0], rotation=90) pl.show()"}
{"text_id": "9442", "text": "docstring: def db_eval_sequence(technique,sequence,inputdir,metrics=None): db_sequence = DAVISAnnotationLoader(cfg,sequence) db_segmentation = DAVISSegmentationLoader(cfg,sequence, osp.join(inputdir,technique)) if metrics is None or 'J' in metrics: J,j_M,j_O,j_D = db_sequence.eval(db_segmentation,'J') else: J,j_M,j_O,j_D = [np.nan],np.nan,np.nan,np.nan if metrics is None or 'F' in metrics: F,f_M,f_O,f_D = db_sequence.eval(db_segmentation,'F') else: F,f_M,f_O,f_D = [np.nan],np.nan,np.nan,np.nan db_sequences_t_eval = map(lambda s: s.name if s.eval_t else None, db_read_sequences()) if sequence in db_sequences_t_eval and (metrics is None or 'T' in metrics): T,t_M,_,_ = db_sequence.eval(db_segmentation,'T') else: T,t_M = np.ones_like(J[1:])*np.nan, np.nan return J,j_M,j_O,j_D,F,f_M,f_O,f_D,T,t_M"}
{"text_id": "9443", "text": "docstring: def scale(self, x_scale:float = 0, y_scale:float = 0, z_scale:float = 0): for i in range(len(self.verticies)): self.verticies[i][0] *= 1 + x_scale self.verticies[i][1] *= 1 + y_scale self.verticies[i][2] *= 1 + z_scale"}
{"text_id": "9444", "text": "docstring: async def botinfo(self, ctx: Context): bot_uptime = discord_timestamp(self.bot.boot_time) created = discord_timestamp(self.bot.user.created_at) bot_guilds = len(self.bot.guilds) bot_users = len(self.bot.users) dpy_version = f\"[Discord.py v{discord.__version__}](https://github.com/Rapptz/discord.py)\" py_version = f\"[Python v{platform.python_version()}](https://www.python.org/)\" bot_version = f\"[Makoda v{self.bot.version}]({self.bot.invite_link})\" channel_types = Counter(type(c) for c in self.bot.get_all_channels()) channels = channel_types[discord.channel.TextChannel] voice_channel = channel_types[discord.channel.VoiceChannel] ashish = self.bot.get_user(711043296025378856) latency = f\"{round(ctx.bot.latency*1000, 2)} ms\" pid = os.getpid() py = psutil.Process(pid) memory_use = py.memory_info() cpu = f\"{psutil.cpu_percent()}%\" total_ram_usage = f\"{round(psutil.virtual_memory().used/1024/1024)}MB\" py_ram_usage = f\"{round(memory_use[0] / 1024 / 1024)}MB\" recent_updates = get_last_commits() embed = discord.Embed(colour=self.bot.colour) embed.set_author( name=f\"About {self.bot.user}\", icon_url=self.bot.user.avatar.url) embed.description = f\"{ctx.guild.me.mention} is simple discord bot that helps you getting started within discord.py with asyncpg & tortoise-orm.\" embed.add_field( name=\"**__Makoda Info__**\", value=f\"**Created:** {created}\\n**Online Since:** {bot_uptime}\\n**Latency:** {latency}\", inline=False) embed.add_field( name=\"**__System Info__**\", value=f\"**CPU Usage:** {cpu}\\n**RAM Usage:** `{total_ram_usage}` (`{py_ram_usage}` is unique to this process)\", inline=False) embed.add_field( name=\"**__Total__**\", value=f\"**Bot Server(s):** {bot_guilds}\\n**Bot User(s):** {bot_users}\\n**Bot Channel(s):** {emote.channel} {channels} | {emote.voice} {voice_channel}\\n**Code Line(s)**: {linecount()}\", inline=False) embed.add_field( name=\"**__Version Info__**\", value=f\"{emote.python} {py_version}\\n{emote.discord} {dpy_version}\\n{emote.bot} {bot_version}\", inline=False) embed.add_field(name=\"**__Latest Changes__**\", value=recent_updates, inline=False) buttons = [ discord.ui.Button(style=discord.ButtonStyle.link, label=\"Invite Makoda\", url=self.bot.invite_link), discord.ui.Button(style=discord.ButtonStyle.link, label=\"Source Code\", url=self.bot.source_code), discord.ui.Button(style=discord.ButtonStyle.link, label=\"Support Server\", url=self.bot.support_server), discord.ui.Button(style=discord.ButtonStyle.link, label=\"Makoda Developer\", url=f\"https://discord.com/users/{ashish.id}\") ] view = CustomButtonView(ctx, buttons, disable_button=False) return await ctx.reply(embed=embed, view=view)"}
{"text_id": "9445", "text": "docstring: def apply_nn(self, model, calculations, f_, readout=False): input_nn = self.compute_all_input(calculations, f_) input_size = model.input_shape[-1] input_nn = tf.ensure_shape(input_nn, [None, input_size]) return model(input_nn)"}
{"text_id": "9446", "text": "docstring: def repair(cls, file): gxapi_cy.WrapDB._repair(GXContext._get_tls_geo(), file.encode())"}
{"text_id": "9447", "text": "docstring: def symbolUploads_create_with_http_info(self, owner_name, app_name, body, **kwargs): \"\"\"symbolUploads_create Begins the symbol upload process for a new set of symbols for the specified application This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async=True >>> thread = api.symbolUploads_create_with_http_info(owner_name, app_name, body, async=True) >>> result = thread.get() :param async bool :param string owner_name: The name of the owner (required) :param string app_name: The name of the application (required) :param object body: The symbol information (required) :return: Failure If the method is called asynchronously, returns the request thread. \"\"\" all_params = ['owner_name', 'app_name', 'body'] all_params.append('async') all_params.append('_return_http_data_only') all_params.append('_preload_content') all_params.append('_request_timeout') params = locals() for key, val in six.iteritems(params['kwargs']): if key not in all_params: raise TypeError( \"Got an unexpected keyword argument '%s'\" \" to method symbolUploads_create\" % key ) params[key] = val del params['kwargs'] if ('owner_name' not in params or params['owner_name'] is None): raise ValueError(\"Missing the required parameter `owner_name` when calling `symbolUploads_create`\") if ('app_name' not in params or params['app_name'] is None): raise ValueError(\"Missing the required parameter `app_name` when calling `symbolUploads_create`\") if ('body' not in params or params['body'] is None): raise ValueError(\"Missing the required parameter `body` when calling `symbolUploads_create`\") collection_formats = {} path_params = {} if 'owner_name' in params: path_params['owner_name'] = params['owner_name'] if 'app_name' in params: path_params['app_name'] = params['app_name'] query_params = [] header_params = {} form_params = [] local_var_files = {} body_params = None if 'body' in params: body_params = params['body'] header_params['Accept'] = self.api_client.select_header_accept( ['application/json', 'application/json', 'application/json', 'application/json']) header_params['Content-Type'] = self.api_client.select_header_content_type( ['application/json']) auth_settings = ['APIToken'] return self.api_client.call_api( '/v0.1/apps/{owner_name}/{app_name}/symbol_uploads', 'POST', path_params, query_params, header_params, body=body_params, post_params=form_params, files=local_var_files, response_type='Failure', auth_settings=auth_settings, async=params.get('async'), _return_http_data_only=params.get('_return_http_data_only'), _preload_content=params.get('_preload_content', True), _request_timeout=params.get('_request_timeout'), collection_formats=collection_formats)"}
{"text_id": "9448", "text": "docstring: def create_env(): searchpath = list(settings.JINJA2_TEMPLATE_DIRS) return Environment(loader=FileSystemLoader(searchpath), auto_reload=settings.TEMPLATE_DEBUG, cache_size=getattr(settings, 'JINJA2_CACHE_SIZE', 50), extensions=getattr(settings, 'JINJA2_EXTENSIONS', ()))"}
{"text_id": "9449", "text": "docstring: def include_collector( self, collector: \"Collector\", *, dependencies: Optional[Sequence[Depends]] = None, ) -> None: if self.default_message_handler and collector.default_message_handler: raise AssertionError(\"only one default handler can be applied\") if collector.default_message_handler: self._add_default_handler(collector.default_message_handler, dependencies) self._add_handlers(collector.handlers, dependencies)"}
{"text_id": "9450", "text": "docstring: def db_val(self): return str(int(self.float_timestamp * PRECISION))"}
{"text_id": "9451", "text": "docstring: def duplicate_move(OBJECT_OT_duplicate=None, TRANSFORM_OT_translate=None): pass"}
{"text_id": "9452", "text": "docstring: def convert_rst_to_basic_text(contents): converted = publish_string( contents, writer=BasicTextWriter(), settings_overrides={'report_level': 5}) return converted.decode('utf-8')"}
{"text_id": "9453", "text": "docstring: def self_connect(self, host_name, client_name): from labrad.wrappers import connectAsync self.cxn = yield connectAsync(host=host_name, name=client_name, password=\"lab\") try: self.server = self.cxn.rga_server print('Connected to RGA Server.') yield self.server.signal__filament_changed(self.FILSIGNALID) yield self.server.signal__mass_lock_changed(self.MLSIGNALID) yield self.server.signal__high_voltage_changed(self.HVSIGNALID) yield self.server.signal__buffer_read(self.BUFSIGNALID) yield self.server.signal__query_sent(self.QUESIGNALID) yield self.server.addListener(listener = self.update_fil, source = None, ID = self.FILSIGNALID) yield self.server.addListener(listener = self.update_ml, source = None, ID = self.MLSIGNALID) yield self.server.addListener(listener = self.update_hv, source = None, ID = self.HVSIGNALID) yield self.server.addListener(listener = self.update_buf, source = None, ID = self.BUFSIGNALID) yield self.server.addListener(listener = self.update_que, source = None, ID = self.QUESIGNALID) self.signal_connect() except Exception as e: print('RGA Server Unavailable. Client is not connected.')"}
{"text_id": "9454", "text": "docstring: def restock_order_lines(order): country = get_order_country(order) default_warehouse = Warehouse.objects.filter( shipping_zones__countries__contains=country ).first() for line in order: if line.variant and line.variant.track_inventory: if line.quantity_unfulfilled > 0: deallocate_stock(line, line.quantity_unfulfilled) if line.quantity_fulfilled > 0: allocation = line.allocations.first() warehouse = ( allocation.stock.warehouse if allocation else default_warehouse ) increase_stock(line, warehouse, line.quantity_fulfilled) if line.quantity_fulfilled > 0: line.quantity_fulfilled = 0 line.save(update_fields=[\"quantity_fulfilled\"])"}
{"text_id": "9455", "text": "docstring: def resume(self): if self._state == self._STOPPED: self._state = self._STARTED return self else: raise RuntimeError(\"Can not resume a stopwatch that has not been\" \" stopped\")"}
{"text_id": "9456", "text": "docstring: def augment_single(center, steering_angle, range_x=100, range_y=10): image, steering_angle = random_flip(center, steering_angle) image, steering_angle = random_translate(image, steering_angle, range_x, range_y) image = random_shadow(image) image = random_brightness(image) return image, steering_angle"}
{"text_id": "9457", "text": "docstring: def computeOneNode(self, node): if self.intermediates_[node] is None: context = self.grabContext() with self.contextEnterer(context): context.resetInterruptState() if isinstance(node, tuple): with freestoreLock: context.placeInEvaluationStateWithoutRenamingMutableVectors( ImplValContainer_(tuple(node)) ) context.compute() elif isinstance(node, SplitSubcomputation): context.resumePausedComputation(node.pausedComputationTree) context.resetInterruptState() context.compute() else: assert False, \"don't know what to do with node of type %s\" % node self.intermediates_[node] = context elif isinstance(self.intermediates_[node], FORANative.ExecutionContext): context = self.intermediates_[node] req = context.getCacheRequest() if CacheSemantics.isCacheRequestWithResult(req): result = CacheSemantics.getCacheRequestComputationResult(req) with self.contextEnterer(context): context.resetInterruptState() context.addCachecallResult(result) context.compute() else: cacheCalls = [x.extractApplyTuple() for x in CacheSemantics.processCacheCall(req)] res = [] exception = None for t in cacheCalls: assert t in self.finishedValuesAndTimeElapsed_, ( \"Couldn't find result for: %s in %s\" % (t,\"\\n\".join([str(x) for x in self.finishedValuesAndTimeElapsed_.keys()])) ) if self.finishedValuesAndTimeElapsed_[t][0].isException(): if exception is None: exception = self.finishedValuesAndTimeElapsed_[t][0] else: res.append(self.finishedValuesAndTimeElapsed_[t][0].asResult.result) with self.contextEnterer(context): if exception: context.resetInterruptState() context.addCachecallResult(exception) context.compute() else: context.resetInterruptState() context.addCachecallResult( ComputationResult_.Result( ImplValContainer_(tuple(res)) ) ) context.compute() else: splitResult = self.intermediates_[node] for ix in range(len(splitResult.splits)): child = splitResult.childComputations[ix] assert child in self.finishedValuesAndTimeElapsed_ value = self.finishedValuesAndTimeElapsed_[child][0] timeElapsed = self.finishedValuesAndTimeElapsed_[child][1] del self.finishedValuesAndTimeElapsed_[child] if value.isFailure(): self.finishNode_(node, value) self.checkContextBackIn(splitResult.context) return else: splitResult.context.absorbSplitResult( splitResult.splits[ix].computationHash, value, timeElapsed ) with self.lock_: context = splitResult.context context.resetInterruptState() self.intermediates_[node] = context with self.contextEnterer(context): context.compute() while True: if context.isFinished(): result = context.getFinishedResult() timeElapsed = context.getTotalTimeElapsed() self.checkContextBackIn(context) self.finishNode_(node, result, timeElapsed) return elif context.isVectorLoad(): for vectorToLoad in context.getVectorLoadAsVDIDs(): toLoad = None loaded = False if self.offlineCache_ is not None: toLoad = self.offlineCache_.loadIfExists(vectorToLoad.page) if toLoad is not None: self.vdm_.loadSerializedVectorPage(vectorToLoad.page, toLoad) loaded = True if not loaded and vectorToLoad.isExternal(): PythonIoTasks.loadExternalDataset( getCurrentS3Interface(), vectorToLoad, self.vdm_, self.inProcessDownloader ) loaded = True assert loaded, \"lost the definition for VDID: %s\" % vectorToLoad with self.contextEnterer(context): context.resetInterruptState() context.compute() elif context.isInterrupted(): toResume = None if self.checkShouldSplit(context): splits = context.splitComputation() if splits is not None: with self.lock_: splitResult = self.computeIntermediatesForSplitResult(node, splits, context) self.intermediates_[node] = splitResult return with self.contextEnterer(context): if toResume is not None: context.resumePausedComputation(toResume) context.resetInterruptState() context.compute() elif context.isCacheRequest(): req = context.getCacheRequest() deps = set() if CacheSemantics.isCacheRequestWithResult(req): pass else: cacheCalls = [x.extractApplyTuple() for x in CacheSemantics.processCacheCall(req)] with self.lock_: for t in cacheCalls: if t not in self.finishedValuesAndTimeElapsed_ and t not in self.intermediates_: self.intermediates_[t] = None self.completable_.put(t) self.watchers_[t] = threading.Event() if t not in self.finishedValuesAndTimeElapsed_: deps.add(t) self.dependencies_[node] = deps if not deps: with self.lock_: self.completable_.put(node) return"}
{"text_id": "9458", "text": "docstring: def create_links(self): r = requests.get(self.base+self.Category) page = r.text soup=bs(page,'html.parser') vids = soup.findAll('a',attrs={'class':'yt-uix-tile-link'}) for v in vids: tmp = 'https://www.youtube.com' + v['href'] self.links.append(tmp)"}
{"text_id": "9459", "text": "docstring: def strip(text): pass"}
{"text_id": "9460", "text": "docstring: def GetInformationContent(model): info = np.ones(model.size()) info[1:-1] = np.array( [(model.y[i + 1] - model.y[i - 1]) / (model.x[i + 1] - model.x[i - 1]) for i in range(1, model.size() - 1)]) return info ** 2"}
{"text_id": "9461", "text": "docstring: def _update_data(self, resp): for monitor in resp['monitors']: name = monitor['friendly_name'] if name == self.monitor_name: self.uptime_ratio = monitor['all_time_uptime_ratio'] self.response_time = monitor['average_response_time'] break return { 'response_time': self.response_time, 'uptime_ratio': self.uptime_ratio }"}
{"text_id": "9462", "text": "docstring: def log_status(status: str): _check_experiment() _current_experiment.log_status(status)"}
{"text_id": "9463", "text": "docstring: def execute(self, sql): cursor = dbconn.execSQL(self.conn, sql) if cursor.description: names = [desc[0] for desc in cursor.description] rows = list() if cursor.rowcount > 0: for res in cursor.fetchall(): row = DbConn.Tuple() for i, name in enumerate(names): setattr(row, name, res[i]) rows.append(row) if self.autocommit: self.commit() cursor.close() return rows"}
{"text_id": "9464", "text": "docstring: def quarantine_renamer(device_path, corrupted_file_path): from_dir = dirname(corrupted_file_path) to_dir = join(device_path, 'quarantined', get_data_dir(extract_policy_index(corrupted_file_path)), basename(from_dir)) invalidate_hash(dirname(from_dir)) try: renamer(from_dir, to_dir) except OSError as e: if e.errno not in (errno.EEXIST, errno.ENOTEMPTY): raise to_dir = \"%s-%s\" % (to_dir, uuid.uuid4().hex) renamer(from_dir, to_dir) return to_dir"}
{"text_id": "9465", "text": "docstring: def from_zip(cls, zfile: Union[str, BinaryIO], use_filenames: bool = False): with TemporaryZipDirectory(zfile) as tmpz: obj = cls(tmpz, use_filenames=use_filenames) return obj"}
{"text_id": "9466", "text": "docstring: def recall(y_real, y_predicted, c=1): tp = np.sum(np.logical_and(y_predicted == c, y_real == c)) fn = np.sum(np.logical_and(y_predicted != c, y_real == c)) return tp / (tp + fn)"}
{"text_id": "9467", "text": "docstring: def next_atoms(self): smiles_to_expand = [] if p.config['expansion'] == \"all\": for v in p.tokens[2:]: if v == \")\" and (self.element.count(\"(\") <= self.element.count(')')): pass elif v in [\"1\", \"2\", \"3\", \"4\", \"=\", \"#\", \"(\"] and not self.element: pass elif v == \"2\" and (self.element.count(\"1\") == 0): pass elif v == \"3\" and ((self.element.count(\"1\") == 0) and (self.element.count(\"2\") == 0)): pass elif v == \"4\" and ((self.element.count(\"1\") == 0) and (self.element.count(\"2\") == 0) and (self.element.count(\"3\") == 0)): pass elif v in [\"#\", \"=\"] and (self.element[-1] in [\"#\", \"=\", 'Cl']): pass elif v == \"Cl\" and self.element and (self.element[-1] in [\"#\", \"=\"]): pass else: smiles_to_expand.append(SMILES(self.element + [v])) elif p.config['expansion'] == \"best\": smiles_int = mol_to_int(['&'] + self.element) x = np.reshape(smiles_int, (1, len(smiles_int))) x_pad = sequence.pad_sequences(x, maxlen=81, dtype='int32', padding='post', truncating='pre', value=0.) predictions = p.models[0].predict(x_pad) preds = np.asarray(predictions[0][len(smiles_int) - 1]).astype('float64') preds = preds / np.sum(preds) next_probas = np.random.multinomial(1, preds[1:], 30) next_int = np.argmax(next_probas, axis=1) + 1 to_expand = list(set(next_int)) for s in to_expand: smiles_to_expand.append(SMILES(int_to_smile(smiles_int[1:]) + [p.tokens[s]])) elif p.config['expansion'] == \"proba\": smiles_int = mol_to_int(['&'] + self.element) x = np.reshape(smiles_int, (1, len(smiles_int))) x_pad = sequence.pad_sequences(x, maxlen=81, dtype='int32', padding='post', truncating='pre', value=0.) predictions = p.models[0].predict(x_pad) preds = np.asarray(predictions[0][len(smiles_int) - 1]).astype('float64') smiles_to_expand = [] for i, pred in enumerate(preds): if pred > p.config['proba_min'] and p.tokens[i] != \"&\" and p.tokens[i] != \"\\n\": smiles_to_expand.append(SMILES(int_to_smile(smiles_int[1:]) + [p.tokens[i]])) if not smiles_to_expand: for i, pred in enumerate(preds): if pred > (p.config['proba_min'] / 100) and p.tokens[i] != \"&\" and p.tokens[i] != \"\\n\": smiles_to_expand.append(SMILES(int_to_smile(smiles_int[1:]) + [p.tokens[i]])) return smiles_to_expand"}
{"text_id": "9468", "text": "docstring: def train_transforms(example_batch): example_batch[\"pixel_values\"] = [ _train_transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[\"image\"] ] return example_batch"}
{"text_id": "9469", "text": "docstring: def has_alpha(self, text: str): return any(char.isalpha() for char in text)"}
{"text_id": "9470", "text": "docstring: def updateCurrentPage(self): self.currentPage = list(self.pages.keys())[self.currentPageNum] self.options = list(self.pages.values())[self.currentPageNum] if len(self.pages) > 1: if self.currentPageNum == len(self.pages) - 1: self.currentPageControls = self.lastPageControls elif self.currentPageNum == 0: self.currentPageControls = self.firstPageControls else: self.currentPageControls = self.midPageControls for optionEmoji in self.currentPageControls: self.options[optionEmoji] = self.currentPageControls[optionEmoji]"}
{"text_id": "9471", "text": "docstring: def initializeGraph(self, nvert, nedge, i, j, v=1): iInd = ParVec(nedge) jInd = ParVec(nedge) if type(v) == int or type(v) == float: vInd = ParVec(nedge, v) else: vInd = ParVec(nedge) for ind in range(nedge): iInd[ind] = i[ind] jInd[ind] = j[ind] if type(v) != int and type(v) != float: vInd[ind] = v[ind] return HyGraph(iInd, jInd, vInd, nvert)"}
{"text_id": "9472", "text": "docstring: def create_policy_stat_table(self, conn): conn.executescript(\"\"\" CREATE TABLE policy_stat ( storage_policy_index INTEGER PRIMARY KEY, object_count INTEGER DEFAULT 0, bytes_used INTEGER DEFAULT 0 ); INSERT OR IGNORE INTO policy_stat ( storage_policy_index, object_count, bytes_used ) SELECT 0, object_count, bytes_used FROM account_stat WHERE container_count > 0; \"\"\")"}
{"text_id": "9473", "text": "docstring: def _maybe_cast_indexer(self, key): if is_float(key) and not self.is_floating(): try: ckey = int(key) if ckey == key: key = ckey except (ValueError, TypeError): pass return key"}
{"text_id": "9474", "text": "docstring: def _handleLeaveNode(self, node): if Frame.nodeCreatesFrame(node): self._logger.debug(\"[-Frame]\") self._currentFrame = self._currentFrame.parent"}
{"text_id": "9475", "text": "docstring: def launch_probability(cls, db, chan): gxapi_cy.WrapCHIMERA._launch_probability(GXContext._get_tls_geo(), db.encode(), chan.encode())"}
{"text_id": "9476", "text": "docstring: def config(self, **kw): group = kw.pop('group', None) for k, v in kw.items(): if group: self.conf.set_override(k, v, group) else: self.conf.set_override(k, v)"}
{"text_id": "9477", "text": "docstring: def feature_stat(self) -> pd.DataFrame: for feature_type in self._feature_type: if hasattr(feature_type, \"feature_stat\"): stat = feature_type.feature_stat(self._obj).reset_index() stat.columns = [\"Metric\", \"Value\"] return stat return None"}
{"text_id": "9478", "text": "docstring: def preprocess_GIF(self, image, **kwargs): if 'transparency' in image.info: save_kwargs = {'transparency': image.info['transparency']} else: save_kwargs = {} return (image, save_kwargs)"}
{"text_id": "9479", "text": "docstring: def doc_field_lengths(self, docnum): raise NotImplementedError"}
{"text_id": "9480", "text": "docstring: def read_message(self, timeout): with self._reader_lock: raw_header = self._transport.read( struct.calcsize(AdbMessage.HEADER_STRUCT_FORMAT), timeout.remaining_ms) if not raw_header: raise usb_exceptions.AdbProtocolError('Adb connection lost') try: raw_message = RawAdbMessage( *struct.unpack(AdbMessage.HEADER_STRUCT_FORMAT, raw_header)) except struct.error as exception: raise usb_exceptions.AdbProtocolError( 'Unable to unpack ADB command (%s): %s (%s)' % (AdbMessage.HEADER_STRUCT_FORMAT, raw_header, exception)) if raw_message.data_length > 0: if timeout.has_expired(): _LOG.warning('Timed out between AdbMessage header and data, reading ' 'data anyway with 10ms timeout') timeout = timeouts.PolledTimeout.from_millis(10) data = self._transport.read(raw_message.data_length, timeout.remaining_ms) else: data = '' return raw_message.to_adb_message(data)"}
{"text_id": "9481", "text": "docstring: def cc(self, current, activate=True): self.set_mode(\"CC\") self.set_cc_current(current) self.enable()"}
{"text_id": "9482", "text": "docstring: def encode_dh_domain_parameters(obj): asn1 = DHParameters({ 'g': int.from_bytes(obj[Attribute.BASE], byteorder='big'), 'p': int.from_bytes(obj[Attribute.PRIME], byteorder='big'), }) return asn1.dump()"}
{"text_id": "9483", "text": "docstring: def compute_minimum_intersection(kp1, kp2, filter_arr1, filter_arr2): return min( compute_intersection(kp1, filter_arr1), compute_intersection(kp2, filter_arr2))"}
{"text_id": "9484", "text": "docstring: def _authenticate_user_dn(self, password: str) -> None: if self.dn is None: raise self.AuthenticationFailed(\"failed to map the username to a DN.\") try: sticky = self.settings.BIND_AS_AUTHENTICATING_USER self._bind_as(self.dn, password, sticky=sticky) except LDAPInvalidCredentialsResult: raise self.AuthenticationFailed(\"user DN/password rejected by LDAP server.\")"}
{"text_id": "9485", "text": "docstring: def find_route(self, uri): for (regex, obj) in self.routes: m = regex.match(uri) if m is not None: return (obj, m.groups()) return None"}
{"text_id": "9486", "text": "docstring: async def async_step_unreachable_appliance( self, user_input: dict[str, Any] | None = None ) -> FlowResult: return await self._async_step_appliance( step_id=\"unreachable_appliance\", user_input=user_input, )"}
{"text_id": "9487", "text": "docstring: def chances(x): import scipy as sp x=sp.array(x) nt=sp.size(x) ns=nt+1 s=sp.zeros(ns) p=binaries(long(2)**nt) p=p[1:,1:] c=p*x+1-p-(1-p)*x ap=sp.sum(p,axis=1) ac=sp.sum(c,axis=1)/sp.sum(c) for i in range(ns): s[i]=sp.sum(ac*sp.where(ap==i,1,0)) return s"}
{"text_id": "9488", "text": "docstring: def children(self): return FileNode.find(Q('parent', 'eq', self._id))"}
{"text_id": "9489", "text": "docstring: def isInt(input): return isinstance(input, int)"}
{"text_id": "9490", "text": "docstring: def api_sensor_in_timeframe(sensor_id, start, end): callbackFunc = request.args.get(\"callback\") if not callbackFunc: callbackFunc = DEFAULT_CALLBACK_FUNC cur = app.sqlite.cursor() query = \"\"\" SELECT device_name, value, value_timestamp FROM {0} WHERE device_uid = '{1}' AND polling_timestamp BETWEEN {2} AND {3}; \"\"\".format(DB_TABLE, sensor_id, start, end) cur.execute(query) rows = cur.fetchall() device_name = \"\" sensor_values = [] for row in rows: device_name = row[0] value = row[1] if \"temperature\" in row[0].lower() or \"temperature\" in row[2].lower(): try: value = float(value)/100 except: pass if \"True\" in str(value): value = 1 if \"False\" in str(value): value = 0 if \"none\" in str(value).lower(): value = 0 if \"light\" in row[0].lower() or \"light\" in row[2].lower(): try: value = float(value)/1000 except: pass if not \"none\" in str(row[2]).lower() and value != \"\": sensor_values.append(\"\\\"{0}\\\":{1}\".format(row[2], value)) return \"\"\"{0}({{ \"sensor_id\":\"{1}\", \"sensor_name\":\"{2}\", \"values\":{{{3}}} }});\"\"\".format(callbackFunc, sensor_id, device_name, ','.join(sensor_values))"}
{"text_id": "9491", "text": "docstring: def rhochange(self): if self.opt['HighMemSolve'] and self.cri.Cd == 1: self.c = sl.solvedbd_sm_c( self.Df, np.conj(self.Df), (self.mu / self.rho) * self.GHGf + 1.0, self.cri.axisM)"}
{"text_id": "9492", "text": "docstring: def upconv_block(self,X_tensor,filters,filter_size,skip_connection): e1 = upconv_(X_tensor,filters,filter_size=filter_size,strides =2,padding=\"SAME\") concat = tf.concat([e1,skip_connection],axis=-1) conv1 = downconv_(concat,filters,filter_size=3,strides=1,padding='SAME') relu1 = tf.nn.relu(conv1) conv2 = downconv_(relu1,filters,filter_size=3,strides=1,padding='SAME') relu2 = tf.nn.relu(conv2) return relu2"}
{"text_id": "9493", "text": "docstring: def plot(self, *args, **kwargs): plt.scatter(self.velocity['colloid'], self.velocity['velocity'], *args, **kwargs)"}
{"text_id": "9494", "text": "docstring: def to_utf8(obj): if isinstance(obj, six.text_type): return obj.encode('utf8') elif PY2: return str(obj) else: bytes(obj)"}
{"text_id": "9495", "text": "docstring: def start_master(host): client = mqtt.Client() client.connect(host, 1883, keepalive=60) return client"}
{"text_id": "9496", "text": "docstring: def evaluate_upper( distribution, parameters=None, cache=None, ): cache = cache if cache is not None else {} parameters = load_parameters( distribution, \"_upper\", parameters=parameters) dtype = int if distribution.interpret_as_integer else float upper = distribution._upper(**parameters) upper = numpy.asarray(upper, dtype=dtype)+numpy.zeros(len(distribution), dtype=dtype) cache[distribution] = upper return upper"}
{"text_id": "9497", "text": "docstring: def GetDates(self): start = self.start.GetValue() start = to_datetime(start) end = self.end.GetValue() end = to_datetime(end) return start, end"}
{"text_id": "9498", "text": "docstring: def read_sents_from_file(text_fn): return iter([sent.strip().replace(\" \", \"\") for sent in open(text_fn, encoding = \"utf8\") if sent.strip()])"}
{"text_id": "9499", "text": "docstring: def _read_coefficients(self): coeff = self._read_register(0x88, 24) coeff = list(struct.unpack(\"<HhhHhhhhhhhh\", bytes(coeff))) coeff = [float(i) for i in coeff] self._temp_calib = coeff[:3] self._pressure_calib = coeff[3:] self._humidity_calib = [0] * 6 self._humidity_calib[0] = self._read_byte(0xA1) coeff = self._read_register(0xE1, 7) coeff = list(struct.unpack(\"<hBbBbb\", bytes(coeff))) self._humidity_calib[1] = float(coeff[0]) self._humidity_calib[2] = float(coeff[1]) self._humidity_calib[3] = float((coeff[2] << 4) | (coeff[3] & 0xF)) self._humidity_calib[4] = float((coeff[4] << 4) | (coeff[3] >> 4)) self._humidity_calib[5] = float(coeff[5])"}
{"text_id": "9500", "text": "docstring: async def async_setup(hass, config): discovery_prefix = 'homeassistant' @callback async def async_device_message_received(msg) -> bool: topic = msg.topic retain = msg.retain payload = msg.payload if not retain or not payload or not topic: return False topic_trimmed = topic.replace(f\"{discovery_prefix}/\", \"\", 1) match = discovery.TOPIC_MATCHER.match(topic_trimmed) if not match: return False component, node_id, object_id = match.groups() discovery_id = \" \".join((node_id, object_id)) if node_id else object_id if component is None: return False if component in SUPPORTED_COMPONENTS and DEVICE_ID in discovery_id: _LOGGER.info( \"Component %s found at topic %s\", component, topic, ) hass.components.mqtt.async_publish(topic, \"\", 0, True) await mqtt.device_trigger.async_device_removed(hass, DEVICE_ID) elif node_id == DEVICE_ID: _LOGGER.info( \"Found unsupported device component: %s\", component, ) return True @callback async def remove_device_service(call): global discovery_topic global DEVICE_ID global NO_ENTITIES discovery_topic = config[DOMAIN].get(\"discovery_prefix\") or DEFAULT_PREFIX discovery_topic += '/#' DEVICE_ID = call.data.get(\"device_id\") if not DEVICE_ID.strip(): _LOGGER.warning( \"A blank device_id is not permitted!\" ) return await hass.components.mqtt.async_subscribe(discovery_topic, async_device_message_received) hass.services.async_register(DOMAIN, \"apply\", remove_device_service) return True"}
{"text_id": "9501", "text": "docstring: def first_child(self): if not self.expanded: return None else: if self._node.has_children(): firstnode = self._node.get_first_child() return firstnode.get_widget() else: return None"}
{"text_id": "9502", "text": "docstring: def adjust_extremes_linear(self, feature, track, extend_feature=1): feature[0] = self.track_init[0] - extend_feature feature[-1] = self.track_init[-1] + extend_feature extend_track = self.feature2track_lin(feature[[0, -1]], feature, track) track[0] = extend_track[0] track[-1] = extend_track[-1] return feature, track"}
{"text_id": "9503", "text": "docstring: def _new_client(self, connect=True): client = TestStompClient(self.server_address) self.clients.append(client) if connect: client.connect() res = client.received_frames.get(timeout=1) self.assertEqual(res.cmd, frames.CONNECTED) return client"}
{"text_id": "9504", "text": "docstring: def create_resnet_visual_observation_encoder( image_input: tf.Tensor, h_size: int, activation: ActivationFunction, num_layers: int, scope: str, reuse: bool, ) -> tf.Tensor: n_channels = [16, 32, 32] n_blocks = 2 with tf.variable_scope(scope): hidden = image_input for i, ch in enumerate(n_channels): hidden = tf.layers.conv2d( hidden, ch, kernel_size=[3, 3], strides=[1, 1], reuse=reuse, name=\"layer%dconv_1\" % i, ) hidden = tf.layers.max_pooling2d( hidden, pool_size=[3, 3], strides=[2, 2], padding=\"same\" ) for j in range(n_blocks): block_input = hidden hidden = tf.nn.relu(hidden) hidden = tf.layers.conv2d( hidden, ch, kernel_size=[3, 3], strides=[1, 1], padding=\"same\", reuse=reuse, name=\"layer%d_%d_conv1\" % (i, j), ) hidden = tf.nn.relu(hidden) hidden = tf.layers.conv2d( hidden, ch, kernel_size=[3, 3], strides=[1, 1], padding=\"same\", reuse=reuse, name=\"layer%d_%d_conv2\" % (i, j), ) hidden = tf.add(block_input, hidden) hidden = tf.nn.relu(hidden) hidden = tf.layers.flatten(hidden) with tf.variable_scope(scope + \"/\" + \"flat_encoding\"): hidden_flat = LearningModel.create_vector_observation_encoder( hidden, h_size, activation, num_layers, scope, reuse ) return hidden_flat"}
{"text_id": "9505", "text": "docstring: def _findHighestImpactIndex(self,returnValue=False): point = None avg = 0 prototype = self.expImpact[self.targets[0]] for pt in sorted(prototype.keys()): new = sum(self.expImpact[t][pt] for t in self.targets)/len(self.targets) if avg < new: avg = new point = pt self.raiseADebug('Highest impact point is',point,'with expected average impact',avg) if returnValue: return point,avg else: return point"}
{"text_id": "9506", "text": "docstring: def find(cls, id: str, **kwargs): (header, kwargs) = cls.configuration().create_header(**kwargs) api_endpoint = cls.configuration().api_endpoint resp = cls.get_request( f\"{api_endpoint}/{cls.resource_name()}/{id}\", headers=header ) return cls.from_json(resp.json())"}
{"text_id": "9507", "text": "docstring: def find_user(account_name): return User.find_by_account_name(account_name)"}
{"text_id": "9508", "text": "docstring: def flight_get( self, EOBT: timelike, callsign: str, origin: str, destination: str ) -> FlightInfo: EOBT = to_datetime(EOBT) data = REQUESTS[\"FlightRetrievalRequest\"].format( send_time=datetime.now(timezone.utc), callsign=callsign, origin=origin, destination=destination, eobt=EOBT, ) rep = self.post(data) return FlightInfo.fromET(rep.reply.find(\"data/flight\"))"}
{"text_id": "9509", "text": "docstring: def login_TestLink(self): SelLib = self._seleniumlib SelLib.location_should_contain('login.php') SelLib.input_text('tl_login', self._username) SelLib.input_text('tl_password', self._password) SelLib.click_button('login_submit')"}
{"text_id": "9510", "text": "docstring: def compute_pairwise_sim(embeddings): mc_list = [0] * len(embeddings) pair_sim_sum = [] for embed1 in embeddings: pair_sim = 0 for embed2 in embeddings: pair_sim += cosine_similarity(embed1.reshape(1, -1), embed2.reshape(1, -1)) pair_sim_sum.append(pair_sim / len(embeddings)) index_max = max(range(len(pair_sim_sum)), key=pair_sim_sum.__getitem__) mc_list[index_max] = 1 return mc_list"}
{"text_id": "9511", "text": "docstring: def distance_matrix_vector(anchor, positive): eps = 1e-8 FeatSimi_Mat = 2 - 2 * torch.mm(anchor, positive.t()) FeatSimi_Mat = FeatSimi_Mat.clamp(min=eps, max=4.0) FeatSimi_Mat = torch.sqrt(FeatSimi_Mat) return FeatSimi_Mat"}
{"text_id": "9512", "text": "docstring: def init(self): from fcntl import ioctl print('Opening %s...' % self.dev_fn) self.jsdev = open(self.dev_fn, 'rb') buf = array.array('B', [0] * 64) ioctl(self.jsdev, 0x80006a13 + (0x10000 * len(buf)), buf) self.js_name = buf.tobytes().decode('utf-8') print('Device name: %s' % self.js_name) buf = array.array('B', [0]) ioctl(self.jsdev, 0x80016a11, buf) self.num_axes = buf[0] buf = array.array('B', [0]) ioctl(self.jsdev, 0x80016a12, buf) self.num_buttons = buf[0] buf = array.array('B', [0] * 0x40) ioctl(self.jsdev, 0x80406a32, buf) for axis in buf[:self.num_axes]: axis_name = self.axis_names.get(axis, 'unknown(0x%02x)' % axis) self.axis_map.append(axis_name) self.axis_states[axis_name] = 0.0 buf = array.array('H', [0] * 200) ioctl(self.jsdev, 0x80406a34, buf) for btn in buf[:self.num_buttons]: btn_name = self.button_names.get(btn, 'unknown(0x%03x)' % btn) self.button_map.append(btn_name) self.button_states[btn_name] = 0 return True"}
{"text_id": "9513", "text": "docstring: def update_flagged_cell(self, index: Coordinate) -> int: if index in self.board.cells_revealed(): return 0 if index not in self.board.cells_flagged(): self.board.add_to_cells_flagged(index) return 1 else: self.board.remove_from_cells_flagged(index) return -1"}
{"text_id": "9514", "text": "docstring: def collide(self, position, height): pad = 0.25 p = list(position) np = normalize(position) for face in FACES: for i in xrange(3): if not face[i]: continue d = (p[i] - np[i]) * face[i] if d < pad: continue for dy in xrange(height): op = list(np) op[1] -= dy op[i] += face[i] if get_world_pos(tuple(op)) not in self.model.world or self.model.world[get_world_pos(tuple(op))] == WATER: continue p[i] -= (d - pad) * face[i] if face == (0, -1, 0) or face == (0, 1, 0): self.dy = 0 elif self.autoJump: if self.dy == 0: self.dy = JUMP_SPEED break return tuple(p)"}
{"text_id": "9515", "text": "docstring: def resources(names, conf = None): global CONFIGURATION if Text.isstring(names): names = [names] if conf is None: if CONFIGURATION is None: CONFIGURATION = load_resources_config() conf = CONFIGURATION bundling = conf[\"bundling\"] minification = conf[\"minification\"] sets = conf[\"sets\"] a = [] for name in names: if not name in sets: raise Exception(\"The set `{}` is not configured inside /configuration/scripts.js\".format(name)) if minification: a.append(\"<script src=\\\"/scripts/{}{}\\\"></script>\".format(name, \".min.js\")) elif bundling: a.append(\"<script src=\\\"/scripts/{}{}\\\"></script>\".format(name, \".built.js\")) else: files = sets[name] for f in files: a.append(\"<script src=\\\"{}\\\"></script>\".format(f)) return \"\\n\".join(a)"}
{"text_id": "9516", "text": "docstring: def check_migration_name(): args = _parse_migration_check_args() if args.migrations_to_ignore is None: args.migrations_to_ignore = 0 migrations_set = set() migrations = os.listdir(args.migration_path) for migration in migrations: if (re.match(r'^[0-9]{4}_auto_[0-9]{2}', migration) and int(migration[:4]) > args.migrations_to_ignore): migrations_set.add(migration) if bool(migrations_set): migrations = list(migrations_set) file_ = 'file' if len(migrations) < 2 else 'files' message = 'Migration %s %s in directory %s must ' \\ 'be renamed to something more descriptive.' \\ % (file_, ', '.join(migrations), args.migration_path) if not args.quiet: print(message) sys.exit(1)"}
{"text_id": "9517", "text": "docstring: def update_from_toml_file(self, file_path_or_obj): _check_toml_module() return self._update_from_file(file_path_or_obj, toml.load)"}
{"text_id": "9518", "text": "docstring: def load_PLC_AutoLable(self, dataset_dir, subset): self.add_class(\"PLC\",1, \"0\") self.add_class(\"PLC\", 2, \"1\") self.add_class(\"PLC\", 3, \"2\") self.add_class(\"PLC\", 4, \"3\") self.add_class(\"PLC\", 5, \"4\") self.add_class(\"PLC\", 6, \"5\") self.add_class(\"PLC\", 7, \"6\") self.add_class(\"PLC\", 8, \"7\") self.add_class(\"PLC\", 9, \"8\") self.add_class(\"PLC\", 10, \"9\") self.add_class(\"PLC\", 11, \"A\") self.add_class(\"PLC\", 12, \"B\") self.add_class(\"PLC\", 13, \"C\") self.add_class(\"PLC\", 14, \"D\") self.add_class(\"PLC\", 15, \"E\") self.add_class(\"PLC\", 16, \"F\") self.add_class(\"PLC\", 17, \"G\") self.add_class(\"PLC\", 18, \"H\") self.add_class(\"PLC\", 20-1, \"J\") self.add_class(\"PLC\", 21-1, \"K\") self.add_class(\"PLC\", 22-1, \"L\") self.add_class(\"PLC\", 23-1, \"M\") self.add_class(\"PLC\", 24-1, \"N\") self.add_class(\"PLC\", 26-2, \"P\") self.add_class(\"PLC\", 28-3, \"R\") self.add_class(\"PLC\", 29-3, \"S\") self.add_class(\"PLC\", 30-3, \"T\") self.add_class(\"PLC\", 31-3, \"U\") self.add_class(\"PLC\", 32-3, \"V\") self.add_class(\"PLC\", 33-3, \"W\") self.add_class(\"PLC\", 34-3, \"X\") self.add_class(\"PLC\", 35-3, \"Y\") self.add_class(\"PLC\", 36-3, \"Z\") dataset_dir = os.path.join(dataset_dir, subset) print(dataset_dir) inputjson_f = open(os.path.join(dataset_dir, 'label.txt'), 'r') line = inputjson_f.readline() while line != \"\": tag=1 lines_items = line.split(' ') img_name = lines_items[0] if not os.path.exists(dataset_dir+'/'+img_name): print(\"file doesn't exist:\",subset,img_name) line = inputjson_f.readline() continue obj_num = lines_items[1] obj_start_index = 2 obj_pts = 0 polygons = [] names = [] for i in range(int(obj_num)): polygon = {} polygon['name'] = 'polygon' obj_name = lines_items[obj_start_index] print(obj_name) names.append(obj_name) obj_pts = int(lines_items[obj_start_index + 1]) polygon['all_points_x'] = [int(lines_items[obj_start_index + 1 + j]) for j in range(1, obj_pts + 1)] polygon['all_points_y'] = [int(lines_items[obj_start_index + 1 + j]) for j in range(obj_pts + 1, obj_pts * 2 + 1)] for j in range(obj_pts + 1, obj_pts * 2 + 1): if(int(lines_items[obj_start_index + 1 + j])>=720): tag=0 break polygons.append(polygon) obj_start_index += obj_pts * 2 + 2 line = inputjson_f.readline() if subset == \"json_val_screen\": line = inputjson_f.readline() name_dict = { \"0\": 1, \"1\": 2, \"2\": 3, \"3\": 4, \"4\": 5, \"5\": 6, \"6\": 7, \"7\": 8, \"8\": 9, \"9\": 10, \"A\": 11, \"B\": 12, \"C\": 13, \"D\": 14, \"E\": 15, \"F\": 16, \"G\": 17, \"H\": 18, \"J\": 20-1, \"K\": 21-1, \"L\": 22-1, \"M\": 23-1, \"N\": 24-1, \"P\": 26-2, \"R\": 28-3, \"S\": 29-3, \"T\": 30-3, \"U\": 31-3, \"V\": 32-3, \"W\": 33-3, \"X\": 34-3, \"Y\": 35-3, \"Z\": 36-3, } name_id = [name_dict[n] for n in names] for index in range(len(polygons)): polygons[index]['name'] = a['regions'][index]['region_attributes']['name'] a['regions'][0]['region_attributes']['name'] = a['regions'][0]['region_attributes']['name'] load_mask() needs the image size to convert polygons to masks. Unfortunately, VIA doesn't include it in JSON, so we must read the image. This is only managable since the dataset is tiny. image_path = os.path.join(dataset_dir, img_name) image_path=image_path+'.jpg' if img_name=='screenshot-1534355667.png': s=0 if not os.path.exists(image_path): continue image = skimage.io.imread(image_path) height, width = image.shape[:2] if tag==1: self.add_image( \"PLC\", image_id=img_name, use file name as a unique image id path=image_path, width=width, height=height, class_id = name_id, polygons=polygons ) print('------------------------------------img:', img_name)"}
{"text_id": "9519", "text": "docstring: def _get_single_variable(self, name, shape=None, dtype=dtypes.float32, initializer=None, regularizer=None, partition_info=None, reuse=None, trainable=None, collections=None, caching_device=None, validate_shape=True, use_resource=None, constraint=None, synchronization=VariableSynchronization.AUTO, aggregation=VariableAggregation.NONE): initializing_from_value = False if initializer is not None and not callable(initializer): initializing_from_value = True if shape is not None and initializing_from_value: raise ValueError(\"If initializer is a constant, do not specify shape.\") dtype = dtypes.as_dtype(dtype) shape = tensor_shape.as_shape(shape) if name in self._vars: if reuse is False: var = self._vars[name] err_msg = (\"Variable %s already exists, disallowed.\" \" Did you mean to set reuse=True or \" \"reuse=tf.AUTO_REUSE in VarScope?\" % name) if isinstance(var, resource_variable_ops.ResourceVariable): raise ValueError(err_msg) tb = var.op.traceback[::-1] tb = [x for x in tb if \"tensorflow/python\" not in x[0]][:5] raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(traceback.format_list(tb)))) found_var = self._vars[name] if not shape.is_compatible_with(found_var.get_shape()): raise ValueError(\"Trying to share variable %s, but specified shape %s\" \" and found shape %s.\" % (name, shape, found_var.get_shape())) if not dtype.is_compatible_with(found_var.dtype): dtype_str = dtype.name found_type_str = found_var.dtype.name raise ValueError(\"Trying to share variable %s, but specified dtype %s\" \" and found dtype %s.\" % (name, dtype_str, found_type_str)) return found_var if reuse is True: raise ValueError(\"Variable %s does not exist, or was not created with \" \"tf.get_variable(). Did you mean to set \" \"reuse=tf.AUTO_REUSE in VarScope?\" % name) if initializer is None: initializer, initializing_from_value = self._get_default_initializer( name=name, shape=shape, dtype=dtype) with ops.init_scope(): if initializing_from_value: init_val = initializer variable_dtype = None else: if tf_inspect.isclass(initializer): initializer = initializer() if shape.is_fully_defined(): if \"partition_info\" in tf_inspect.getargspec(initializer).args: init_val = functools.partial(initializer, shape.as_list(), dtype=dtype, partition_info=partition_info) else: init_val = functools.partial(initializer, shape.as_list(), dtype=dtype) variable_dtype = dtype.base_dtype elif _needs_no_arguments(initializer): init_val = initializer variable_dtype = None else: raise ValueError(\"The initializer passed is not valid. It should \" \"be a callable with no arguments and the \" \"shape should not be provided or an instance of \" \"`tf.keras.initializers.*' and `shape` should be \" \"fully defined.\") if use_resource is None: use_resource = _DEFAULT_USE_RESOURCE v = variables.VariableV1( initial_value=init_val, name=name, trainable=trainable, collections=collections, caching_device=caching_device, dtype=variable_dtype, validate_shape=validate_shape, constraint=constraint, use_resource=use_resource, synchronization=synchronization, aggregation=aggregation) if context.executing_eagerly() and self._store_eager_variables: if collections: ops.add_to_collections(collections, v) else: ops.add_to_collection(ops.GraphKeys.GLOBAL_VARIABLES, v) if trainable: ops.add_to_collection(ops.GraphKeys.TRAINABLE_VARIABLES, v) if not context.executing_eagerly() or self._store_eager_variables: self._vars[name] = v logging.vlog(1, \"Created variable %s with shape %s and init %s\", v.name, format(shape), initializer) if regularizer: def make_regularizer_op(): with ops.colocate_with(v): with ops.name_scope(name + \"/Regularizer/\"): return regularizer(v) if regularizer(v) is not None: lazy_eval_tensor = _LazyEvalTensor(make_regularizer_op) ops.add_to_collection(ops.GraphKeys.REGULARIZATION_LOSSES, lazy_eval_tensor) return v"}
{"text_id": "9520", "text": "docstring: def cluster(tdf, cluster_radius_km=0.1, min_samples=1): tdf = tdf.sort_by_uid_and_datetime() frame = inspect.currentframe() args, _, _, arg_values = inspect.getargvalues(frame) arguments = dict([('function', cluster.__name__)]+[(i, arg_values[i]) for i in args[1:]]) groupby = [] if utils.is_multi_user(tdf): groupby.append(constants.UID) stops_df = tdf if len(groupby) > 0: ctdf = stops_df.groupby(groupby, group_keys=False, as_index=False).apply(_cluster_trajectory, cluster_radius_km=cluster_radius_km, min_samples=min_samples).reset_index(drop=True) else: ctdf = _cluster_trajectory(stops_df, cluster_radius_km=cluster_radius_km, min_samples=min_samples).reset_index(drop=True) ctdf.reset_index(inplace=True, drop=True) ctdf.parameters = tdf.parameters ctdf.set_parameter(constants.CLUSTERING_PARAMS, arguments) return ctdf"}
{"text_id": "9521", "text": "docstring: def transform(self, X): if self.origin_for_time_vars is None: raise NotFittedError( \"This instance is not fitted yet. Call 'fit' with appropriate arguments \" \"before calling 'transform'.\") assert isinstance(X, pd.DataFrame) dt = X[self.time_col] features_ts = build_time_features_df(dt, conti_year_origin=self.origin_for_time_vars) output = pd.concat([dt, features_ts], axis=1) return output"}
{"text_id": "9522", "text": "docstring: def __init_from_CONFIG (self, format=DEFAULT_NFFG_FORMAT): path = CONFIG.get_mininet_topology() if path is None: raise TopologyBuilderException(\"Missing Topology!\") self.__init_from_file(path=path, format=format)"}
{"text_id": "9523", "text": "docstring: def _objective(self, parameters_dict, pipeline, data, loss_function, loss_params: dict): pipeline = self.set_arg_pipeline(pipeline=pipeline, parameters=parameters_dict) metric_value = self.get_metric_value(data=data, pipeline=pipeline, loss_function=loss_function, loss_params=loss_params) return metric_value"}
{"text_id": "9524", "text": "docstring: def _find_fld(self): frame = inspect.currentframe().f_back.f_back while frame is not None: try: pkt = frame.f_locals['self'] except KeyError: pass else: if isinstance(pkt, tuple(self.dflt.owners)): return self._find_fld_pkt(pkt) frame = frame.f_back return self.dflt"}
{"text_id": "9525", "text": "docstring: def nparray_to_named_tensor(self, tensor_key, nparray): tensor_name, origin, round_number, report, tags = tensor_key if 'trained' in tags and self.delta_updates: model_nparray = self.tensor_db.get_tensor_from_cache( TensorKey( tensor_name, origin, round_number, report, ('model',) ) ) if model_nparray is not None: delta_tensor_key, delta_nparray = self.tensor_codec.generate_delta( tensor_key, nparray, model_nparray ) delta_comp_tensor_key, delta_comp_nparray, metadata = self.tensor_codec.compress( delta_tensor_key, delta_nparray ) named_tensor = utils.construct_named_tensor( delta_comp_tensor_key, delta_comp_nparray, metadata, lossless=False ) return named_tensor compressed_tensor_key, compressed_nparray, metadata = self.tensor_codec.compress( tensor_key, nparray, require_lossless=True ) named_tensor = utils.construct_named_tensor( compressed_tensor_key, compressed_nparray, metadata, lossless=True ) return named_tensor"}
{"text_id": "9526", "text": "docstring: def update_priorities(self, idxes, priorities): assert len(idxes) == len(priorities) for idx, priority in zip(idxes, priorities): assert priority > 0 assert 0 <= idx < len(self._storage) self._it_sum[idx] = priority**self._alpha self._it_min[idx] = priority**self._alpha self._it_max[idx] = priority**self._alpha self._max_priority = max(self._max_priority, priority)"}
{"text_id": "9527", "text": "docstring: def delete(self): self._delete = True"}
{"text_id": "9528", "text": "docstring: def InsertTextUTF8(self, pos, text): if not wx.USE_UNICODE: u = text.decode('utf-8') text = u.encode(wx.GetDefaultPyEncoding()) self.InsertTextRaw(pos, text)"}
{"text_id": "9529", "text": "docstring: def _add_member(self, member_info: base_page.MemberInfo) -> None: obj_type = obj_type_lib.ObjType.get(member_info.py_object) if obj_type is obj_type_lib.ObjType.MODULE: self._add_module(member_info) elif obj_type is obj_type_lib.ObjType.CLASS: self._add_class(member_info) elif obj_type is obj_type_lib.ObjType.CALLABLE: self._add_function(member_info) elif obj_type is obj_type_lib.ObjType.TYPE_ALIAS: self._add_type_alias(member_info) elif obj_type is obj_type_lib.ObjType.OTHER: self._add_other_member(member_info)"}
{"text_id": "9530", "text": "docstring: def _get_count(queryset): try: return queryset.count() except (AttributeError, TypeError): return len(queryset)"}
{"text_id": "9531", "text": "docstring: def hls2rgb_vt340(h, l, s): (r,g,b) = hls2rgb_normalized(h/360, l/100, s/100) return [ round(x*100) for x in (r,g,b) ]"}
{"text_id": "9532", "text": "docstring: def load_sentences_labels(self, sentences_file, labels_file, d): sentences = [] labels = [] with open(sentences_file, encoding='ISO-8859-1') as f: for sentence in f.readlines(): sentence = sentence.strip() if sentence: s = [self.vocab[token] if token in self.vocab else self.unk_ind for token in sentence.split(' ')] sentences.append(s) with open(labels_file) as f: for sentence in f.read().splitlines(): l = [self.tag_map[label] for label in sentence.split(' ')] labels.append(l) print(len(sentences), len(labels)) assert len(labels) == len(sentences) for i in range(len(labels)): assert len(labels[i]) == len(sentences[i]) d['data'] = sentences d['labels'] = labels d['size'] = len(sentences)"}
{"text_id": "9533", "text": "docstring: def processMessages( self, data ): if (data is None) or (len(data) == 0): return msgs = self.protoMsg.extract(data, self.recvCrypter, self.recvHMAC) if (msgs is None) or (len(msgs) == 0): return for msg in msgs: if msg.flags == const.FLAG_PAYLOAD: self.circuit.upstream.write(msg.payload) elif self.weAreClient and (msg.flags == const.FLAG_NEW_TICKET): assert len(msg.payload) == (const.TICKET_LENGTH + const.MASTER_KEY_LENGTH) peer = self.circuit.downstream.transport.getPeer() ticket.storeNewTicket(msg.payload[0:const.MASTER_KEY_LENGTH], msg.payload[const.MASTER_KEY_LENGTH: const.MASTER_KEY_LENGTH + const.TICKET_LENGTH], peer) elif self.weAreClient and (msg.flags == const.FLAG_PRNG_SEED): assert len(msg.payload) == const.PRNG_SEED_LENGTH log.debug(\"Obtained PRNG seed.\") prng = random.Random(msg.payload) pktDist = probdist.new(lambda: prng.randint(const.HDR_LENGTH, const.MTU), seed=msg.payload) self.pktMorpher = packetmorpher.new(pktDist) self.iatMorpher = probdist.new(lambda: prng.random() % const.MAX_PACKET_DELAY, seed=msg.payload) else: log.warning(\"Invalid message flags: %d.\" % msg.flags)"}
{"text_id": "9534", "text": "docstring: def onDelete( self, callback: Callable[['Value'], None], ) -> None: def _cb(obj, method): try: if method == WappstoMethod.DELETE: callback(self) except Exception: self.log.exception(\"onDelete callback error.\") raise self.connection.subscribe_value_event( uuid=self.uuid, callback=_cb )"}
{"text_id": "9535", "text": "docstring: def onaction(self): return self._onaction"}
{"text_id": "9536", "text": "docstring: async def store_problem(problem): connection = await aiosqlite.connect(\"data.db\") await connection.execute( \"INSERT INTO problems VALUES(?, ?, ?, ?, ?)\", ( problem[\"contestId\"], problem[\"index\"], problem[\"name\"], repr(problem[\"tags\"]), problem[\"rating\"], ), ) await connection.commit() await connection.close()"}
{"text_id": "9537", "text": "docstring: def _revert(self, *, snapshot_name: str) -> None: if not self._snapshot_exists(snapshot_name=snapshot_name): return self._line_round_dot_setting = self._line_round_dot_setting_snapshots[ snapshot_name]"}
{"text_id": "9538", "text": "docstring: def brush_add(): pass"}
{"text_id": "9539", "text": "docstring: def add_col(file_name, data): try: os.path.isfile(file_name) except IOError: print(\"Due to an error in the IO component of this program, the referrenced output file does not exist\") table_data = pd.read_csv(file_name) table_data.insert(len(table_data.columns),str(data[0]),data[1:],allow_duplicates=True) table_data.to_csv(file_name,index=False)"}
{"text_id": "9540", "text": "docstring: def deserialize(self, str): if python3: codecs.lookup_error(\"rosmsg\").msg_type = self._type try: end = 0 start = end end += 8 (self.target_distance,) = _get_struct_d().unpack(str[start:end]) return self except struct.error as e: raise genpy.DeserializationError(e)"}
{"text_id": "9541", "text": "docstring: def itermap(func, initial, n_steps=None, stop_func = None, include_zeroth = False): assert (n_steps is not None) or (stop_func is not None), 'You must either specify a number of steps or a stopping function.' val = initial results = [val] if include_zeroth else [] for _ in (xrange(n_steps) if n_steps is not None else itertools.count(start=0, step=1)): val = func(val) results.append(val) if stop_func is not None and stop_func(val): break return results"}
{"text_id": "9542", "text": "docstring: def handler( self: AddHandlerProtocol, handler: Optional[Callable] = None, *, command: Optional[str] = None, commands: Optional[Sequence[str]] = None, name: Optional[str] = None, description: Optional[str] = None, full_description: Optional[str] = None, include_in_status: Union[bool, Callable] = True, dependencies: Optional[Sequence[Depends]] = None, dependency_overrides_provider: Any = None, ) -> Callable: if handler: handler_commands: List[ Optional[str] ] = converters.optional_sequence_to_list(commands) if command and commands: handler_commands.insert(0, command) elif not commands: handler_commands = [command] for command_body in handler_commands: self.add_handler( body=command_body, handler=handler, name=name, description=description, full_description=full_description, include_in_status=include_in_status, dependencies=dependencies, dependency_overrides_provider=dependency_overrides_provider, ) return handler return partial( cast(HandlerDecoratorProtocol, self).handler, command=command, commands=commands, name=name, description=description, full_description=full_description, include_in_status=include_in_status, dependencies=dependencies, dependency_overrides_provider=dependency_overrides_provider, )"}
{"text_id": "9543", "text": "docstring: def impute_all(self, mask_impute, t_init_mask, n=99999, plot=True): results = [] for i in range(self.test_data.sequences // self.config.batch_size): results.append(self.impute(mask_impute, t_init_mask=t_init_mask, idx_batch=i, n=n, plot=plot)) return np.array(results).mean(axis=0)"}
{"text_id": "9544", "text": "docstring: def addToExtent(self, extent , i , amount): temp=[extent[0],extent[1],extent[2],extent[3],extent[4],extent[5]] temp[i] = temp[i] + amount extent = (temp[0],temp[1],temp[2],temp[3],temp[4],temp[5]) return extent"}
{"text_id": "9545", "text": "docstring: def members(self): allmembers = set() for team in self.teams(): allmembers.update(team.members()) return sorted(allmembers)"}
{"text_id": "9546", "text": "docstring: def add_additional_private_key(self, private_key): self._transport.add_issuer_key(private_key)"}
{"text_id": "9547", "text": "docstring: def update(self): if self._source.lower() == 'instagram': self._comic_url = self.scrape_instagram_user(self._ig_user) else: self._comic_url = self.scrape_url(self._url)"}
{"text_id": "9548", "text": "docstring: def run_pipeline_template(dataflow_project, template_region, template_location, input_location, group_by, write_disposition, dataset, stage, load_time, num_shards, add_load_date_suffix, runtime_environment): credentials = GoogleCredentials.get_application_default() df_service = build('dataflow', 'v1b3', credentials=credentials, cache_discovery=False) job_name = get_job_name(load_time) body = { 'jobName': job_name, 'parameters': { 'input': input_location, 'load_time': load_time, 'stage': stage, 'group_by': group_by, 'write_disposition': write_disposition, 'num_shards': num_shards, 'add_load_date_suffix': add_load_date_suffix, 'dataset': dataset, }, 'environment': runtime_environment } logging.info('launching template %s in %s:%s with %s', template_location, dataflow_project, template_region, pprint.pformat(body)) launch_result = df_service.projects().locations().templates().launch( location=template_region, projectId=dataflow_project, gcsPath=template_location, body=body).execute(num_retries=5) logging.info('waiting on pipeline : %s', pprint.pformat(launch_result)) return wait_on_pipeline_job(df_service, launch_result['job'])"}
{"text_id": "9549", "text": "docstring: def process_risk_level(self, data): return self._process_job(data)"}
{"text_id": "9550", "text": "docstring: def updatePosition(self): newpos = self.modules['position'].get() if(not newpos['valid']): return oldpos = self.get('ownpos'); if ((oldpos['valid']) and (oldpos['lon'] == newpos['lon']) and (oldpos['lat'] == oldpos['lat'])): return self.set('ownpos', newpos) self.handleUpdatedPosition()"}
{"text_id": "9551", "text": "docstring: def fillDiagWithRandomTokens(self): for j in range(0, self.block_size): tokens = Helper.shuffle(self.tokens.copy()) self.setBlock(tokens, j, j)"}
{"text_id": "9552", "text": "docstring: def _check_capture(self, new_r, new_c, game): piece = game[(new_r, new_c)] if not isinstance(piece, Null): return piece.is_white != self.is_white"}
{"text_id": "9553", "text": "docstring: def _set_system_time(self, timestamp, force=False): self._logger.info(f\"Set system time to {self._time}\") try: p = Popen(['timedatectl', 'show', '--property=NTPSynchronized', '--value'], stdout=PIPE, stderr=PIPE) out, err = p.communicate() if p.returncode != 0: self._logger.error( f\"Error retrieving NTPSynchronized: {err.decode('utf-8')}\" ) if out.decode('utf-8') != 'yes': t = timestamp.strftime('%Y/%m/%d %H:%M:%S') p = Popen(['sudo', 'timedatectl', 'set-time', f'\"{t}\"'], stdout=PIPE, stderr=PIPE) out, err = p.communicate() if p.returncode == 0: self._logger.debug(f\"System time set to {t}\") else: self._logger.error( f\"timedatectl time-set error: {err.decode('utf-8')}\" ) except OSError as e: self._logger.error(f\"Could not set the system time: {e}\")"}
{"text_id": "9554", "text": "docstring: def feature_creation(data_directory): logger = logging.getLogger(__name__) logger.info(\"Feature Creation: Reading in data from %s\" % data_directory) df = pd.read_csv(data_directory) assert np.isin(BASE_FEATURES, df.columns).all() logger.info(\"Feature Creation: Generating temporal features\") assert 'click_time' in df.columns drop_cols = [c for c in df.columns if \"_time\" in c] df = df.drop(columns=drop_cols) return df"}
{"text_id": "9555", "text": "docstring: def discover(bot, update, args, client): ret = client.discover(args[0]) category_id = get_categoryid(update.message.chat_id) client.create_feed(ret[0]['url'], category_id, crawler=True) bot.send_message(chat_id=update.message.chat_id, text=ADD_FEED_OK_MSG)"}
{"text_id": "9556", "text": "docstring: def update_duration(self, new_duration, new_calories_burned): self.calories_burned=new_calories_burned self.exercise_duration=new_duration db.session.commit()"}
{"text_id": "9557", "text": "docstring: async def load_environments(self) -> None: self.set_op(\"load_environments\") environments_cursor = self.central_mongo_connection.find( collection=AsyncMongoConnector.environments_collection, query={\"imported\": True} ) self.pods = {} async for env in environments_cursor: pod = PodData.from_env_config(env) if not pod: self.log.warning(\"Failed to parse pod data from environment config: {}\".format(env[\"name\"])) continue self.pods[pod.full_name] = pod"}
{"text_id": "9558", "text": "docstring: def create_entries_ldap_external_user_directory_config_content(entries, config_d_dir=\"/etc/clickhouse-server/config.d\", config_file=\"ldap_external_user_directories.xml\"): uid = getuid() path = os.path.join(config_d_dir, config_file) name = config_file root = xmltree.fromstring(\"<yandex><user_directories></user_directories></yandex>\") xml_user_directories = root.find(\"user_directories\") xml_user_directories.append(xmltree.Comment(text=f\"LDAP external user directories {uid}\")) for entry in entries: servers, roles_entries = entry xml_directory = xmltree.Element(\"ldap\") for server in servers: if server is not None: xml_append(xml_directory, \"server\", server) if roles_entries: for roles_entry in roles_entries: xml_roles = xmltree.Element(\"roles\") if roles_entry: for role in roles_entry: if role is not None: xml_append(xml_roles, role, \"\") xml_directory.append(xml_roles) xml_user_directories.append(xml_directory) xml_indent(root) content = xml_with_utf8 + str(xmltree.tostring(root, short_empty_elements=False, encoding=\"utf-8\"), \"utf-8\") return Config(content, path, name, uid, \"config.xml\")"}
{"text_id": "9559", "text": "docstring: def forward(self, x, dropout=True): x = self.activation(self.linear1(x)) x = self.norm1(x) x = dorefa_connect.DorefaQuant(x, self.bitwight) x = self.activation(self.linear2(x)) x = self.norm2(x) x = dorefa_connect.DorefaQuant(x, self.bitwight) x = self.activation(self.linear3(x)) x = self.norm3(x) x = dorefa_connect.DorefaQuant(x, self.bitwight) x = self.linear4(x) return self.act_end(x)"}
{"text_id": "9560", "text": "docstring: def LevelColormap(levels, cmap=None, reverse=False): if cmap is None: cmap = plt.get_cmap() nlev = len(levels) S = np.arange(nlev, dtype=\"float\") / (nlev - 1) A = cmap(S) levels = np.array(levels, dtype=\"float\") L = (levels - levels[0]) / (levels[-1] - levels[0]) S = list(range(nlev)) if reverse: levels = levels[::-1] L = (levels - levels[-1]) / (levels[0] - levels[-1]) S.reverse() R = [(L[i], A[i, 0], A[i, 0]) for i in S] G = [(L[i], A[i, 1], A[i, 1]) for i in S] B = [(L[i], A[i, 2], A[i, 2]) for i in S] cdict = dict(red=tuple(R), green=tuple(G), blue=tuple(B)) return plt.matplotlib.colors.LinearSegmentedColormap( \"%s_levels\" % cmap.name, cdict, 256 )"}
{"text_id": "9561", "text": "docstring: def _validate_all_scalars_have_implementations(self) -> List[str]: errors = [] for type_name, gql_type in self.type_definitions.items(): if isinstance(gql_type, GraphQLScalarType) and ( gql_type.coerce_output is None or gql_type.coerce_input is None or gql_type.parse_literal is None ): errors.append( f\"Scalar < {type_name} > \" f\"is missing an implementation\" ) return errors"}
{"text_id": "9562", "text": "docstring: def calculateWeights(self): for lut in self.luts: lut.weight = sum(1 for _lut in self.luts if lut.output in _lut.inputs)"}
{"text_id": "9563", "text": "docstring: def post_sys_capabilities_self(self, **kwargs): \"\"\"Fetches the capabilities of the given token on the given path. This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.post_sys_capabilities_self(async_req=True) >>> result = thread.get() :param async_req bool :param Body47 body: :return: None If the method is called asynchronously, returns the request thread. \"\"\" kwargs['_return_http_data_only'] = True if kwargs.get('async_req'): return self.post_sys_capabilities_self_with_http_info(**kwargs) else: (data) = self.post_sys_capabilities_self_with_http_info(**kwargs) return data"}
{"text_id": "9564", "text": "docstring: def filter_equal(self, column_name, value, new_table_name, column_types): return self.filter_by_function(column_name, lambda x: x == value, new_table_name, column_types)"}
{"text_id": "9565", "text": "docstring: def filter_with_date_range(self, queryset): if not self.date_range_field_name: return queryset if self.start_date: start_datetime = utils.datetime_combine(self.start_date, time.min) filter_kwargs = { \"%s__gte\" % self.date_range_field_name: start_datetime, } queryset = queryset.filter(**filter_kwargs) if self.end_date: end_datetime = utils.datetime_combine(self.end_date, time.max) filter_kwargs = { \"%s__lte\" % self.date_range_field_name: end_datetime, } queryset = queryset.filter(**filter_kwargs) return queryset"}
{"text_id": "9566", "text": "docstring: def from_map(self, schema, indicts): for key in list(indicts.keys()): if not key.startswith('text search dictionary '): raise KeyError(\"Unrecognized object type: %s\" % key) tsd = key[23:] self[(schema.name, tsd)] = tsdict = TSDictionary( schema=schema.name, name=tsd) indict = indicts[key] if indict: for attr, val in list(indict.items()): setattr(tsdict, attr, val) if 'oldname' in indict: tsdict.oldname = indict['oldname'] del indict['oldname'] if 'description' in indict: tsdict.description = indict['description']"}
{"text_id": "9567", "text": "docstring: def create_on_client_thread(self, client_data): (client_socket, client_addr) = client_data while True: try: with client_socket: logger.info(temp().client_socket_data( client_socket, client_addr )) chat_thread = threading.Thread( target=ServerSocket.loop_chat_handler, args=(client_socket, client_addr), daemon=True) chat_thread.start() logger.info(\"thread start\") except Exception as err: logger.error(err)"}
{"text_id": "9568", "text": "docstring: def display_wealth(results: Dict[str, str]) -> None: if \"wealth\" not in results.keys(): return print(\"\\n\", \"Wealth\", \"\\n===============================\") net_worth = results[\"wealth\"][\"investment_net_worth\"] gains = results[\"wealth\"][\"income_gains\"] debts = results[\"wealth\"][\"liabilities\"] salaries = results[\"wealth\"][\"salary_income\"] gross_low, gross_high = [ color.RED + color.BOLD + str(\"${:,}\".format(x)) + color.END if x > 1000000 else str(\"${:,}\".format(x)) for x in net_worth ] gains_low, gains_high = [ color.RED + color.BOLD + str(\"${:,}\".format(x)) + color.END if x > 50000 else str(\"${:,}\".format(x)) for x in gains ] if (net_worth[0] - gains[0]) > 0: percent = 100 * gains[0] / (net_worth[0] - gains[0]) yoy_percent = ( color.RED + color.BOLD + str(\"{:,.2f}%\".format(percent)) + color.END if percent > 5 else str(\"{:,.2f}%\".format(percent)) ) else: yoy_percent = 0 debts_low, debts_high = [ color.RED + color.BOLD + str(\"${:,}\".format(x)) + color.END if x > 50000 else str(\"${:,}\".format(x)) for x in debts ] print(\"Investments Total:\".ljust(25), \"%s to %s\" % (gross_low, gross_high)) print( \"Investments gains YOY:\".ljust(25), \"%s to %s\" % (gains_low, gains_high), ) print(\"Percent gains YOY:\".ljust(25), \"%s\" % (yoy_percent)) print(\"Debts:\".ljust(25), \"%s to %s\" % (debts_low, debts_high)) print( \"Other incomes totaling:\".ljust(25), \"%s\" % (\"${:,.2f}\".format(salaries)), ) print(\"\\n\")"}
{"text_id": "9569", "text": "docstring: def clean_df(self,manymissing_p = 0.9,drop_col = None,filter_constantcol = True): col_to_remove = [] if manymissing_p: col_to_remove += list(self.manymissing(manymissing_p)) if filter_constantcol: col_to_remove += list(self.constantcol()) if isinstance(drop_col,list): col_to_remove += drop_col elif isinstance(drop_col,str): col_to_remove += [drop_col] else : pass return self.drop(pd.unique(col_to_remove),axis = 1)"}
{"text_id": "9570", "text": "docstring: def translit(self, value, reversed=False, strict=False, fail_silently=True): if not six.PY3: value = unicode(value) if reversed: if self.reversed_specific_mapping: value = value.translate( self.reversed_specific_translation_table ) if self.reversed_specific_pre_processor_mapping: for rule in self.reversed_specific_pre_processor_mapping_keys: value = value.replace( rule, self.reversed_specific_pre_processor_mapping[rule] ) if self.reversed_pre_processor_mapping: for rule in self.reversed_pre_processor_mapping_keys: value = value.replace( rule, self.reversed_pre_processor_mapping[rule] ) return value.translate(self.reversed_translation_table) if self.pre_processor_mapping: for rule in self.pre_processor_mapping_keys: value = value.replace(rule, self.pre_processor_mapping[rule]) res = value.translate(self.translation_table) if strict: res = self._make_strict(value=res, reversed=reversed, fail_silently=fail_silently) return res"}
{"text_id": "9571", "text": "docstring: def find_col_by_name(self, name): ret_val = self._find_col_by_name(name.encode()) return ret_val"}
{"text_id": "9572", "text": "docstring: def build_body(self): parts = [] part_boundary = '--' + self.boundary parts.extend( [bytes(part_boundary.encode(self.charset)), bytes(('Content-Disposition: form-data; name=\"%s\"' % name).encode(self.charset)) if PYTHON_VERSION_3 else ('Content-Disposition: form-data; name=\"%s\"' % name), bytes(('Content-Type: text/plain; charset=%s' % self.charset).encode(self.charset)), bytes(''.encode(self.charset)), bytes(value.encode(self.charset)) if PYTHON_VERSION_3 else value ] for name, value in self.form_fields ) parts.extend( [bytes(part_boundary.encode(self.charset)), bytes(('Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"' % (field_name, filename)).encode(self.charset)) if PYTHON_VERSION_3 else ('Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"' % (field_name, filename)), bytes(('Content-Type: %s' % content_type).encode(self.charset)), bytes('Content-Transfer-Encoding: binary'.encode(self.charset)), bytes(''.encode(self.charset)), body, ] for field_name, filename, content_type, body in self.files ) flattened = list(itertools.chain(*parts)) flattened.append(bytes(('--' + self.boundary + '--').encode(self.charset))) flattened.append(bytes(''.encode(self.charset))) return bytes('\\r\\n'.encode(self.charset)).join(flattened)"}
{"text_id": "9573", "text": "docstring: def load_input_mhe(self, src_kind, **kwargs): src = kwargs.pop(\"src\", self.d1) fe = kwargs.pop(\"fe\", 1) if src_kind == \"mod\": for u in self.u: usrc = getattr(src, u) utrg = getattr(self.lsmhe, u) utrg[fe].value = value(usrc[1]) elif src_kind == \"self.dict\": for u in self.u: utrg = getattr(self.lsmhe, u) utrg[fe].value = value(self.curr_u[u])"}
{"text_id": "9574", "text": "docstring: async def async_step_user(self, user_input=None): self._errors = {} if any( ( user_input is not None and e.data.get(CONF_HOSTNAME) == user_input[CONF_HOSTNAME] ) for e in self._async_current_entries() ): return self.async_abort(reason=\"hostname_in_use\") if user_input is not None: valid = await self._test_hostname(user_input[CONF_HOSTNAME]) if valid: return self.async_create_entry( title=user_input[CONF_DISPLAY_NAME], data=user_input, ) else: self._errors[\"base\"] = \"hostname\" return await self._show_config_form(user_input) user_input = {} user_input[CONF_DISPLAY_NAME] = \"\" user_input[CONF_HOSTNAME] = \"\" return await self._show_config_form(user_input)"}
{"text_id": "9575", "text": "docstring: def _get_router_ext_subnet_for_l3p(self, context, l3policy): rtr_sn = {} routers = self._get_routers(context._plugin_context, {'id': l3policy['routers']}) for r in routers: if (not r['external_gateway_info'] or not r['external_gateway_info']['external_fixed_ips']): continue for ip in r['external_gateway_info']['external_fixed_ips']: rtr_sn[ip['subnet_id']] = r['id'] return rtr_sn"}
{"text_id": "9576", "text": "docstring: def price(self) -> int: return sum(food.price for food in self.foods)"}
{"text_id": "9577", "text": "docstring: def _build_app_dict(self, request, label=None): app_dict = {} if label: models = { m: m_a for m, m_a in self._registry.items() if m._meta.app_label == label } else: models = self._registry for model, model_admin in models.items(): app_label = model._meta.app_label has_module_perms = model_admin.has_module_permission(request) if not has_module_perms: continue perms = model_admin.get_model_perms(request) if True not in perms.values(): continue info = (app_label, model._meta.model_name) model_dict = { 'name': capfirst(model._meta.verbose_name_plural), 'object_name': model._meta.object_name, 'icon': model._meta.icon, 'perms': perms, } if perms.get('change'): try: model_dict['admin_url'] = reverse('admin:%s_%s_changelist' % info, current_app=self.name) except NoReverseMatch: pass if perms.get('add'): try: model_dict['add_url'] = reverse('admin:%s_%s_add' % info, current_app=self.name) except NoReverseMatch: pass if app_label in app_dict: app_dict[app_label]['models'].append(model_dict) else: app_dict[app_label] = { 'name': apps.get_app_config(app_label).verbose_name, 'app_label': app_label, 'app_url': reverse( 'admin:app_list', kwargs={'app_label': app_label}, current_app=self.name, ), 'has_module_perms': has_module_perms, 'models': [model_dict], } if label: return app_dict.get(label) return app_dict"}
{"text_id": "9578", "text": "docstring: def _train_or_load_model(self, x: np.ndarray, y: np.ndarray, validation_data: Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, Any]] = None, validation_split: float = 0., ) -> None: assert len(x) == len(y), \\ f\"The x and y data matrices need to contain the same number of instances (actual: {len(x)} and {len(y)})!\" model_file = self._experiment_folder / \"keras-model.hd5\" model: Optional[Sequential] = None if model_file.exists(): logging.info(f\"Loading models from: {model_file}\") model: Sequential = load_model(str(model_file)) if not model: train the model prepare data (shuffle, i.e., sorted data) if self._shuffle_training_data is not False: if self._shuffle_training_data is not True: assume `shuffle_training_data` contains the random seed np.random.seed(self._shuffle_training_data) indices = np.arange(len(x)) np.random.shuffle(indices) x = x[indices] y = y[indices] create a sequential model model, loss = self._setup_model(y=y) model.compile(loss=loss, optimizer=Adam(lr=self._learning_rate, decay=self._learning_decay), metrics=['accuracy', 'mean_squared_error'], ) run training and catch Ctrl+C to create plots etc anyway try: prepare training progress CSV if not self._experiment_folder.exists(): self._experiment_folder.mkdir(parents=True, exist_ok=True) csv_log = self._experiment_folder / \"epoch_results.csv\" start the training (fit the model to the data) model.fit(x=x, y=y, shuffle=False, validation_split=validation_split, validation_data=validation_data, batch_size=self._batch_size, epochs=self._num_epochs, callbacks=[CSVLogger(str(csv_log), append=False, separator=';')]) except KeyboardInterrupt: logging.warning(f\"KeyboardInterrupt: Interrupting model fit ...\") noinspection PyUnresolvedReferences history_data: dict = model.history.history if model and model.history else None if history_data: plot training progress self._plot_training_stats(history_data) serialize model iff the training finished (was not interrupted) if self._num_epochs <= len(history_data['loss']): if the training finished: serialize data for next time save_model(model, filepath=str(model_file)) gc.collect() model.summary() self._model = model"}
{"text_id": "9579", "text": "docstring: def estimate_barycorr(self): obstime = self.astropy_time loc = EarthLocation.of_site(self.site_name) sc = SkyCoord(ra=self.RA, dec=self.DEC) barycorr = sc.radial_velocity_correction(obstime=obstime, location=loc) return barycorr"}
{"text_id": "9580", "text": "docstring: def _assert_validation_error(self, expected_error_substring): with self.assertRaisesRegexp( utils.ValidationError, expected_error_substring): self.blog_post.validate()"}
{"text_id": "9581", "text": "docstring: def _run_wrapped(func, master_ip, port, world_size, rank, dev, args, kwargs): init_process_group( master_ip=master_ip, port=port, world_size=world_size, rank=rank, device=dev ) func(*args, **kwargs)"}
{"text_id": "9582", "text": "docstring: def initialize( root: Optional[Path] = None, ) -> None: root = root or Path.cwd() logger.debug(\"Initializing new repository at path %s.\", root) if Repository.is_repository_directory(root): raise InitializationException( f\"Found existing ZenML repository at path '{root}'.\" ) config_directory = str(root / REPOSITORY_DIRECTORY_NAME) utils.create_dir_recursive_if_not_exists(config_directory) Repository(root=root)"}
{"text_id": "9583", "text": "docstring: def from_qcschema( cls, qca_record, client=None, toolkit_registry=GLOBAL_TOOLKIT_REGISTRY, allow_undefined_stereo=False, ): if not isinstance(qca_record, dict): try: qca_record = qca_record.dict(encoding=\"json\") except AttributeError: raise AttributeError( \"The object passed could not be converted to a dict with json encoding\" ) if \"attributes\" in qca_record: mapped_smiles = qca_record[\"attributes\"][ \"canonical_isomeric_explicit_hydrogen_mapped_smiles\" ] if client is not None: try: input_mols = client.query_molecules( id=qca_record[\"initial_molecules\"] ) except KeyError: input_mols = client.query_molecules( id=qca_record[\"initial_molecule\"] ) except AttributeError: raise AttributeError( \"The provided client can not query molecules, make sure it is an instance of\" \"qcportal.client.FractalClient() with the correct address.\" ) else: input_mols = [] elif \"extras\" in qca_record: mapped_smiles = qca_record[\"extras\"][ \"canonical_isomeric_explicit_hydrogen_mapped_smiles\" ] input_mols = [qca_record] else: raise KeyError( \"The record must contain the hydrogen mapped smiles to be safely made from the archive. \" \"It is not present in either 'attributes' or 'extras' on the provided `qca_record`\" ) offmol = cls.from_mapped_smiles( mapped_smiles, toolkit_registry=toolkit_registry, allow_undefined_stereo=allow_undefined_stereo, ) initial_ids = {} for molecule in input_mols: if not isinstance(molecule, dict): mol = molecule.dict(encoding=\"json\") else: mol = molecule geometry = unit.Quantity( np.array(mol[\"geometry\"], float).reshape(-1, 3), unit.bohr ) try: offmol._add_conformer(geometry.in_units_of(unit.angstrom)) if \"id\" in mol: initial_ids[mol[\"id\"]] = offmol.n_conformers - 1 except InvalidConformerError: print( \"Invalid conformer for this molecule, the geometry could not be attached.\" ) if initial_ids: offmol._properties[\"initial_molecules\"] = initial_ids return offmol"}
{"text_id": "9584", "text": "docstring: def accept_bid(self, *args): bid_id = self.bm.add_bid(*args) return bid_id"}
{"text_id": "9585", "text": "docstring: def prepare_rootfs_ext4_signed(cls, cr_workdir, oe_builddir, rootfs_dir, native_sysroot, partition): print ('Started the correct thing') p_prefix = os.environ.get(\"PSEUDO_PREFIX\", \"%s/usr\" % native_sysroot) p_localstatedir = os.environ.get(\"PSEUDO_LOCALSTATEDIR\", \"%s/../pseudo\" % rootfs_dir) p_passwd = os.environ.get(\"PSEUDO_PASSWD\", rootfs_dir) p_nosymlinkexp = os.environ.get(\"PSEUDO_NOSYMLINKEXP\", \"1\") pseudo = \"export PSEUDO_PREFIX=%s;\" % p_prefix pseudo += \"export PSEUDO_LOCALSTATEDIR=%s;\" % p_localstatedir pseudo += \"export PSEUDO_PASSWD=%s;\" % p_passwd pseudo += \"export PSEUDO_NOSYMLINKEXP=%s;\" % p_nosymlinkexp pseudo += \"%s/usr/bin/pseudo \" % native_sysroot rootfs = \"%s/rootfs_%s.%s.%s\" % (cr_workdir, partition.label, partition.lineno, partition.fstype) if os.path.isfile(rootfs): os.remove(rootfs) du_cmd = \"du -ks %s\" % rootfs_dir out = exec_cmd(du_cmd) actual_rootfs_size = int(out.split()[0]) extra_blocks = partition.get_extra_block_count(actual_rootfs_size) if extra_blocks < partition.extra_space: extra_blocks = partition.extra_space rootfs_size = actual_rootfs_size + extra_blocks rootfs_size *= partition.overhead_factor msger.debug(\"Added %d extra blocks to %s to get to %d total blocks\" % \\ (extra_blocks, partition.mountpoint, rootfs_size)) exec_cmd(\"truncate %s -s %d\" % (rootfs, rootfs_size * 1024)) extra_imagecmd = \"-i 8192\" key_file = \"/home/esven/openembedded/krogoth/poky/meta-intel-iot-security/meta-integrity/scripts/keys/privkey_ima.pem\" label_str = \"\" if partition.label: label_str = \"-L %s\" % partition.label mkfs_cmd = \"mkfs.%s -F %s %s -k %s %s -d %s\" % \\ (partition.fstype, extra_imagecmd, rootfs, key_file, label_str, rootfs_dir) print (\"%s\" % mkfs_cmd) exec_native_cmd(mkfs_cmd, native_sysroot, pseudo=pseudo) partition.source_file = rootfs du_cmd = \"du -Lbks %s\" % rootfs out = exec_cmd(du_cmd) partition.size = out.split()[0]"}
{"text_id": "9586", "text": "docstring: def create_visibility(trace_lengths: List[int], visible_list: List[bool]) -> List[bool]: visibility = [] for visible, trace_length in zip(visible_list, trace_lengths): visibility += list(repeat(visible, trace_length)) return visibility"}
{"text_id": "9587", "text": "docstring: def extract_pandas_labels(labels): if isinstance(labels, pd.DataFrame): if len(labels.columns) > 1: raise ValueError('Only one column for labels is allowed.') if all(dtype.name in PANDAS_DTYPES for dtype in labels.dtypes): return labels.values.astype('float') else: raise ValueError('Data types for labels must be int, float, or bool.') else: return labels"}
{"text_id": "9588", "text": "docstring: def switch_intervention(patient, inputs, new_intervention, new_obs_state, resource_utilization, resource_availability, state_tracker): rec_reqs = inputs.resource_requirements resource_utilization -= np.unpackbits(rec_reqs[patient[INTERVENTION], patient[OBSERVED_STATE]]) new_intv = new_intervention new_req = rec_reqs[new_intv, new_obs_state] count = 0 while np.packbits(0 >= resource_availability - resource_utilization) & new_req: new_intv = inputs.fallback_interventions[new_intv, new_obs_state] new_req = rec_reqs[new_intv, new_obs_state] count += 1 if count >= INTERVENTIONS_NUM: raise UserWarning(\"No intervention available for the given resource constraints\") resource_utilization += np.unpackbits(new_req) if patient[INTERVENTION] != new_intv: patient[INTERVENTION] = new_intv vax = inputs.vaccination[new_intv] if (0 <= vax < VACCINES_NUM) and not (patient[FLAGS] & IS_INFECTED): new_istate = vax + 2 patient[IMMUNE_STATE] = new_istate patient[DISEASE_PROGRESSION] = np.random.choice(DISEASE_PROGRESSIONS_NUM, p=inputs.severity_dist[new_istate, patient[SUBPOPULATION]]) if np.random.random() < inputs.prob_full_immunity[new_istate, patient[SUBPOPULATION]]: patient[DISEASE_STATE] = IMMUNE"}
{"text_id": "9589", "text": "docstring: def trimmable(dmrs, leaves=True): if leaves: initial = iter_leaves forward = dmrs.get_in_nodes back = dmrs.get_out_nodes else: initial = iter_roots forward = dmrs.get_out_nodes back = dmrs.get_in_nodes discard = {n.nodeid for n in initial(dmrs)} parents = {p for leaf in discard \\ for p in forward(leaf, nodeids=True, itr=True)} n = True while n: n = 0 next_parents = set() for mother in parents: if back(mother, nodeids=True) - discard: next_parents.add(mother) else: n += 1 discard.add(mother) next_parents.update(forward(mother, nodeids=True, itr=True)) parents = next_parents return discard"}
{"text_id": "9590", "text": "docstring: def saveProfile(profile): print(profile[\"id\"]) added = False with open(\"json/controlProfiles.json\", \"r+\") as file: data = json.load(file) for i in range(0, len(data)): if(str(data[i][\"id\"]) == str(profile[\"id\"])): data[i] = profile added = True if not added: data.append(profile) print(data) file.seek(0) json.dump(data, file, indent=4) file.truncate()"}
{"text_id": "9591", "text": "docstring: def create_optimizer( config: ml_collections.ConfigDict) -> optax.GradientTransformation: if config.optimizer == 'adam': return optax.adam( learning_rate=config.learning_rate) if config.optimizer == 'sgd': return optax.sgd( learning_rate=config.learning_rate, momentum=config.momentum) raise ValueError(f'Unsupported optimizer: {config.optimizer}.')"}
{"text_id": "9592", "text": "docstring: async def google(self, ctx, *, query: str.lower): await self.do_google_search(ctx, query, search_images=False)"}
{"text_id": "9593", "text": "docstring: def add_coexpression_clusters(): form = AddCoexpressionClustersForm(request.form) form.populate_networks() if request.method == 'POST' and form.validate(): network_id = int(request.form.get('network_id')) description = request.form.get('description') min_size = int(request.form.get('min_size')) file = request.files[form.file.name].read() if file != b'': fd, temp_path = mkstemp() with open(temp_path, 'wb') as cluster_writer: cluster_writer.write(file) CoexpressionClusteringMethod.add_lstrap_coexpression_clusters(temp_path, description, network_id, min_size=min_size) os.close(fd) os.remove(temp_path) flash('Added coexpression clusters for network method %d' % network_id, 'success') else: flash('Empty or no file provided, cannot add coexpression network', 'warning') return redirect(url_for('admin.index')) else: if not form.validate(): flash('Unable to validate data, potentially missing fields', 'danger') return redirect(url_for('admin.index')) else: abort(405)"}
{"text_id": "9594", "text": "docstring: def load_data(cls, input_file): job = autopilot.ReviewAutoPilot() category_transitions = {} with open(input_file) as src: for line in src: (category, start), transitions = job.parse_output_line(line) category_transitions.setdefault(category, {})[start] = transitions return category_transitions"}
{"text_id": "9595", "text": "docstring: def _compose_date(entity): full_date = '' if 'year' in entity['metadata']: full_date = entity['metadata']['year'] if 'month' in entity['metadata']: if len(entity['metadata']['month']) == 1: month = '0' + entity['metadata']['month'] else: month = entity['metadata']['month'] if full_date: full_date = full_date + '-' + month else: full_date = month if 'day' in entity['metadata']: if len(entity['metadata']['day']) == 1: day = '0' + entity['metadata']['day'] else: day = entity['metadata']['day'] if full_date: full_date = full_date + '-' + day else: full_date = day return [full_date, entity['name']]"}
{"text_id": "9596", "text": "docstring: def load_psyonix_bots(): psyonix_allstar = get_bot_config_bundle(PackageFiles.psyonix_allstar) psyonix_pro = get_bot_config_bundle(PackageFiles.psyonix_pro) psyonix_rookie = get_bot_config_bundle(PackageFiles.psyonix_rookie) psyonix_bots_skill[psyonix_allstar.name] = 1.0 psyonix_bots_skill[psyonix_pro.name] = 0.5 psyonix_bots_skill[psyonix_rookie.name] = 0.0 return psyonix_allstar, psyonix_pro, psyonix_rookie"}
{"text_id": "9597", "text": "docstring: def download_dir(self, blob, local_path, container_name=None, use_basename=True): if not container_name: container_name, _, blob = self.parse_wasbs_url(blob) local_path = os.path.abspath(local_path) if use_basename: local_path = append_basename(local_path, blob) try: check_dir_exists(local_path, is_dir=True) except DblueStoresException: os.makedirs(local_path) results = self.list(container_name=container_name, key=blob, delimiter='/') for prefix in sorted(results['prefixes']): direname = os.path.join(local_path, prefix) prefix = os.path.join(blob, prefix) self.download_dir(blob=prefix, local_path=direname, container_name=container_name, use_basename=False) for file_key in results['blobs']: file_key = file_key[0] filename = os.path.join(local_path, file_key) file_key = os.path.join(blob, file_key) self.download_file(blob=file_key, local_path=filename, container_name=container_name, use_basename=False)"}
{"text_id": "9598", "text": "docstring: def draw_header(self, canvas, doc, header_paragraph: Paragraph, header_pagenum_style: ParagraphStyle): if not header_paragraph: return style = header_paragraph.style if style.borderWidth: canvas.setStrokeColor(style.borderColor) canvas.setLineWidth(style.borderWidth) canvas.line( self.doc.left, self.doc.top, self.doc.right, self.doc.top) canvas.setFont(style.fontName, style.fontSize) canvas.setFillColor(style.textColor) y_pos = self.doc.pagesize[1] - (self.doc.topMargin * 0.85) if style.alignment == TA_LEFT: x_pos = self.doc.left canvas.drawString(x_pos, y_pos, header_paragraph.text) elif style.alignment == TA_RIGHT: x_pos = self.doc.right canvas.drawRightString(x_pos, y_pos, header_paragraph.text) elif style.alignment == TA_CENTER: x_pos = self.doc.center canvas.drawCentredString(x_pos, y_pos, header_paragraph.text) if header_pagenum_style is not None: if header_pagenum_style.alignment == TA_LEFT: canvas.drawString(self.doc.left, y_pos, str(canvas.getPageNumber())) elif header_pagenum_style.alignment == TA_RIGHT: canvas.drawRightString(self.doc.right, y_pos, str(canvas.getPageNumber())) elif header_pagenum_style.alignment == TA_CENTER: canvas.drawCenteredString(self.doc.center, y_pos, str(canvas.getPageNumber()))"}
{"text_id": "9599", "text": "docstring: def _is_target_prompt(self, line): found = self._regex_helper.search_compiled(self._re_expected_prompt, line) return found"}
{"text_id": "9600", "text": "docstring: def Eandg(rOH,thetaHOH): kOH = 50.0 rOHe = 0.95 kHOH = 50.0 thetaHOHe = 104.5 E = 2 * kOH * (rOH - rOHe)**2 + kHOH * (thetaHOH - thetaHOHe)**2 grOH = 2 * kOH * (rOH - rOHe) grthetaHOH = 2 * kHOH * (thetaHOH - thetaHOHe) return (E, grOH, grthetaHOH)"}
{"text_id": "9601", "text": "docstring: def listen_for_prompt(self): with sr.Microphone() as mike: while True: try: print \"Say 'VIRA' to activate me...\", '\\r', sys.stdout.flush() audio = self.recognizer.listen(mike) prompt = self.recognizer.recognize_google_cloud(audio, credentials_json=self.google_credentials, preferred_phrases=self.preferred_phrases) if \"vira\" in prompt.lower(): sys.stdout.write(\"\\033[K\") sys.stdout.flush() return True except sr.UnknownValueError: continue except sr.RequestError: print \"RequestError: The requested transmission failed.\" raise return False"}
{"text_id": "9602", "text": "docstring: def onselection(self, widget): self._selected_key = None for k in self.children: if self.children[k] == widget: self._selected_key = k if (self._selected_item is not None) and self._selectable: self._selected_item.attributes['selected'] = False self._selected_item = self.children[self._selected_key] if self._selectable: self._selected_item.attributes['selected'] = True break return self.eventManager.propagate(self.EVENT_ONSELECTION, (self._selected_key,))"}
{"text_id": "9603", "text": "docstring: def constraint_set_boundary(self): x_box1_pos = np.array([ self.box1_x_y_length[0] - self.box1_x_y_length[-1]/2.0, self.box1_x_y_length[0] - self.box1_x_y_length[-1]/2.0, self.box1_x_y_length[0] + self.box1_x_y_length[-1]/2.0, self.box1_x_y_length[0] + self.box1_x_y_length[-1]/2.0, self.box1_x_y_length[0] - self.box1_x_y_length[-1]/2.0]) x_box2_pos = np.array([ self.box2_x_y_length[0] - self.box2_x_y_length[-1]/2.0, self.box2_x_y_length[0] - self.box2_x_y_length[-1]/2.0, self.box2_x_y_length[0] + self.box2_x_y_length[-1]/2.0, self.box2_x_y_length[0] + self.box2_x_y_length[-1]/2.0, self.box2_x_y_length[0] - self.box2_x_y_length[-1]/2.0]) x_box3_pos = np.array([ self.box3_x_y_length[0] - self.box3_x_y_length[-1]/2.0, self.box3_x_y_length[0] - self.box3_x_y_length[-1]/2.0, self.box3_x_y_length[0] + self.box3_x_y_length[-1]/2.0, self.box3_x_y_length[0] + self.box3_x_y_length[-1]/2.0, self.box3_x_y_length[0] - self.box3_x_y_length[-1]/2.0]) y_box1_pos = np.array([ self.box1_x_y_length[1] - self.box1_x_y_length[-1]/2.0, self.box1_x_y_length[1] + self.box1_x_y_length[-1]/2.0, self.box1_x_y_length[1] + self.box1_x_y_length[-1]/2.0, self.box1_x_y_length[1] - self.box1_x_y_length[-1]/2.0, self.box1_x_y_length[1] - self.box1_x_y_length[-1]/2.0]) y_box2_pos = np.array([ self.box2_x_y_length[1] - self.box2_x_y_length[-1]/2.0, self.box2_x_y_length[1] + self.box2_x_y_length[-1]/2.0, self.box2_x_y_length[1] + self.box2_x_y_length[-1]/2.0, self.box2_x_y_length[1] - self.box2_x_y_length[-1]/2.0, self.box2_x_y_length[1] - self.box2_x_y_length[-1]/2.0]) y_box3_pos = np.array([ self.box3_x_y_length[1] - self.box3_x_y_length[-1]/2.0, self.box3_x_y_length[1] + self.box3_x_y_length[-1]/2.0, self.box3_x_y_length[1] + self.box3_x_y_length[-1]/2.0, self.box3_x_y_length[1] - self.box3_x_y_length[-1]/2.0, self.box3_x_y_length[1] - self.box3_x_y_length[-1]/2.0]) return (x_box1_pos, x_box2_pos, x_box3_pos, y_box1_pos, y_box2_pos, y_box3_pos)"}
{"text_id": "9604", "text": "docstring: def compare_training_plot(df: DataFrame): _start_index_from_one(df) fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12.8, 4.8)) for i in range(2): sns.lineplot(data=df[COMPARE_TRAINING_COLUMNS[i]], markers=['o', 'o'], ax=axes[i]) axes[i].set(title=f\"{_LABELS['train_validation'][i]} dataset\", xlabel='Epoch', ylabel='Accuracy') axes[i].legend(title='Model', labels=['Baseline', 'Fine-tuned']) fig.show()"}
{"text_id": "9605", "text": "docstring: def _get_backend_name(self, frontend): if frontend == 'default': defaults = self.config.get_section('default-targets', default={}) key = '%s+%s' % (self.declaration.configuration, self.declaration.tool) if key not in defaults: key = 'default+%s' % (self.declaration.tool,) try: frontend = defaults[key] except KeyError: raise LookupError(\"No default target declared.\") mapping = self.config.get_section(self.name, fallback='default') try: return mapping[frontend] except KeyError: return mapping['*']"}
{"text_id": "9606", "text": "docstring: def obtain_rules_keep_reset(df_anomalies_no_sub, df_anomalies_yes_sub, X_train, sc, n_vertex_numerical, numerical_cols, categorical_cols, clustering_algorithm, use_inverse): n = 0 max_iters = MAX_ITERS df_anomalies_no_sub.drop_duplicates(inplace=True) df_anomalies_yes_sub.drop_duplicates(inplace=True) if len(df_anomalies_no_sub)<2: df_anomalies_no_sub = df_anomalies_no_sub.append(df_anomalies_no_sub) df_anomalies_no_sub = df_anomalies_no_sub.reset_index(drop=True) if len(df_anomalies_yes_sub)<2: df_anomalies_yes_sub = df_anomalies_yes_sub.append(df_anomalies_yes_sub) df_anomalies_yes_sub = df_anomalies_yes_sub.reset_index(drop=True) df_final = [] df_datapoints_used = df_anomalies_no_sub.copy() j = 0 while len(df_datapoints_used) != 0: n += 1 j += 1 if j > max_iters: print(\"Max iters {max_iters} reached! Finishing process...\".format(max_iters=max_iters)) break df_bounds = [] print(\"\") print(\"*\" * 100) print(\"Iteration {0} | n\u00ba clusters used {1}\".format(j, n)) print(\"Remaining datapoints {0}/{1}\".format(len(df_datapoints_used), len(df_anomalies_no_sub))) try: dict_vectors_bound_all = obtain_vertices( df_datapoints_used, X_train, sc, n_vertex_numerical, numerical_cols, categorical_cols, clustering_algorithm, n_clusters=n) except ValueError: print(\"Too many clusters for the remaining datapoints -- ending process\") print(n, len(df_datapoints_used)) dict_vectors_bound_all = obtain_vertices( df_datapoints_used, X_train, sc, n_vertex_numerical, numerical_cols, categorical_cols, clustering_algorithm, n_clusters=n-3) df_datapoints_used = pd.DataFrame() for key, value in dict_vectors_bound_all.items(): vectors_bound_all = value[0].copy() n_points_cluster = len(value[2]) if vectors_bound_all.empty: continue if vectors_bound_all['distances'].iloc[0] == None: fake_hyper_limits = obtain_limits(vectors_bound_all) df_anomalies_yes_sub[\"outside_hcube\"] = df_anomalies_yes_sub.apply( lambda x: function_check(x, fake_hyper_limits, numerical_cols), axis=1) list_check = list( df_anomalies_yes_sub[\"outside_hcube\"].unique()) n_inside = len(df_anomalies_yes_sub[df_anomalies_yes_sub[ \"outside_hcube\"] == False]) df_bounds.append(fake_hyper_limits) if False in list_check: print(\"Less points than vertices...\") print(\"Points cluster: {0} | N\u00ba Vertices: {1}\".format( n_points_cluster, n_vertex_numerical)) if n == len(df_datapoints_used) -1: df_datapoints_used = df_datapoints_used[~df_datapoints_used.index.isin(value[2].index)] df_final.append(fake_hyper_limits) n = 0 else: continue else: df_datapoints_used = df_datapoints_used[~df_datapoints_used.index.isin(value[2].index)] df_final.append(fake_hyper_limits) else: limits = obtain_limits(vectors_bound_all) df_bounds.append(limits) df_anomalies_yes_sub[ \"outside_hcube\"] = df_anomalies_yes_sub.apply( lambda x: function_check(x, limits, numerical_cols), axis=1) n_inside = len(df_anomalies_yes_sub[df_anomalies_yes_sub[ \"outside_hcube\"] == False]) list_check = list( df_anomalies_yes_sub[\"outside_hcube\"].unique()) if False in list_check: print(\"Anomalies inside hypercube!. Key: \", key, \"|list_check: \", list_check, \"|n_anomalies: \", n_inside) continue else: df_datapoints_used = df_datapoints_used[~df_datapoints_used.index.isin(value[2].index)] df_final.append(limits) n = 0 return df_final"}
{"text_id": "9607", "text": "docstring: def null_coercer_wrapper(coercer: Callable) -> Callable: async def wrapper( parent_node: Union[ \"VariableDefinitionNode\", \"InputValueDefinitionNode\" ], node: \"Node\", value: Any, ctx: Optional[Any], **kwargs, ) -> \"CoercionResult\": if value is None: return CoercionResult(value=None) return await coercer(parent_node, node, value, ctx, **kwargs) return wrapper"}
{"text_id": "9608", "text": "docstring: def button_release_callback(self, event): if not self.showverts: return if event.button != 1: return if self._ind is not None: self.do_release() self._ind = None"}
{"text_id": "9609", "text": "docstring: def filter_pdf_files(filepaths): return [x for x in filepaths if x.endswith('.pdf')]"}
{"text_id": "9610", "text": "docstring: def materialize_subnetwork_reports(self, sess, iteration_number, subnetwork_reports, included_subnetwork_names): metric_update_ops = [] for subnetwork_report in subnetwork_reports.values(): for metric_tuple in subnetwork_report.metrics.values(): metric_update_ops.append(tf_compat.metric_op(metric_tuple)[1]) tensors_to_materialize = {} for name, subnetwork_report in subnetwork_reports.items(): metrics = { metric_key: tf_compat.metric_op(metric_tuple)[0] for metric_key, metric_tuple in subnetwork_report.metrics.items() } tensors_to_materialize[name] = { \"attributes\": subnetwork_report.attributes, \"metrics\": metrics } if self.steps is None: logging_frequency = 1000 elif self.steps < 10: logging_frequency = 1 else: logging_frequency = math.floor(self.steps / 10.) steps_completed = 0 while True: if self.steps is not None and steps_completed == self.steps: break try: steps_completed += 1 if (steps_completed % logging_frequency == 0 or self.steps == steps_completed): logging.info(\"Report materialization [%d/%s]\", steps_completed, self.steps or \"??\") sess.run(metric_update_ops) except tf.errors.OutOfRangeError: logging.info(\"Encountered end of input during report materialization\") break materialized_tensors_dict = sess.run(tensors_to_materialize) logging.info(\"Materialized subnetwork_reports.\") materialized_reports = [] for name, materialized_tensors in materialized_tensors_dict.items(): attributes = { key: value.item() if hasattr(value, \"item\") else value for key, value in materialized_tensors[\"attributes\"].items() } metrics = { key: value.item() if hasattr(value, \"item\") else value for key, value in materialized_tensors[\"metrics\"].items() } materialized_reports.append( subnetwork.MaterializedReport( iteration_number=iteration_number, name=name, hparams=subnetwork_reports[name].hparams, attributes=attributes, metrics=metrics, included_in_final_ensemble=(name in included_subnetwork_names))) return materialized_reports"}
{"text_id": "9611", "text": "docstring: def GetPDB(pdbidlist=[]): for i in pdbidlist: templist = [] templist.append(i) pdbDownload(templist) return True"}
{"text_id": "9612", "text": "docstring: def forceDGEval(self, mesh): relatives = pm.listRelatives(mesh,c=1) relatives.append(mesh) for m in relatives: conns = pm.listConnections(m, d=0,s=1,p=1) if not len(conns): continue for con in conns: revs = pm.listConnections(con, d=1,s=0,p=1) for rev in revs: pm.getAttr(rev, sl=1)"}
{"text_id": "9613", "text": "docstring: def function_from_method_array( method: 'Callable[..., PythonValue]' ) -> 'Callable[..., PythonValue]': def fun(arr: PythonValue, *args: PythonValue, pos: Optional[Pos]) -> PythonValue: if arr.is_top(): newinput = NdArray.top() absval = arr.val assert isinstance(absval, AbstractValue) newinput = array_from_AbstractValue(absval, pos) if newinput is None: newinput = NdArray.top() if len(args) == 0: assert len(signature(method).parameters) == 1 return method(newinput) else: assert len(signature(method).parameters) == len(args) + 2 return method(newinput, *args, pos) return fun"}
{"text_id": "9614", "text": "docstring: def log_settings(log_name, settings): with open_file(get_log_folder(log_name) + '/settings.txt', 'a') as f: for key, value in vars(settings).items(): f.write(key + ' ' + str(value) + '\\n')"}
{"text_id": "9615", "text": "docstring: def aggregate_rule_groups(self: object, body: list = None, **kwargs) -> dict: if not body: body = [aggregate_payload(submitted_keywords=kwargs)] return process_service_request( calling_object=self, endpoints=Endpoints, operation_id=\"aggregate_rule_groups\", body=body )"}
{"text_id": "9616", "text": "docstring: def address(self, address): self.ble_driver.ble_gap_addr_set(address)"}
{"text_id": "9617", "text": "docstring: def contract_gaslib_instance(nodes, edges, demand, aux_elements, contract_aux_edges, min_beta, debug=False): contracted_nodes = dict() if contract_aux_edges: for ae in aux_elements: v, w = (aux_elements[ae]['from'], aux_elements[ae]['to']) while v in contracted_nodes and not v == contracted_nodes[v]: v = contracted_nodes[v] if w in contracted_nodes: if debug: print(w, \"was already contracted\", w, '->', v) u = contracted_nodes[w] for x in contracted_nodes: if contracted_nodes[x] == u: if debug: print(\"Updating \", x, ':', x, '->', u, ' updated to ', x, '->', v) contracted_nodes[x] = v if debug: print(\"Contracting also\", u, '->', v) contracted_nodes[u] = v for x in contracted_nodes: if contracted_nodes[x] == w: if debug: print(\"! Updating \", x, ':', x, '->', w, ' updated to ', x, '->', v) contracted_nodes[x] = v contracted_nodes[w] = v else: for ae in aux_elements: aux_elements[ae]['beta'] = min_beta * 1e-10 edges.update(aux_elements) if debug: print(\"== CONTRACTED NODES ==\") for v in contracted_nodes: print(\"Contracting node\", v, \"into node\", contracted_nodes[v]) for w in contracted_nodes: v = contracted_nodes[w] if v != w: del nodes[w] if w in demand: if v in demand: demand[w]['b'] += demand[v]['b'] if debug: print(\"Contracting two entries/exits\", v, 'and', w, \"\\nNew b \", demand[w]['b']) demand[v] = demand[w] del demand[w] for eid in edges: e = edges[eid] if e['from'] in contracted_nodes: e['from'] = contracted_nodes[e['from']] if e['to'] in contracted_nodes: e['to'] = contracted_nodes[e['to']]"}
{"text_id": "9618", "text": "docstring: def np_dtype_is_homogeneous(A): if not is_sa(A): return True dtype = A.dtype first_dtype = dtype[0] return all(dtype[i] == first_dtype for i in xrange(len(dtype)))"}
{"text_id": "9619", "text": "docstring: def method1(n: int, m: int) -> int: def gcd(n: int, m: int) -> int: while m: n, m = m, n % m return n return abs((n * m) // gcd(n, m))"}
{"text_id": "9620", "text": "docstring: def _tree_to_labels( X, single_linkage_tree, min_cluster_size=10, cluster_selection_method=\"eom\", allow_single_cluster=False, match_reference_implementation=False, cluster_selection_epsilon=0.0, max_cluster_size=0, ): condensed_tree = condense_tree(single_linkage_tree, min_cluster_size) stability_dict = compute_stability(condensed_tree) labels, probabilities, stabilities = get_clusters( condensed_tree, stability_dict, cluster_selection_method, allow_single_cluster, match_reference_implementation, cluster_selection_epsilon, max_cluster_size, ) return (labels, probabilities, stabilities, condensed_tree, single_linkage_tree)"}
{"text_id": "9621", "text": "docstring: def list(self, resource_group_name): if resource_group_name is None: raise ValueError('resource_group_name cannot be None.') url = '' url = url + '/subscriptions/' if self.client.credentials.subscription_id is not None: url = url + quote(self.client.credentials.subscription_id) url = url + '/resourceGroups/' url = url + quote(resource_group_name) url = url + '/providers/' url = url + 'Microsoft.Network' url = url + '/localNetworkGateways' query_parameters = [] query_parameters.append('api-version=2015-05-01-preview') if len(query_parameters) > 0: url = url + '?' + '&'.join(query_parameters) base_url = self.client.base_uri if base_url[len(base_url) - 1] == '/': base_url = base_url[0 : len(base_url) - 1] if url[0] == '/': url = url[1 : ] url = base_url + '/' + url url = url.replace(' ', '%20') http_request = Request() http_request.url = url http_request.method = 'GET' response = self.client.send_request(http_request) body = response.content status_code = response.status_code if status_code != 200: error = AzureHttpError(body, response.status_code) raise error result = None if status_code == 200: response_content = body result = LocalNetworkGatewayListResponse(local_network_gateways=[]) response_doc = None if response_content: response_doc = json.loads(response_content.decode()) if response_doc is not None: value_array = response_doc.get('value', None) if value_array is not None: for value_value in value_array: local_network_gateway_json_format_instance = LocalNetworkGateway(tags={}) result.local_network_gateways.append(local_network_gateway_json_format_instance) properties_value = value_value.get('properties', None) if properties_value is not None: gateway_ip_address_value = properties_value.get('gatewayIpAddress', None) if gateway_ip_address_value is not None: gateway_ip_address_instance = gateway_ip_address_value local_network_gateway_json_format_instance.gateway_ip_address = gateway_ip_address_instance local_network_site_address_space_value = properties_value.get('localNetworkSiteAddressSpace', None) if local_network_site_address_space_value is not None: local_network_site_address_space_instance = AddressSpace(address_prefixes=[]) local_network_gateway_json_format_instance.local_network_site_address_space = local_network_site_address_space_instance address_prefixes_array = local_network_site_address_space_value.get('addressPrefixes', None) if address_prefixes_array is not None: for address_prefixes_value in address_prefixes_array: local_network_site_address_space_instance.address_prefixes.append(address_prefixes_value) provisioning_state_value = properties_value.get('provisioningState', None) if provisioning_state_value is not None: provisioning_state_instance = provisioning_state_value local_network_gateway_json_format_instance.provisioning_state = provisioning_state_instance etag_value = value_value.get('etag', None) if etag_value is not None: etag_instance = etag_value local_network_gateway_json_format_instance.etag = etag_instance id_value = value_value.get('id', None) if id_value is not None: id_instance = id_value local_network_gateway_json_format_instance.id = id_instance name_value = value_value.get('name', None) if name_value is not None: name_instance = name_value local_network_gateway_json_format_instance.name = name_instance type_value = value_value.get('type', None) if type_value is not None: type_instance = type_value local_network_gateway_json_format_instance.type = type_instance location_value = value_value.get('location', None) if location_value is not None: location_instance = location_value local_network_gateway_json_format_instance.location = location_instance tags_sequence_element = value_value.get('tags', None) if tags_sequence_element is not None: for property in tags_sequence_element: tags_key = property tags_value = tags_sequence_element[property] local_network_gateway_json_format_instance.tags[tags_key] = tags_value next_link_value = response_doc.get('nextLink', None) if next_link_value is not None: next_link_instance = next_link_value result.next_link = next_link_instance result.status_code = status_code result.request_id = response.headers.get('x-ms-request-id') return result"}
{"text_id": "9622", "text": "docstring: def _dump(f, mesh, e_color=None): dae = mesh_to_collada(mesh, e_color=e_color) dae.write(f.name)"}
{"text_id": "9623", "text": "docstring: def open_csv_as_sa(fin, delimiter=',', header=True, col_names=None, verbose=True, parse_datetimes=[]): df = pd.read_csv( fin, sep=delimiter, header=0 if header else None, names=col_names, index_col=False, prefix='f') df.fillna( inplace=True, value={col_name : '' for col_name, dtype_desc in df.dtypes.iteritems() if dtype_desc == np.dtype('O')}) if parse_datetimes: fix_pandas_datetimes(df, parse_datetimes) sa = df.to_records(index=False) return sa"}
{"text_id": "9624", "text": "docstring: def add_control_endpoint(self): control_endpoint = USBControlEndpoint(utmi=self.utmi) self.add_endpoint(control_endpoint) return control_endpoint"}
{"text_id": "9625", "text": "docstring: def side_chain_or_backbone(self): if (self.atomname.strip() == \"CA\" or self.atomname.strip() == \"C\" or self.atomname.strip() == \"O\" or self.atomname.strip() == \"N\"): return \"BACKBONE\" else: return \"SIDECHAIN\""}
{"text_id": "9626", "text": "docstring: def train_faster_rcnn( path_config_yaml: str, dataset_train: DatasetBase, path_weights: str = \"\", config_overwrite: Optional[List[str]] = None, log_dir: str = \"train_log/frcnn\", build_train_config: Optional[Sequence[str]] = None, dataset_val: Optional[DatasetBase] = None, build_val_config: Optional[Sequence[str]] = None, metric_name: Optional[str] = None, metric: Optional[Type[MetricBase]] = None, pipeline_component_name: Optional[str] = None, ) -> None: assert disable_tfv2() build_train_dict: Dict[str, str] = {} if build_train_config is not None: build_train_dict = string_to_dict(\",\".join(build_train_config)) if \"split\" not in build_train_dict: build_train_dict[\"split\"] = \"train\" build_val_dict: Dict[str, str] = {} if build_val_config is not None: build_val_dict = string_to_dict(\",\".join(build_val_config)) if \"split\" not in build_val_dict: build_val_dict[\"split\"] = \"val\" config_overwrite = [] if config_overwrite is None else config_overwrite log_dir = \"TRAIN.LOG_DIR=\" + log_dir config_overwrite.append(log_dir) config = set_config_by_yaml(path_config_yaml) if config_overwrite: config.update_args(config_overwrite) categories = dataset_train.dataflow.categories.get_categories(filtered=True) model_frcnn_config(config, categories, False) model = ResNetFPNModel(config=config) warmup_schedule, lr_schedule, step_number = train_frcnn_config(config) train_dataflow = get_train_dataflow(dataset_train, config, True, **build_train_dict) try: size = train_dataflow.size() total_passes = config.TRAIN.LR_SCHEDULE[-1] * 8 / size logger.info(\"Total passes of the training set is: %i\", total_passes) except NotImplementedError: logger.info(\"Cannot evaluate size of dataflow and total passes\") callbacks = [ PeriodicCallback( ModelSaver(max_to_keep=10, keep_checkpoint_every_n_hours=1, checkpoint_dir=config.TRAIN.LOG_DIR), every_k_epochs=config.TRAIN.CHECKPOINT_PERIOD, ), ScheduledHyperParamSetter(\"learning_rate\", warmup_schedule, interp=\"linear\", step_based=True), ScheduledHyperParamSetter(\"learning_rate\", lr_schedule), GPUMemoryTracker(), HostMemoryTracker(), ThroughputTracker(samples_per_step=config.TRAIN.NUM_GPUS), EstimatedTimeLeft(median=True), SessionRunTimeout(60000), GPUUtilizationTracker(), ] if ( config.TRAIN.EVAL_PERIOD > 0 and dataset_val is not None and (metric_name is not None or metric is not None) and pipeline_component_name is not None ): if metric_name is not None: metric = metric_registry.get(metric_name) detector = TPFrcnnDetector( path_config_yaml, path_weights, dataset_val.dataflow.categories.get_categories(filtered=True), config_overwrite, True, ) pipeline_component_cls = pipeline_component_registry.get(pipeline_component_name) pipeline_component = pipeline_component_cls(detector) assert isinstance(pipeline_component, PredictorPipelineComponent) category_names = list(categories.values()) callbacks.extend( [ EvalCallback( dataset_val, category_names, dataset_val.dataflow.categories.cat_to_sub_cat, metric, pipeline_component, *model.get_inference_tensor_names(), **build_val_dict ) ] ) session_init = SmartInit(path_weights, ignore_mismatch=True) factor = 8.0 / config.TRAIN.NUM_GPUS train_cfg = TrainConfig( model=model, data=QueueInput(train_dataflow), callbacks=callbacks, steps_per_epoch=step_number, max_epoch=config.TRAIN.LR_SCHEDULE[-1] * factor // step_number, session_init=session_init, starting_epoch=config.TRAIN.STARTING_EPOCH, ) trainer = SyncMultiGPUTrainerReplicated(config.TRAIN.NUM_GPUS, average=False) launch_train_with_config(train_cfg, trainer)"}
{"text_id": "9627", "text": "docstring: def model1(X, y): model = Sequential() model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape=(160, 320, 3))) model.add(Cropping2D(cropping=((70, 25), (0, 0)))) model.add(Convolution2D(6, 5, 5)) model.add(Activation('relu')) model.add(MaxPooling2D()) model.add(Convolution2D(6, 5, 5)) model.add(Activation('relu')) model.add(MaxPooling2D()) model.add(Flatten()) model.add(Activation('relu')) model.add(Dense(120, activation='relu')) model.add(Dense(84, activation='relu')) model.add(Dense(1)) model.compile(loss='mse', optimizer='adam') model.fit(X, y, validation_split=0.2, shuffle=True, nb_epoch=5) model.save(\"model1.h5\")"}
{"text_id": "9628", "text": "docstring: def _ends_statement(self, line: str) -> bool: return self._backtick_count % 2 == 0 and ( (self._indention_level > 0 and line.rstrip() == '') or (self._indention_level == 0 and line.rstrip() != ''))"}
{"text_id": "9629", "text": "docstring: def data_cache_ready(self, whofrom: base.base_obj, data_in: base.MSGdata_Type): ec = self._dcache.exitcode errmsg = self._dcache.errmsg print(\"controller: got datacache ready {} {}\".format(ec, errmsg)) self.init_view()"}
{"text_id": "9630", "text": "docstring: def add(self, e: Experience): self.obs[self.ptr] = e.obs self.action[self.ptr] = e.action self.next_obs[self.ptr] = e.next_obs self.reward[self.ptr] = e.reward self.bcs[self.ptr] = e.bcs self.done[self.ptr] = e.done self.ptr = (self.ptr + 1) % self.capacity self.size = min(self.size + 1, self.capacity) self.additions += 1"}
{"text_id": "9631", "text": "docstring: def extend(self, values: Iterable['Document']) -> None: with self._client.batch(batch_size=50) as _b: for d in values: _b.add_data_object(**self._doc2weaviate_create_payload(d)) self._offset2ids.append(self._wmap(d.id)) self._update_offset2ids_meta()"}
{"text_id": "9632", "text": "docstring: def to_role(sub_roles: Sequence[SubRole]) -> Sequence[Role]: return [sub_role.role for sub_role in sub_roles]"}
{"text_id": "9633", "text": "docstring: def executeSoilFileAsSex(cls, useFile, soilFile, prequelFileName=None): def is_empty_soil(abs_soil_file): lines=readFileLines( file=abs_soil_file, origin=None) for line in lines: if re.match( '^ *(!|\\?)', line): return False return True def empty_soil_to_sex(abs_soil_file, sex_filename): lines=readFileLines( file=abs_soil_file, origin=None) out_lines=[] for (no, line) in enumerate(lines): out_lines.append('%05i:%s' % ( no, line)) writeFileLines(out_lines, sex_filename) def trace_and_merge_to_sex( prequel_file_name, abs_use_file, abs_soil_file, sex_filename ): if DEBUG>=3: print(('USE: executeSoilFileAsSex: Soil file: %s' % abs_soil_file)) displayFileContent(abs_soil_file) print('USE: executeSoilFileAsSex: executeSoilFileAsTrace') trace_filename = cls.executeSoilFileAsTrace( abs_use_file, abs_soil_file, prequelFileName=prequel_file_name) if DEBUG>=3: print(( 'USE: executeSoilFileAsSex: ' 'TRACE RESULT saved in %s' % trace_filename)) displayFileContent(trace_filename, prefix='USE: ') print('USE: executeSoilFileAsSex: now merging') from modelscript.tools.use.engine.merger import merge merge( abs_soil_file, trace_filename, prequelFileName=prequel_file_name) if DEBUG>=3: print(('USE: executeSoilFileAsSex: ' 'SEX FILE saved in %s' % sex_filename)) displayFileContent(sex_filename) return sex_filename if DEBUG>=2: print(('USE: '+' executeSoilFileAsSex '.center(80,'#'))) if prequelFileName is None: prequelFileName=soilFile abs_use_file=os.path.realpath(useFile) abs_soil_file=os.path.realpath(soilFile) sex_filename = Environment.getWorkerFileName( basicFileName=replaceExtension(prequelFileName, '.sex')) if is_empty_soil(abs_soil_file): empty_soil_to_sex( abs_soil_file=abs_soil_file, sex_filename=sex_filename) else: trace_and_merge_to_sex( prequel_file_name=prequelFileName, abs_use_file=abs_use_file, abs_soil_file=abs_soil_file, sex_filename=sex_filename) with open(sex_filename, 'rU') as f: cls.outAndErr=f.read() if DEBUG >= 2: print(('USE: ' + ' END executeSoilFileAsSex '.center(80, '#'))) return sex_filename"}
{"text_id": "9634", "text": "docstring: def scrape_books(url_list): for url in url_list: result = url_connection(url) x = result.find_all(\"h3\", {\"class\" : \"book-title\"}) for a in x: list_of_books.append(tuple((a.text).replace('\\n', '').rsplit('by ', 1) )) return list_of_books"}
{"text_id": "9635", "text": "docstring: def linear_acceleration(self): self._process_available_packets() try: return self._readings[BNO_REPORT_LINEAR_ACCELERATION] except KeyError: raise RuntimeError(\"No lin. accel report found, is it enabled?\") from None"}
{"text_id": "9636", "text": "docstring: def predict(self, state): options = list(self.transitions[state].keys()) total = sum(list(self.transitions[state].values())) roll = random.randint(0, total) result = None cumsum = 0 while cumsum < roll: result = options.pop() cumsum += self.transitions[state][result] return result"}
{"text_id": "9637", "text": "docstring: def pythonize_name(name: str) -> str: name = name.replace(\"--\", \"\").replace(\"-\", \"_\") if name in keyword.kwlist: name = name + \"_\" if name[0] in ['0','1','2','3','4','5','6','7','8','9']: name = \"_\" + name return name"}
{"text_id": "9638", "text": "docstring: def display_pngs(pngs: List[str], config: Smiles2PngConfig): if len(pngs) == 0: print(\"No png files produced\") return files = ' '.join(pngs) os.system(f\"eog {files}\") if not config.keep_png: for png in pngs: os.remove(png)"}
{"text_id": "9639", "text": "docstring: def os_file_name(self): path = self.file_name.replace(\"\\\\\", \"/\") return utility.joinpath(path.split(\"/\"))"}
{"text_id": "9640", "text": "docstring: def transition(directory, functional=(\"pbe\", {}), is_metal=False, optimize_initial=False): directory = os.path.abspath(directory) (initial_cathode, final_cathode) = find_transition_cathodes( directory) user_incar_settings = {\"ISIF\": 2} user_incar_settings.update(_load_functional(functional)) if \"magmom\" in initial_cathode.site_properties.keys(): user_incar_settings.update({\"ISPIN\": 2, \"MAGMOM\": True}) if is_metal: user_incar_settings.update({\"ISMEAR\": 2, \"SIGMA\": 0.2}) if optimize_initial: initial_optimization = BulkRelaxSet( structure=initial_cathode.as_ordered_structure(), potcar_functional=DFT_FUNCTIONAL, user_incar_settings=user_incar_settings ) initial_optimization.write_input(os.path.join(directory, \"initial\")) initial_cathode.to(\"json\", os.path.join(directory, \"initial\", \"initial_cathode.json\")) else: os.makedirs(os.path.join(directory, \"initial\"), exist_ok=True) initial_cathode.to(\"json\", os.path.join(directory, \"initial\", \"final_cathode.json\")) final_optimization = BulkRelaxSet( structure=final_cathode.as_ordered_structure(), potcar_functional=DFT_FUNCTIONAL, user_incar_settings=user_incar_settings ) final_optimization.write_input(os.path.join(directory, \"final\")) final_cathode.to(\"json\", os.path.join(directory, \"final\", \"initial_cathode.json\"))"}
{"text_id": "9641", "text": "docstring: def check_udev_rules(): ok = True udev_dir = Path(\"/etc/udev/rules.d/\") desired_rules = { 'dfu': {_udev_rule(\"03eb\", \"2ff4\"), _udev_rule(\"03eb\", \"2ffb\"), _udev_rule(\"03eb\", \"2ff0\")}, 'input_club': {_udev_rule(\"1c11\", \"b007\")}, 'stm32': {_udev_rule(\"1eaf\", \"0003\"), _udev_rule(\"0483\", \"df11\")}, 'bootloadhid': {_udev_rule(\"16c0\", \"05df\")}, 'caterina': { _udev_rule(\"2341\", \"0036\", 'ENV{ID_MM_DEVICE_IGNORE}=\"1\"'), _udev_rule(\"1b4f\", \"9205\", 'ENV{ID_MM_DEVICE_IGNORE}=\"1\"'), _udev_rule(\"1b4f\", \"9203\", 'ENV{ID_MM_DEVICE_IGNORE}=\"1\"'), _udev_rule(\"2a03\", \"0036\", 'ENV{ID_MM_DEVICE_IGNORE}=\"1\"') } } deprecated_rules = { 'dfu': {_deprecated_udev_rule(\"03eb\", \"2ff4\"), _deprecated_udev_rule(\"03eb\", \"2ffb\"), _deprecated_udev_rule(\"03eb\", \"2ff0\")}, 'input_club': {_deprecated_udev_rule(\"1c11\")}, 'stm32': {_deprecated_udev_rule(\"1eaf\", \"0003\"), _deprecated_udev_rule(\"0483\", \"df11\")}, 'bootloadhid': {_deprecated_udev_rule(\"16c0\", \"05df\")}, 'caterina': {'ATTRS{idVendor}==\"2a03\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"', 'ATTRS{idVendor}==\"2341\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"'} } if udev_dir.exists(): udev_rules = [rule_file for rule_file in udev_dir.glob('*.rules')] current_rules = set() for rule_file in udev_rules: for line in rule_file.read_text().split('\\n'): line = line.strip() if not line.startswith(\"#\") and len(line): current_rules.add(line) for bootloader, rules in desired_rules.items(): if bootloader == \"caterina\": if check_modem_manager(): ok = False cli.log.warn(\"{bg_yellow}Detected ModemManager without the necessary udev rules. Please either disable it or set the appropriate udev rules if you are using a Pro Micro.\") if not rules.issubset(current_rules): deprecated_rule = deprecated_rules.get(bootloader) if deprecated_rule and deprecated_rule.issubset(current_rules): cli.log.warn(\"{bg_yellow}Found old, deprecated udev rules for '%s' boards. The new rules on https://docs.qmk.fm/#/faq_build?id=linux-udev-rules offer better security with the same functionality.\", bootloader) else: cli.log.warn(\"{bg_yellow}Missing udev rules for '%s' boards. You'll need to use `sudo` in order to flash them.\", bootloader) return ok"}
{"text_id": "9642", "text": "docstring: def normalise(instance): if instance.isempty: return instance lower = instance.lower upper = instance.upper bounds = list(instance._bounds) if lower is not None and lower == upper and instance._bounds != \"[]\": return instance.__class__(empty=True) if lower is None: bounds[0] = \"(\" elif bounds[0] == \"(\" and type(lower) in OFFSET: lower += OFFSET[type(lower)] bounds[0] = \"[\" if upper is None: bounds[1] = \")\" elif bounds[1] == \"]\" and type(upper) in OFFSET: upper += OFFSET[type(upper)] bounds[1] = \")\" if lower is not None and lower == upper and bounds != [\"[\", \"]\"]: return instance.__class__(empty=True) return instance.__class__(lower, upper, \"\".join(bounds))"}
{"text_id": "9643", "text": "docstring: def to_grid_pos(x, y, maze_size): grid_width = 100 row = maze_size - y / grid_width - 0.5 col = x / grid_width - 0.5 return row, col"}
{"text_id": "9644", "text": "docstring: def perturb_volume(self, factor: float, affix_id: bool = True) -> 'MonoCut': assert self.has_recording, 'Cannot perturb volume on a MonoCut without Recording.' if self.has_features: logging.warning( 'Attempting to perturb volume on a MonoCut that references pre-computed features. ' 'The feature manifest will be detached, as we do not support feature-domain ' 'volume perturbation.' ) self.features = None recording_vp = self.recording.perturb_volume(factor=factor, affix_id=affix_id) supervisions_vp = [s.perturb_volume(factor=factor, affix_id=affix_id) for s in self.supervisions] return fastcopy( self, id=f'{self.id}_vp{factor}' if affix_id else self.id, recording=recording_vp, supervisions=supervisions_vp )"}
{"text_id": "9645", "text": "docstring: def show_visible(self): frame = self.interesting_frames[self.frame_num] if frame in self.labels[\"frames\"]: plate = self.labels[\"frames\"][frame] self.findChild(QtWidgets.QRadioButton, f\"radioButton_{plate}\").setChecked( True )"}
{"text_id": "9646", "text": "docstring: def eachmonth(startDate, day=1, endDate=None): if hasattr(startDate, 'time'): startDate = startDate.date() day = int(day) highzero = (day < 1) index = 0 while True: firstDate = sane_date(startDate.year, startDate.month + index, day, highzero) if firstDate >= startDate: break index += 1 startDate = firstDate while (endDate is None) or (startDate <= endDate): yield startDate startDate = sane_date(startDate.year, startDate.month + 1, day, highzero)"}
{"text_id": "9647", "text": "docstring: async def lsb(self, a): stype = type(a) await returnType((stype, True)) Zp = stype.field l = stype.bit_length k = self.options.sec_param f = stype.frac_length b = self.random_bit(stype) a, b = await self.gather(a, b) b >>= f r = self._random(Zp, 1 << (l + k - 1)).value c = await self.output(a + ((1<<l) + (r << 1) + b.value)) x = 1 - b if c.value & 1 else b x <<= f return x"}
{"text_id": "9648", "text": "docstring: def configure_executable_type(cls, cfg): return []"}
{"text_id": "9649", "text": "docstring: def hilbert_convert(data_linear): total_blocks = len(data_linear) m = 1 while (2**m) * 2 < total_blocks: m += 1 pixels = (2**m) * 2 width = int(math.sqrt(pixels)) height = width data_linear = stretch_array(data_linear, pixels) data = bytearray(pixels) for i, byte in zip(range(len(data_linear)), data_linear): x, y = hilbert_curve.d2xy(m, i) index = (y * width) + x data[index] = byte return data, width, height"}
{"text_id": "9650", "text": "docstring: def recognition_probe(self, allowed_keys = (K_o, K_n), begin_time = 0, finish_time = None, c = None, fps = 30, exit_keys = (K_ESCAPE,), files = ()): self.response = self.get_keypress( response_keys = allowed_keys, start_time = begin_time, end_time = finish_time, ticker = c, frame_rate = fps, quit_keys = exit_keys, files = files )"}
{"text_id": "9651", "text": "docstring: async def kick_job(self, job: JobOrID) -> None: await self._send_cmd(b'kick-job %d' % _to_id(job), b'KICKED')"}
{"text_id": "9652", "text": "docstring: def vflip(img: Tensor) -> Tensor: if not torch.jit.is_scripting() and not torch.jit.is_tracing(): _log_api_usage_once(vflip) if not isinstance(img, torch.Tensor): return F_pil.vflip(img) return F_t.vflip(img)"}
{"text_id": "9653", "text": "docstring: def destroy_session(self, sessionkey): self.options.store.destroy_session(sessionkey) return self"}
{"text_id": "9654", "text": "docstring: def reset_db_command(): basedir = os.path.abspath(os.path.dirname(__file__)) path = os.path.join(basedir, 'migrations') shutil.rmtree(path) path = os.path.join(basedir, 'app.db') os.remove(path) flask_migrate.init() flask_migrate.migrate() flask_migrate.upgrade() click.echo('Database full reset. done.')"}
{"text_id": "9655", "text": "docstring: def PostReply(self, tweet, tweetToReplyID): api.PostUpdate(tweet, in_reply_to_status_id=tweetToReplyID)"}
{"text_id": "9656", "text": "docstring: def _evaluator(idx_dct): idx_dct = augment_index_dict_with_hydrogen_keys( gra, idx_dct, break_ties=False, neg=True) def _parity(key): key1, key2 = key nkey1s = nkeys_dct[key1] - {key2} nkey2s = nkeys_dct[key2] - {key1} nmax1 = max(nkey1s, key=idx_dct.__getitem__) nmax2 = max(nkey2s, key=idx_dct.__getitem__) xyz1 = xyz_dct[key1] xyz2 = xyz_dct[key2] nxyz1 = xyz_dct[nmax1] nxyz2 = xyz_dct[nmax2] bnd1_vec = numpy.subtract(nxyz1, xyz1) bnd2_vec = numpy.subtract(nxyz2, xyz2) dot_val = numpy.vdot(bnd1_vec, bnd2_vec) assert dot_val != 0. par = dot_val < 0. return par return _parity"}
{"text_id": "9657", "text": "docstring: def open(self): self.close() self._log.info('open') self._closing = False self._thread = threading.Thread(name='view', target=self._run) self._thread.start() self._post_block('ping') return"}
{"text_id": "9658", "text": "docstring: def DeleteFileOrDirWithRegex(self, path: str, regex: str): if os.path.exists(path): filepath = os.path.join(path, regex) files = glob.glob(filepath) for file in files: try: if os.path.isfile(file): os.remove(file) elif os.path.isdir(file): shutil.rmtree(file) except Exception as e: sys.stderr.write(f'ERROR: DeleteFileOrDirWithRegex(): Failed to delete: {file}, error: {str(e)}\\n') raise e"}
{"text_id": "9659", "text": "docstring: def copy(self): copyOfMe = FreeformDataSet(circuit_indices=self.cirIndex) copyOfMe._info = _copy.deepcopy(self._info) return copyOfMe"}
{"text_id": "9660", "text": "docstring: def enable_reporting(self): self.reporting = True msg = bytearray([REPORT_DIGITAL + self.port_number, 1]) self.board.sp.write(msg) for pin in self.pins: if pin.mode == INPUT: pin.reporting = True"}
{"text_id": "9661", "text": "docstring: def find_by_number(cls,number): for contact in cls.contact_list: if contact.phone_number == number: return contact"}
{"text_id": "9662", "text": "docstring: def _repr_values(self): def getattr_better(obj, field): try: return getattr(obj, field) except AttributeError as e: try: return getattr(obj, '_' + field) except AttributeError: raise e return (getattr_better(self, attr) for attr in self._repr_attributes)"}
{"text_id": "9663", "text": "docstring: def load_fixtures(self, nodes=None, progress_callback=None, dry_run=False): if progress_callback and not callable(progress_callback): raise Exception(\"Callback should be callable\") plan = self.get_plan(nodes=nodes) try: with transaction.atomic(): self.load_plan(plan=plan, progress_callback=progress_callback) if dry_run: raise DryRun except DryRun: pass return len(plan)"}
{"text_id": "9664", "text": "docstring: def forceful_stop_thread(thread): if thread.isAlive(): logger.warning(\"Stopping thread %s forcefully\", thread.getName()) try: thread._Thread__stop() except Exception as e: logger.warning(\"Error stopping thread %s: %s\", thread.getName(), e) return not thread.isAlive()"}
{"text_id": "9665", "text": "docstring: async def ingredients(ctx, *, name=None): async with ctx.typing(): content, real_name = mkt.get_item_recipe(name, levels=5) if real_name is not None: content = '**Ingredients for {}**\\n'.format(real_name) + content content = content + '\\n\\nNote: bux prices listed here may not always be accurate due to transfers between alts/friends or other reasons' await ctx.send(content) recipe_found = True"}
{"text_id": "9666", "text": "docstring: def run_action(task, action): log.debug(\"Executing %s on %s for %s\" % (action, task.node, task)) try: task.node.submit_command(action) return True except node.Error, e: log.error(\"Failed to run %s on %s: %s\", action, task.node, e) stream = task.buffer_store.open(actioncommand.ActionCommand.STDERR) stream.write(\"Node run failure for %s: %s\" % (task.task_name, e)) task.notify(task.NOTIFY_FAILED)"}
{"text_id": "9667", "text": "docstring: def post(): input_json = request.get_json() decoded = decode_token() if not input_json: raise HTTPError(400, error=\"Expected JSON request\") if 'ecosystem' not in input_json: raise HTTPError(400, error=\"Expected ecosystem in the request\") if 'component' not in input_json: raise HTTPError(400, error=\"Expected component in the request\") if 'tags' not in input_json or not any(input_json.get('tags', [])): raise HTTPError(400, error=\"Expected some tags in the request\") status, _error = set_tags_to_component(input_json.get('ecosystem'), input_json.get('component'), input_json.get('tags'), decoded.get('email'), decoded.get('company')) if status: return {'status': 'success'}, 200 else: raise HTTPError(400, error=_error)"}
{"text_id": "9668", "text": "docstring: def load_dfk_session(request, pytestconfig): config = pytestconfig.getoption('config')[0] if config != 'local': spec = importlib.util.spec_from_file_location('', config) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) if DataFlowKernelLoader._dfk is not None: raise ValueError(\"DFK didn't start as None - there was a DFK from somewhere already\") dfk = parsl.load(module.config) yield if(parsl.dfk() != dfk): raise ValueError(\"DFK changed unexpectedly during test\") dfk.cleanup() parsl.clear() else: yield"}
{"text_id": "9669", "text": "docstring: def diamond_db_name(config): name = \"reference_proteomes\" parts = [\"diamond\", name] return \".\".join(parts)"}
{"text_id": "9670", "text": "docstring: def create(self, validated_data): validated_data.pop('category') response = create_card(**validated_data) print(response.status_code) return response.json()"}
{"text_id": "9671", "text": "docstring: def response_xml_or_json_list(request, collection, collection_string, item_string): def return_json(): return Response(json_converter.list_to_json(collection ), mimetype='application/json') def return_xml(): return Response(xml_converter.list_to_xml(collection, collection_string, item_string), mimetype='application/xml') def return_csv(): response = Response(csv_converter.list_to_csv(collection ), mimetype='text/csv', content_type='application/octet-stream') response.headers[\"Content-Disposition\"] = 'attachment; filename=\"' + collection_string + '\".csv' return response def return_jsonp(): function = request.args.get('jsonp') if request.args.get('jsonp') is not None else 'callback' response = function + '(' + json_converter.list_to_json(collection) + ');' return Response(response, mimetype='application/javascript') functions = { 'json': return_json, 'xml': return_xml, 'csv': return_csv, 'jsonp': return_jsonp } functions_accept = { 'application/json': return_json, 'application/xml': return_xml, 'text/csv': return_csv, 'application/javascript': return_jsonp } if request.args.get('format') in functions.keys(): return functions[request.args.get('format')]() else: return (functions_accept[request.headers.get('Accept')] if request.headers.get('Accept') in functions_accept.keys() else functions['json'])()"}
{"text_id": "9672", "text": "docstring: def saved_model_video(self, video_path: Text, output_video: Text, **kwargs): import cv2 driver = inference.ServingDriver( self.model_name, self.ckpt_path, enable_ema=self.enable_ema, use_xla=self.use_xla) driver.load(self.saved_model_dir) cap = cv2.VideoCapture(video_path) if not cap.isOpened(): print('Error opening input video: {}'.format(video_path)) out_ptr = None if output_video: frame_width, frame_height = int(cap.get(3)), int(cap.get(4)) out_ptr = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc( 'm', 'p', '4', 'v'), 25, (frame_width, frame_height)) while cap.isOpened(): ret, frame = cap.read() if not ret: break raw_frames = [np.array(frame)] detections_bs = driver.serve_images(raw_frames) new_frame = driver.visualize(raw_frames[0], detections_bs[0], **kwargs) if out_ptr: out_ptr.write(new_frame) else: cv2.imshow('Frame', new_frame) if cv2.waitKey(1) & 0xFF == ord('q'): break"}
{"text_id": "9673", "text": "docstring: def feature_count(self) -> Dict[str, int]: counter = { \"aromatic ring\": 0, \"hydrophobicity\": 0, \"hb acceptor\": 0, \"hb donor\": 0, \"positive charge\": 0, \"negative charge\": 0, } for element in self._pharmacophoric_points: counter[element.feature_name] += 1 return counter"}
{"text_id": "9674", "text": "docstring: def startsWith(self, prefix: str) -> bool: curr = self.root for c in prefix: if not curr.getChild(c): return False curr = curr.getChild(c) return True"}
{"text_id": "9675", "text": "docstring: def _return_rest_body(self, response): if _is_json(response.headers.get('content-type', None)): return response.json() return response.text"}
{"text_id": "9676", "text": "docstring: def rename_var(self, var, var_type, args): if var in self.vars_seen: new_var = self.prefix + str(self.vars_seen.index(var)) else: new_var = self.prefix + str(len(self.vars_seen)) self.type_vars[new_var] = var_type self.vars_seen.append(var) self.var_map[new_var] = var return new_var"}
{"text_id": "9677", "text": "docstring: def likelihood(param, D): dA_Cp = D['At_Cp']-feval_Cp_plt(param, D['Tt_Cp']) dA_H = D['At_H']-feval_H_plt(param, D['Tt_H']) nhyp_H = len(D['name_list_H']) hyp_H = param[-nhyp_H:] nhyp_Cp = len(D['name_list_Cp']) hyp_Cp = param[-(nhyp_H+nhyp_Cp):-nhyp_H] if np.any(hyp_Cp <= 0) or np.any(hyp_H <= 0): return -np.inf hypvec_Cp = np.zeros(D['Tt_Cp'].shape) for ii in range(nhyp_Cp): hypvec_Cp[D['It_Cp'] == ii] = hyp_Cp[ii] hypvec_H = np.zeros(D['Tt_H'].shape) for ii in range(nhyp_H): hypvec_H[D['It_H'] == ii] = hyp_H[ii] dof = 2+1e-6 prob_Cp = ss.t.logpdf(dA_Cp, dof, loc=0, scale=D['Et_Cp']/hypvec_Cp).sum() prob_H = ss.t.logpdf(dA_H, dof, loc=0, scale=D['Et_H']/hypvec_H).sum() prob = prob_Cp + prob_H if np.isnan(prob): return -np.inf return prob"}
{"text_id": "9678", "text": "docstring: def reporterFailed(self, failure, net_test): pass"}
{"text_id": "9679", "text": "docstring: def produce(topic, bootstrap_brokers, json_value, verbose): logging.basicConfig( format='[%(asctime)s] %(name)s.%(levelname)s %(threadName)s %(message)s', level=logging.DEBUG if verbose else logging.INFO ) logging.captureWarnings(True) bootstrap_brokers = bootstrap_brokers.split(',') producer = KafkaProducer( bootstrap_servers=bootstrap_brokers, value_serializer=value_serializer, key_serializer=key_serializer, acks='all' ) try: for line in sys.stdin: message = json.loads(line) value = message['value'] value_string = value if json_value: value_string = json.dumps(value, indent=True, ensure_ascii=False, sort_keys=True) args = { 'topic': topic, 'value': value_string } if 'key' in message: args['key'] = message['key'] if 'partition' in message: args['partition'] = message['partition'] producer.send(**args) except KeyboardInterrupt: pass producer.flush() producer.close()"}
{"text_id": "9680", "text": "docstring: def split_data(self, data, ratio=0.8, shuffle=True, seed=0): if shuffle: random.seed(seed) random.shuffle(data) size = int(len(data) * ratio) data_1 = data[:size] data_2 = data[size:] return data_1, data_2"}
{"text_id": "9681", "text": "docstring: def parse_arguments(sys_args): parser = argparse.ArgumentParser(description='') parser.add_argument('--iter_num', type=int, required=True, help='number of iterations') parser.add_argument('--pop_num', type=int, required=True, help='size of population (min 2)') parser.add_argument('--prob_cross', type=float, default=0.7, help='crossover probability') parser.add_argument('--prob_mutation', type=float, default=0.05, help='mutation probability') parser.add_argument('--aggregate', default=False, action='store_true', help='aggregate data groups') parser.add_argument('--adapt_function', type=str, default=\"silhouette\", choices=['silh', 'info', 'info_gain', \"silhouette\"], help='silhouette or information gain') parser.add_argument('--dist_measure', type=str, default=\"euclidean\", choices=['eucl', 'manh', 'cos', \"euclidean\", \"manhattan\", \"cosine\"], help='euclidean, manhattan, cosine') parser.add_argument('--repeat', type=int, default=1, help='repeat experiment n times and average results') parser.add_argument('--logdir', type=str, default=\"logs\", help='aggregate data groups') parser.add_argument('--data', type=str, default=\"train\", choices=['train', 'test'], help='aggregate data groups') parser.add_argument('--showdata', default=False, action='store_true', help='only show data to be used in experiment') args = parser.parse_args(sys_args) args.sys_args = sys_args if args.pop_num < 2: raise TypeError(\"'pop_num' can't be less than 2\") if args.dist_measure == 'eucl': args.dist_measure = \"euclidean\" elif args.dist_measure == 'manh': args.dist_measure = \"manhattan\" elif args.dist_measure == 'cos': args.dist_measure = \"cosine\" if args.adapt_function == 'silh': args.adapt_function = \"silhouette\" elif args.adapt_function == 'info': args.adapt_function = \"info_gain\" return args"}
{"text_id": "9682", "text": "docstring: def _unauthorized(self, *args, **kwargs): if self._unauthorized_callback: return self._unauthorized_callback(*args, **kwargs) if self.LOGIN_REQUIRED_ROUTE: _, resp = args self._api.redirect(resp, self.LOGIN_REQUIRED_ROUTE) return UNAUTHORIZED(*args, **kwargs)"}
{"text_id": "9683", "text": "docstring: def comp_desc(smiles, calculator): try: mol = Chem.MolFromSmiles(smiles, sanitize=False) Chem.SanitizeMol(mol) result = np.array(calculator.CalcDescriptors(mol)) if np.all(result == -666): return np.array([np.nan]*len(descriptors)) if np.all(np.isfinite(result)) == False: return np.array([np.nan]*len(descriptors)) return result except: return np.array([np.nan]*len(descriptors))"}
{"text_id": "9684", "text": "docstring: def from_credentials(cls, credentials: _Credentials.Credentials): return User(User.get_pv_fest_ids(credentials)[0], credentials)"}
{"text_id": "9685", "text": "docstring: def _check_resourceid(cls, resourceid: Union[str, None]) -> bool: if resourceid is None: return True else: return isinstance(resourceid, str)"}
{"text_id": "9686", "text": "docstring: def hist_wavedisp(waves, disps, dispbin=None, wavebin=None, scale=1.0, debug=False): if dispbin is None: dispbin = np.linspace(-3.0, 1.0, 1000) if wavebin is None: wavebin = [np.min(waves), np.max(waves)] lin_dispbin = np.power(10.0, dispbin) lin_disps = np.power(10.0, disps) nelem = np.zeros(dispbin.size-1, dtype=nb.types.uint64) for dd in range(dispbin.size-1): dispval = 0.5*(lin_dispbin[dd] + lin_dispbin[dd+1]) nelem[dd] = np.int(0.5 + scale*(wavebin[1]-wavebin[0])/dispval) nhistv = np.sum(nelem) hist_wd = np.zeros(nhistv, dtype=nb.types.uint64) cent_w = np.zeros(nhistv, dtype=nb.types.uint64) cent_d = np.zeros(nhistv, dtype=nb.types.uint64) cntr = 0 for dd in range(dispbin.size-1): wbin = np.linspace(wavebin[0], wavebin[1], nelem[dd]+1) wdsp = np.where((lin_disps > lin_dispbin[dd]) & (lin_disps <= lin_dispbin[dd+1])) if wdsp[0].size != 0: hist_wd[cntr:cntr+nelem[dd]], _ = np.histogram(waves[wdsp], bins=wbin) cent_d[cntr:cntr+nelem[dd]] = 0.5 * (lin_dispbin[dd] + lin_dispbin[dd + 1]) cent_w[cntr:cntr+nelem[dd]] = 0.5 * (wbin[1:] + wbin[:-1]) cntr += nelem[dd] return hist_wd, cent_w, cent_d"}
{"text_id": "9687", "text": "docstring: def cbui(request, settings=None): log.debug(\"CBUI Received: \\n%s\" % pprint.pformat(dict(request.GET), indent=10)) cart = _find_cart(request.GET) handler = AmazonIPN(cart) handler._update_with_cart_settings(cart_settings_kwargs={'request': request}) if not handler.verify_signature(request.GET.urlencode(), \"GET\", handler.settings[\"CBUI_RETURN_URL\"]): log.error(\"Validation of Amazon request failed!\") return HttpResponseRedirect(handler.settings.get(\"ERROR_RETURN_URL\", handler.settings.get(\"RETURN_URL\", \"/\"))) if request.GET[\"status\"] not in (\"SA\", \"SB\", \"SC\"): log.error(\"CBUI unsuccessful. Status code: %s\" % request.GET[\"status\"]) return HttpResponseRedirect(handler.settings.get(\"CANCEL_RETURN_URL\", handler.settings.get(\"RETURN_URL\", \"/\"))) if not cart: log.error(\"Unable to find cart.\") return HttpResponseRedirect(handler.settings.get(\"ERROR_RETURN_URL\", handler.settings.get(\"RETURN_URL\", \"/\"))) cart.bill_first_name = cart.bill_first_name or request.GET.get(\"billingName\", \"\") cart.ship_first_name = cart.ship_first_name or request.GET.get(\"addressName\", \"\") cart.bill_street1 = cart.bill_street1 or request.GET.get(\"addressLine1\", \"\") cart.ship_street1 = cart.ship_street1 or cart.bill_street1 cart.bill_street2 = cart.bill_street1 or request.GET.get(\"addressLine2\", \"\") cart.ship_street2 = cart.ship_street1 or cart.bill_street1 cart.bill_state = cart.bill_state or request.GET.get(\"state\", \"\") cart.ship_state = cart.ship_state or cart.bill_state cart.bill_postal_code = cart.bill_postal_code or request.GET.get(\"zip\", \"\") cart.ship_postal_code = cart.ship_postal_code or cart.bill_postal_code country = request.GET.get(\"country\", \"\").upper() cart.bill_country = cart.bill_country or COUNTRIES.get(country, \"\") cart.ship_country = cart.ship_country or cart.bill_country cart.bill_email = cart.bill_email = request.GET.get(\"buyerEmailAddress\", \"\"); cart.ship_email = cart.ship_email or cart.bill_email cart.save() recurring = cart.recurring_lineitems if len(recurring) > 0: handler.save_recurring_token(request.GET[\"tokenID\"]) if recurring[0].recurring_start is None: result = handler.make_pay_request(request.GET[\"tokenID\"]) if result == \"Success\": handler.begin_recurring() else: handler.begin_recurring() else: log.debug(\"Making pay request: %s\" % request.GET['tokenID']) result = handler.make_pay_request(request.GET[\"tokenID\"]) log.debug(\"Pay request result: %s\" % result) if 'RETURN_URL' in handler.settings: return HttpResponseRedirect(handler.settings['RETURN_URL']) return HttpResponseRedirect(\"/\")"}
{"text_id": "9688", "text": "docstring: def multiply(self, amount): self.x *= amount self.y *= amount self.z *= amount"}
{"text_id": "9689", "text": "docstring: def add_taxon_to_meta(meta, taxon_meta): LOGGER.info(\"Adding taxon metadata to assembly metadata\") ranks = [ \"species\", \"genus\", \"family\", \"order\", \"class\", \"phylum\", \"kingdom\", \"superkingdom\", ] for obj in taxon_meta[\"lineage\"]: if obj[\"taxon_rank\"] in ranks: meta[\"taxon\"].update({obj[\"taxon_rank\"]: obj[\"scientific_name\"]})"}
{"text_id": "9690", "text": "docstring: def on_finish_unloading(self, simulator, data): if self.verbose: print('{:02.0f}:{:02.0f} Train {} going from port {} to terminal {}'.format(simulator.time // 3600, (simulator.time % 3600) // 60, data[3], data[0], data[1])) ip = data[0] im = data[2] it = self.dispatch_to_terminal(simulator.time, ip, im) data[1] = it t = simulator.time + self.distance[ip, it] / self.train_speed[im] simulator.add_event(t, self.on_finish_unloaded_path, data)"}
{"text_id": "9691", "text": "docstring: async def create_or_update( self, scope: str, iot_sensor_name: str, **kwargs ) -> \"models.IotSensor\": cls = kwargs.pop('cls', None) error_map = { 401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError } error_map.update(kwargs.pop('error_map', {})) api_version = \"2020-08-06-preview\" accept = \"application/json\" url = self.create_or_update.metadata['url'] path_format_arguments = { 'scope': self._serialize.url(\"scope\", scope, 'str', skip_quote=True), 'iotSensorName': self._serialize.url(\"iot_sensor_name\", iot_sensor_name, 'str'), } url = self._client.format_url(url, **path_format_arguments) query_parameters = {} query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str') header_parameters = {} header_parameters['Accept'] = self._serialize.header(\"accept\", accept, 'str') request = self._client.put(url, query_parameters, header_parameters) pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs) response = pipeline_response.http_response if response.status_code not in [200, 201]: map_error(status_code=response.status_code, response=response, error_map=error_map) raise HttpResponseError(response=response, error_format=ARMErrorFormat) if response.status_code == 200: deserialized = self._deserialize('IotSensor', pipeline_response) if response.status_code == 201: deserialized = self._deserialize('IotSensor', pipeline_response) if cls: return cls(pipeline_response, deserialized, {}) return deserialized"}
{"text_id": "9692", "text": "docstring: def isnt_all_regs_values(list) : result = False p_reg = re.compile ('^v[0-9]+$') for i in list : if not(p_reg.match(i)) : result = True return result"}
{"text_id": "9693", "text": "docstring: def grabWindowPixmap(self): return QtGui.QPixmap.grabWidget(self.canvas)"}
{"text_id": "9694", "text": "docstring: def focal_length(W, theta): return W / 2 / np.tan(theta / 2)"}
{"text_id": "9695", "text": "docstring: def _get_tool_paths(repository_ctx, darwin, cc): return {k: _which(repository_ctx, k, \"/usr/bin/\" + k) for k in [ \"ld\", \"cpp\", \"dwp\", \"gcov\", \"nm\", \"objcopy\", \"objdump\", \"strip\", ]} + { \"gcc\": cc, \"ar\": \"/usr/bin/libtool\" if darwin else _which(repository_ctx, \"ar\", \"/usr/bin/ar\") }"}
{"text_id": "9696", "text": "docstring: async def async_setup_platform( hass: HomeAssistant, config: ConfigType, async_add_entities: AddEntitiesCallback, discovery_info: DiscoveryInfoType | None = None, ) -> None: entities = [] for device in hass.data[DOMAIN].xknx.devices: if isinstance(device, XknxBinarySensor): entities.append(KNXBinarySensor(device)) async_add_entities(entities)"}
{"text_id": "9697", "text": "docstring: def extract_family(family): family = [treat_family(x) for x in family] return family"}
{"text_id": "9698", "text": "docstring: def iterate_containers(self): for container_name in os.listdir(self.base_path): full_path = os.path.join(self.base_path, container_name) if not os.path.isdir(full_path): continue yield self._make_container(container_name)"}
{"text_id": "9699", "text": "docstring: def zero_normal_occlude(feature_indices, data, model, occlusion_type, pd_df, header): if occlusion_type == \"zero\": for key in feature_indices.keys(): cols = feature_indices[key][1:] occluded_data = data.copy() occluded_data[:, :, cols] = 0 predictions = model.predict(occluded_data, batch_size=batch_size, verbose=1) predictions = np.array(predictions)[:, 0] pd_df[key] = predictions elif occlusion_type == \"normal-value\": with open(\"./utils/resources/discretizer_config.json\", \"r\") as f: config = json.load(f) possible_values = config['possible_values'] normal_values = config['normal_values'] normalizer = Normalizer() normalizer.load_params(normalizer_state) for key in feature_indices.keys(): full_name = key.replace(\"_\", \" \") if feature_indices[key][0] == \"discrete\": if \"presence_\" in key: to_sub = 1 else: normal_value = normal_values[full_name] category_id = possible_values[full_name].index( normal_value) N_values = len(possible_values[full_name]) one_hot = np.zeros((N_values, )) one_hot[category_id] = 1 to_sub = one_hot elif feature_indices[key][0] == \"continuous\": index = header.index(full_name) to_sub = ((float(normal_values[full_name]) - normalizer._means[index]) / normalizer._stds[index]) cols = feature_indices[key][1:] occluded_data = data.copy() occluded_data[:, :, cols] = to_sub predictions = model.predict(occluded_data, batch_size=batch_size, verbose=1) predictions = np.array(predictions)[:, 0] pd_df[key] = predictions if not os.path.isdir(os.path.join(\"./output/\", occlusion_type)): os.mkdir(os.path.join(\"./output/\", occlusion_type)) pd_df.to_csv(os.path.join(\"./output/\", occlusion_type, \"result.csv\"), index=False) return pd_df"}
{"text_id": "9700", "text": "docstring: def game_objects(self): return self._game_objects"}
{"text_id": "9701", "text": "docstring: def handle(req): import io import sys from cadquery import cqgi from cadquery.occ_impl.exporters.json import JsonMesh text_trap = io.StringIO() sys.stderr = text_trap mesher = JsonMesh() cqModel = cqgi.parse(req) build_result = cqModel.build({}) if build_result.success: for result in build_result.results: tess = result.shape.val().tessellate(0.001) for v in tess[0]: mesher.addVertex(v.x, v.y, v.z) for ixs in tess[1]: mesher.addTriangleFace(*ixs) return mesher.toJson()"}
{"text_id": "9702", "text": "docstring: def handle(infiles, tables, user_input_path, **kwargs): msg = 'Starting {name}'.format(name=__name__) logging.info(msg) meshFileName = infiles['MPAS_mesh'] mappingFileName = infiles['MPAS_map'] timeSeriesFiles = infiles['MPASSI'] rhoi = 917.0 dsMesh = xarray.open_dataset(meshFileName, mask_and_scale=False) cellMask2D, _ = mpas.get_cell_masks(dsMesh) variableList = ['timeMonthly_avg_iceAreaCell', 'timeMonthly_avg_iceVolumeCell', 'xtime_startMonthly', 'xtime_endMonthly'] ds = xarray.Dataset() with mpas.open_mfdataset(timeSeriesFiles, variableList) as dsIn: ds[VAR_NAME] = rhoi*dsIn.timeMonthly_avg_iceVolumeCell ds['siconc'] = dsIn.timeMonthly_avg_iceAreaCell ds = mpas.add_time(ds, dsIn) ds.compute() ds = mpas.add_si_mask(ds, cellMask2D, ds.siconc) ds['cellMask'] = ds.siconc * ds.cellMask ds.compute() ds = mpas.remap(ds, mappingFileName) mpas.setup_cmor(VAR_NAME, tables, user_input_path, component='seaice') axes = [{'table_entry': 'time', 'units': ds.time.units}, {'table_entry': 'latitude', 'units': 'degrees_north', 'coord_vals': ds.lat.values, 'cell_bounds': ds.lat_bnds.values}, {'table_entry': 'longitude', 'units': 'degrees_east', 'coord_vals': ds.lon.values, 'cell_bounds': ds.lon_bnds.values}] try: mpas.write_cmor(axes, ds, VAR_NAME, VAR_UNITS) except Exception: return \"\" return VAR_NAME"}
{"text_id": "9703", "text": "docstring: def batch_shape_tensor(self, name=\"batch_shape_tensor\"): with self._name_scope(name): if self.batch_shape.is_fully_defined(): return ops.convert_to_tensor(self.batch_shape.as_list(), dtype=dtypes.int32, name=\"batch_shape\") return self._batch_shape_tensor()"}
{"text_id": "9704", "text": "docstring: def find_softhsm_lib(libname=softhsm_libname, searchpath=softhsm_searchpath): lib = os.environ.get('SOFTHSM_LIB') if lib: return lib for p in searchpath: lib = p.joinpath(libname) if lib.is_file(): return str(lib)"}
{"text_id": "9705", "text": "docstring: def _make_xselect_commands_apply_gti(infile, outfile, gtifile): import glob for oldfile in glob.glob(\"session1*\"): os.system(f\"rm {oldfile}\") xsel=open(\"xsel.xco\",\"w\") xsel.write(\"session1\\n\") xsel.write(\"read events \\n\") evdir=os.path.dirname(infile) xsel.write(f'{evdir} \\n ' ) evfile = os.path.basename(infile) xsel.write(f'{evfile} \\n ') xsel.write('yes \\n') xsel.write(f'filter time \\n') xsel.write('file \\n') xsel.write(f'{gtifile}\\n') xsel.write('extract events\\n') xsel.write(\"save events\\n\") xsel.write(\"%s \\n\" % outfile) xsel.write('n \\n') xsel.write('exit\\n') xsel.write('n \\n') xsel.close() return 'xsel.xco'"}
{"text_id": "9706", "text": "docstring: def handle_sid_removed_from_universe(self, sid): self.dividend_frame = self.dividend_frame[ self.dividend_frame.sid != sid ]"}
{"text_id": "9707", "text": "docstring: def _serialize_input( self, data: Union[ Dict, str, List, np.ndarray, pd.core.series.Series, pd.core.frame.DataFrame, ], ): try: return _serialize_input_helper(data) except: raise TypeError( \"The supported data types are Dict, str, list, \" \"numpy.ndarray, pd.core.series.Series, \" \"pd.core.frame.DataFrame. Please \" \"convert to the supported data types first. \" )"}
{"text_id": "9708", "text": "docstring: def ifup(device): cmd = 'sudo ip link set dev' if not device: raise netem_exceptions.CommandError('device') return r'{cmd} {device} up'.format(cmd=cmd, device=device)"}
{"text_id": "9709", "text": "docstring: def check_ecosystems_in_bucket(found_ecosystems, bucket_name): expected_ecosystems = set(ECOSYSTEMS) not_found_ecosystems = expected_ecosystems - set(found_ecosystems) if not_found_ecosystems: logging.error(\"the following ecosystem{s} can't we found in the '{b}' bucket: {e}\".format( s='s' if len(not_found_ecosystems) > 1 else '', e=not_found_ecosystems, b=bucket_name)) leftovers = set(found_ecosystems) - expected_ecosystems if leftovers: logging.error( \"the following unexpected object{s} were found in the '{b}' bucket: {o}\".format( s='s' if len(leftovers) > 1 else '', o=leftovers, b=bucket_name))"}
{"text_id": "9710", "text": "docstring: def binary_search_object_by_id(self, server_id, calltype=1): left, right = 0, len(self.server_objects) - 1 count = 0 for _ in self.server_objects: count += 1 middle = int((left + right) / 2) if int(self.server_objects[middle].id) < int(server_id): left = middle + 1 elif int(self.server_objects[middle].id) > int(server_id): right = middle - 1 else: if calltype == 1: return self.server_objects[middle] else: return middle try: return self.seq_search_object(server_id, calltype) except: raise Exception(\"No matching server\")"}
{"text_id": "9711", "text": "docstring: def solvevec(self, rhs, adjoint=False, name=\"solve\"): rhs = np.array(rhs) return self._solvevec(rhs, adjoint=adjoint)"}
{"text_id": "9712", "text": "docstring: def _normalizeLineEnds(text, desired=LINEEND): unlikely = '\\000\\001\\002\\003' text = text.replace('\\015\\012', unlikely) text = text.replace('\\015', unlikely) text = text.replace('\\012', unlikely) text = text.replace(unlikely, desired) return text"}
{"text_id": "9713", "text": "docstring: def train(self): self.mode = \"train\" for md in self.modules(): md.train()"}
{"text_id": "9714", "text": "docstring: def pg_connection_request_property(request): return d.sessionmaker()"}
{"text_id": "9715", "text": "docstring: def create_tf_categorical_feature_cols(categorical_col_list, vocab_dir='./diabetes_vocab/'): output_tf_list = [] for c in categorical_col_list: vocab_file_path = os.path.join(vocab_dir, c + \"_vocab.txt\") output_tf_list.append(tf_categorical_feature_column) return output_tf_list"}
{"text_id": "9716", "text": "docstring: def migrate(): global counter, enable body = request.get_json(force=True) server = body[\"server\"] response = requests.post(f\"{server}/api/admin/disable\").json() counter = response[\"counter\"] enable = True return jsonify(counter=counter)"}
{"text_id": "9717", "text": "docstring: def yield_last_file_if_it_exists() -> Iterator[ Tuple[str, List[List[int]], List[List[int]]] ]: nonlocal last_file_path, last_from_lines_list, last_to_lines_list if last_file_path is not None: yield (last_file_path, last_from_lines_list, last_to_lines_list) last_file_path = None last_from_lines_list = [] last_to_lines_list = []"}
{"text_id": "9718", "text": "docstring: def login(self, service, context): operation_args = {} connection = context['connection'] conection_information = CONNECTION_OPERATIONS[connection['wsdl_endpoint']] login_operation = conection_information['operation'] for login_key, login_value in conection_information[\"login_args\"].items(): arg_value = connection.get(login_key, None) if not arg_value: arg_value = context['kwargs_dict'].get(login_key, None) operation_args[login_value] = arg_value op_obj = service.__getitem__(login_operation) result = op_obj(**operation_args) session = result if not isinstance(session, str): session = result[conection_information['session']] return session"}
{"text_id": "9719", "text": "docstring: def has_permission(user, cmd): blacklisted = user in sms()['blacklist'] return not blacklisted and (user in _WHO_CAN['all'] or 'all' in _WHO_CAN[cmd] or user in _WHO_CAN[cmd])"}
{"text_id": "9720", "text": "docstring: def format_sql_table_column_declaration_str(headers, primary_keys, rows): column_types = get_column_types(headers, rows) column_name_type_pairs = ['{} {}'.format(headers[i], column_types[i]) for i in range(len(headers))] column_def = ', '.join(column_name_type_pairs) if len(primary_keys) != 0: column_def += ', PRIMARY KEY ({})'.format(', '.join(primary_keys)) return column_def"}
{"text_id": "9721", "text": "docstring: def from_dict(cls, _dict: Dict) -> 'ResourceLimits': args = {} if 'cpu' in _dict: args['cpu'] = _dict.get('cpu') if 'memory' in _dict: args['memory'] = _dict.get('memory') return cls(**args)"}
{"text_id": "9722", "text": "docstring: def __get_content_type(self, content): from pelican.contents import Article cls = type(content) if cls == Article: return \"article\" return \"page\""}
{"text_id": "9723", "text": "docstring: def add_pubkey(self, user_id, pubkey, _self=False, dis_enc=False): if _self: self._user2pubkey[user_id] = self.pubkey else: self._user2pubkey[user_id] = RSA.importKey(pubkey) if dis_enc: self._dis_enc.add(user_id) else: try: self._dis_enc.remove(user_id) except Exception as e: pass"}
{"text_id": "9724", "text": "docstring: def stationary_log_returns(pair_df): pair_df = pair_df.copy() pair_df[\"Real Close\"] = pair_df[\"Close\"] pair_df[\"Close\"] = np.log(pair_df[\"Close\"] / pair_df[\"Close\"].shift(1)) pair_df[\"EMA_10\"] = np.log(pair_df[\"EMA_10\"] / pair_df[\"EMA_10\"].shift(1)) pair_df[\"EMA_50\"] = np.log(pair_df[\"EMA_50\"] / pair_df[\"EMA_50\"].shift(1)) return pair_df"}
{"text_id": "9725", "text": "docstring: def scope_update_handler(handler: Callable[[Any, str, Any], None]): @locals_diff_tracer def differ(frame, updated, deleted): for key, val in updated.items(): handler(frame, key, val) return differ"}
{"text_id": "9726", "text": "docstring: def format_date(string: str, include_time: bool=False) -> str: try: date = datetime.fromisoformat(string.strip()) except ValueError: return string if include_time: return date.strftime(\"%A %d %B %Y %H:%M\") return date.strftime(\"%A %d %B %Y\")"}
{"text_id": "9727", "text": "docstring: def rewrite(self, source_bucket, source_object, destination_bucket, destination_object=None): destination_object = destination_object or source_object if (source_bucket == destination_bucket and source_object == destination_object): raise ValueError( 'Either source/destination bucket or source/destination object ' 'must be different, not both the same: bucket=%s, object=%s' % (source_bucket, source_object)) if not source_bucket or not source_object: raise ValueError('source_bucket and source_object cannot be empty.') client = self.get_conn() source_bucket = client.bucket(source_bucket) source_object = source_bucket.blob(blob_name=source_object) destination_bucket = client.bucket(destination_bucket) token, bytes_rewritten, total_bytes = destination_bucket.blob( blob_name=destination_object).rewrite( source=source_object ) self.log.info('Total Bytes: %s | Bytes Written: %s', total_bytes, bytes_rewritten) while token is not None: token, bytes_rewritten, total_bytes = destination_bucket.blob( blob_name=destination_object).rewrite( source=source_object, token=token ) self.log.info('Total Bytes: %s | Bytes Written: %s', total_bytes, bytes_rewritten) self.log.info('Object %s in bucket %s copied to object %s in bucket %s', source_object.name, source_bucket.name, destination_object, destination_bucket.name)"}
{"text_id": "9728", "text": "docstring: def _maybe_quote(val): assert isinstance(val, str), 'We should never set non-string values.' needs_quoting = False try: int(val) needs_quoting = True except Exception: pass try: float(val) needs_quoting = True except Exception: pass if needs_quoting: return '\"{}\"'.format(val) else: return val"}
{"text_id": "9729", "text": "docstring: def remove(self, effect): self.connection.send(ProtocolParser.remove(effect))"}
{"text_id": "9730", "text": "docstring: def author_profile_background_image_url(self) -> str: return self.tweet[1][\"user\"][\"profile_background_image_url\"]"}
{"text_id": "9731", "text": "docstring: def remove_control_dep(self, dep): self.update_forwards() if dep.forwarded in self.control_deps: self._control_deps.remove(dep.forwarded) self.invalidate_property_cache('all_deps')"}
{"text_id": "9732", "text": "docstring: def build_instance_tags(clusterid, status='owned'): tags = {'clusterid': clusterid, 'kubernetes.io/cluster/{}'.format(clusterid): status} return tags"}
{"text_id": "9733", "text": "docstring: def process_file( file, logger=None, write=False, min_len=0, min_seqs=0, max_ambiguous=0, remove_dashes=False, remove_dups=False, remove_substrings=False, lengths=False, ): if logger: logger.debug(\"processing %s\", file) out_sequences = [] sanitizer = Sanitizer(remove_dashes=remove_dashes) too_short = ShortCounter(length=min_len, log=logger, name=file.name) too_ambig = AmbigCounter(frac=max_ambiguous, log=logger, name=file.name) duplicated = DuplicateCounter(log=logger, name=file.name) with file.open(\"rU\") as handle: for record in SeqIO.parse(handle, SEQ_FILE_TYPE): seq = record.seq.upper().tomutable() seq = sanitizer.sanitize(seq) if not len(seq): continue if too_short.test(seq, record.id): continue if too_ambig.test(seq, record.id): continue if duplicated.exact(seq, record.id) and remove_dups: continue record.seq = seq.toseq() out_sequences.append(record) if remove_dups and remove_substrings: out_sequences = duplicated.substring(out_sequences) if (remove_dups or remove_substrings) and len(duplicated.dupDict) > 0: for record in out_sequences: if record.id in duplicated.dupDict: record.id = duplicated.dupDict.get(record.id) record.description = \"\" if len(out_sequences) < min_seqs: small = 1 if write: file.unlink() else: small = 0 if lengths and logger: for record in out_sequences: logger.debug( LOGINT_FMT, file.name, record.id, LENGTH_NAME, len(record.seq), ) if write: with file.open(\"w\") as output_handle: SeqIO.write(out_sequences, output_handle, SEQ_FILE_TYPE) lengths_arr = np.array([len(s) for s in out_sequences]) length_std = lengths_arr.std() length_mean = lengths_arr.mean() + 0.5 if np.isnan(length_std): length_std_percent = 0 else: length_std_percent = int(round(length_std * 100 / length_mean)) return ( file.name, sanitizer.chars_in, lengths_arr.sum(), sanitizer.chars_removed, sanitizer.chars_fixed, sanitizer.endchars_removed, sanitizer.seqs_sanitized, len(out_sequences), too_ambig.count, too_short.count, duplicated.exact_count, duplicated.substring_count, length_std_percent, small, )"}
{"text_id": "9734", "text": "docstring: def can_change_group(self, this_group): if __debug__: assert isinstance(this_group, Group) if not self.user.is_active: raise PermissionDenied(\"Requesting user is not active\") if not this_group.gaccess.active: raise PermissionDenied(\"Group is not active\") if self.user.is_superuser: return True return this_group.gaccess.edit_users.filter(id=self.user.id).exists()"}
{"text_id": "9735", "text": "docstring: def recursive_feature_elimination(self): self._debug(f\"Feature importance {self.tag}: Recursive feature elimination\") if not self.is_rfe_model: return if self.is_regression(): if self.is_rfe_model_lasso: ret = self._rfe_model_lasso() self._rfe_add_result(ret) if self.is_rfe_model_ridge: ret = self._rfe_model_ridge() self._rfe_add_result(ret) if self.is_rfe_model_lars_aic: ret = self._rfe_model_lars_aic() self._rfe_add_result(ret) if self.is_rfe_model_lars_bic: ret = self._rfe_model_lars_bic() self._rfe_add_result(ret) if self.is_rfe_model_random_forest: ret = self._rfe_model_random_forest() self._rfe_add_result(ret) if self.is_rfe_model_extra_trees: ret = self._rfe_model_extra_trees() self._rfe_add_result(ret) if self.is_rfe_model_gradient_boosting: ret = self._rfe_model_gradient_boosting() self._rfe_add_result(ret)"}
{"text_id": "9736", "text": "docstring: def draw_number(stdscr, nb, offset): for n in numbers_to_display(nb): draw_raw_number(stdscr, n, offset)"}
{"text_id": "9737", "text": "docstring: def begin_delete( self, resource_group_name, namespace_name, **kwargs ): polling = kwargs.pop('polling', True) cls = kwargs.pop('cls', None) lro_delay = kwargs.pop( 'polling_interval', self._config.polling_interval ) cont_token = kwargs.pop('continuation_token', None) if cont_token is None: raw_result = self._delete_initial( resource_group_name=resource_group_name, namespace_name=namespace_name, cls=lambda x,y,z: x, **kwargs ) kwargs.pop('error_map', None) kwargs.pop('content_type', None) def get_long_running_output(pipeline_response): if cls: return cls(pipeline_response, None, {}) if polling is True: polling_method = ARMPolling(lro_delay, **kwargs) elif polling is False: polling_method = NoPolling() else: polling_method = polling if cont_token: return LROPoller.from_continuation_token( polling_method=polling_method, continuation_token=cont_token, client=self._client, deserialization_callback=get_long_running_output ) else: return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"text_id": "9738", "text": "docstring: def mark_clean(self): for attr in self.values(): attr.mark_clean()"}
{"text_id": "9739", "text": "docstring: def bayesian_hparam_optimization(cfg): dataset = pd.read_csv(cfg['PATHS']['PREPROCESSED_DATA']) dataset['Date'] = pd.to_datetime(dataset['Date']) model_name = cfg['TRAIN']['MODEL'].upper() objective_metric = cfg['TRAIN']['HPARAM_SEARCH']['HPARAM_OBJECTIVE'] results = {'Trial': [], objective_metric: []} dimensions = [] default_params = [] hparam_names = [] for hparam_name in cfg['HPARAM_SEARCH'][model_name]: if cfg['HPARAM_SEARCH'][model_name][hparam_name]['RANGE'] is not None: if cfg['HPARAM_SEARCH'][model_name][hparam_name]['TYPE'] == 'set': dimensions.append(Categorical(categories=cfg['HPARAM_SEARCH'][model_name][hparam_name]['RANGE'], name=hparam_name)) elif cfg['HPARAM_SEARCH'][model_name][hparam_name]['TYPE'] == 'int_uniform': dimensions.append(Integer(low=cfg['HPARAM_SEARCH'][model_name][hparam_name]['RANGE'][0], high=cfg['HPARAM_SEARCH'][model_name][hparam_name]['RANGE'][1], prior='uniform', name=hparam_name)) elif cfg['HPARAM_SEARCH'][model_name][hparam_name]['TYPE'] == 'float_log': dimensions.append(Real(low=cfg['HPARAM_SEARCH'][model_name][hparam_name]['RANGE'][0], high=cfg['HPARAM_SEARCH'][model_name][hparam_name]['RANGE'][1], prior='log-uniform', name=hparam_name)) elif cfg['HPARAM_SEARCH'][model_name][hparam_name]['TYPE'] == 'float_uniform': dimensions.append(Real(low=cfg['HPARAM_SEARCH'][model_name][hparam_name]['RANGE'][0], high=cfg['HPARAM_SEARCH'][model_name][hparam_name]['RANGE'][1], prior='uniform', name=hparam_name)) default_params.append(cfg['HPARAMS'][model_name][hparam_name]) hparam_names.append(hparam_name) results[hparam_name] = [] def objective(vals): hparams = dict(zip(hparam_names, vals)) print('HPARAM VALUES: ', hparams) scores = cross_validation(cfg, dataset=dataset, metrics=[objective_metric], model_name=model_name, hparams=hparams)[objective_metric] score = scores[scores.shape[0] - 2] return score search_results = gp_minimize(func=objective, dimensions=dimensions, acq_func='EI', n_calls=cfg['TRAIN']['HPARAM_SEARCH']['N_EVALS'], verbose=True) print(search_results) plot_bayesian_hparam_opt(model_name, hparam_names, search_results, save_fig=True) trial_idx = 0 for t in search_results.x_iters: results['Trial'].append(str(trial_idx)) results[objective_metric].append(search_results.func_vals[trial_idx]) for i in range(len(hparam_names)): results[hparam_names[i]].append(t[i]) trial_idx += 1 results['Trial'].append('Best') results[objective_metric].append(search_results.fun) for i in range(len(hparam_names)): results[hparam_names[i]].append(search_results.x[i]) results_df = pd.DataFrame(results) results_path = cfg['PATHS']['EXPERIMENTS'] + 'hparam_search_' + model_name + \\ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.csv' results_df.to_csv(results_path, index_label=False, index=False) return search_results"}
{"text_id": "9740", "text": "docstring: def UGE(self, o): unsigned_bounds_1 = self._unsigned_bounds() unsigned_bounds_2 = o._unsigned_bounds() ret = [] for lb_1, ub_1 in unsigned_bounds_1: for lb_2, ub_2 in unsigned_bounds_2: if lb_1 >= ub_2: ret.append(TrueResult()) elif ub_1 < lb_2: ret.append(FalseResult()) else: ret.append(MaybeResult()) if all(r.identical(TrueResult()) for r in ret): return TrueResult() elif all(r.identical(FalseResult()) for r in ret): return FalseResult() else: return MaybeResult()"}
{"text_id": "9741", "text": "docstring: def validate_globcfg(globcfg): assert utils.is_non_neg_int(globcfg[\"base_address\"]), \\ \"Wrong value for 'base_address'='%s'. Must be a non negative integer.\" % globcfg[\"base_address\"] assert utils.is_non_neg_int(globcfg[\"data_width\"]), \\ \"Wrong value for 'data_width'='%s'. Must be a non negative integer.\" % globcfg[\"data_width\"] assert utils.is_non_neg_int(globcfg[\"address_width\"]), \\ \"Wrong value for 'address_width'='%s'. Must be a non negative integer.\" % globcfg[\"address_width\"] register_reset_allowed = ['sync_pos', 'sync_neg', 'async_pos', 'async_neg'] assert globcfg[\"register_reset\"] in register_reset_allowed, \\ \"Wrong value for 'register_reset'='%s'. Must be one of this: %s.\" % (globcfg[\"address_width\"], register_reset_allowed) address_increment_alowed = ['none', 'data_width'] try: is_valid = (globcfg[\"address_increment\"] in address_increment_alowed or utils.is_non_neg_int(globcfg[\"address_increment\"])) except ValueError: is_valid = False assert is_valid, \\ \"Wrong value for 'address_increment'='%s'. Must be one of this: %s or a non negative integer.\" % ( globcfg[\"address_increment\"], address_increment_alowed) address_alignment_alowed = ['none', 'data_width'] try: is_valid = (globcfg[\"address_alignment\"] in address_alignment_alowed or utils.is_non_neg_int(globcfg[\"address_alignment\"])) except ValueError: is_valid = False assert is_valid, \\ \"Wrong value for 'address_alignment'='%s'. Must be one of this: %s or a non negative integer.\" % ( globcfg[\"address_alignment\"], address_alignment_alowed) force_name_case_allowed = ['lower', 'upper', 'none'] assert globcfg[\"force_name_case\"] in force_name_case_allowed, \\ \"Wrong value for 'force_name_case'='%s'. Must be one of this: %s.\" % (globcfg[\"force_name_case\"], force_name_case_allowed)"}
{"text_id": "9742", "text": "docstring: def data_received(self, msg: Message) -> None: self._this_msg, self._prev_msg = msg, self._this_msg self._callback(msg)"}
{"text_id": "9743", "text": "docstring: def handle_link(self, source_index: int, sink_index: int, label: str) -> None: assert source_index in self._index_map assert sink_index in self._index_map assert self._phrase_stack source_id = self._index_map[source_index] sink_id = self._index_map[sink_index] if sink_id in self._links[source_id]: self._links[source_id][sink_id].add(label) else: self._links[source_id][sink_id] = {label} self._phrase_stack[-1][-1].append((source_id, sink_id))"}
{"text_id": "9744", "text": "docstring: def pccs(self): yield from self.infr.pccs"}
{"text_id": "9745", "text": "docstring: def find_zone(self, name_or_id, ignore_missing=False): return self._find(_zone.Zone, name_or_id, ignore_missing=ignore_missing)"}
{"text_id": "9746", "text": "docstring: def dot(self, other): return self.__matmul__(other)"}
{"text_id": "9747", "text": "docstring: def apply_transform(self, x, transform_parameters): img_row_axis = self.row_axis - 1 img_col_axis = self.col_axis - 1 img_channel_axis = self.channel_axis - 1 x = apply_affine_transform( x, transform_parameters.get('theta', 0), transform_parameters.get('tx', 0), transform_parameters.get('ty', 0), transform_parameters.get('shear', 0), transform_parameters.get('zx', 1), transform_parameters.get('zy', 1), row_axis=img_row_axis, col_axis=img_col_axis, channel_axis=img_channel_axis, fill_mode=self.fill_mode, cval=self.cval, order=self.interpolation_order) if transform_parameters.get('channel_shift_intensity') is not None: x = apply_channel_shift(x, transform_parameters['channel_shift_intensity'], img_channel_axis) if transform_parameters.get('flip_horizontal', False): x = flip_axis(x, img_col_axis) if transform_parameters.get('flip_vertical', False): x = flip_axis(x, img_row_axis) if transform_parameters.get('brightness') is not None: x = apply_brightness_shift(x, transform_parameters['brightness'], False) return x"}
{"text_id": "9748", "text": "docstring: def p_columna(t): if(t[1].lower()=='primary'): t[0]=llaveTabla(True, None, t[4], None) elif(t[1].lower()=='foreign'): t[0]=llaveTabla(False, t[7], t[4], t[9]) else: t[0]=columnaTabla(ExpresionIdentificador(t[1]), t[2], t[3],t[4], t[5])"}
{"text_id": "9749", "text": "docstring: def range_included(first_start, first_end, second_start, second_end): if ((first_start == second_start or between(first_start, second_start, second_end)) and (first_end == second_end or between(first_end, second_start, second_end))): return True else: return False"}
{"text_id": "9750", "text": "docstring: def clone(self, deep=False): return copy.deepcopy(self) if deep else copy.copy(self)"}
{"text_id": "9751", "text": "docstring: def exitcode(self): try: return int(self.results.most_significant_state) except ValueError: return 3"}
{"text_id": "9752", "text": "docstring: def splitLines(s, buffer): lines = LINESEP.split(buffer + s) return lines[:-1], lines[-1]"}
{"text_id": "9753", "text": "docstring: def __polling_for_reply(self) -> None: for _ in range(10): self.read_message() if (self.last_reply[2] == self.last_command[2] or (self.last_command[2] == Command.QUERY_DATA.value and self.last_reply[1] == MessageType.DATA.value)): break else: raise TimeoutError('No reply received from sensor!')"}
{"text_id": "9754", "text": "docstring: def instance_config_path(cls, project, instance_config): return cls._INSTANCE_CONFIG_PATH_TEMPLATE.render({ 'project': project, 'instance_config': instance_config, })"}
{"text_id": "9755", "text": "docstring: def fixture(self, fixture_id: int, includes: Includes = None) -> Response: log.info(\"Fetch fixture (id=%s), includes=%s\", fixture_id, includes) return self._http_get(endpoint=[\"fixtures\", fixture_id], includes=includes)"}
{"text_id": "9756", "text": "docstring: def _generate_clauses(num_vars, num_clauses, min_lits, max_lits, ratio): global _qcache, _options clauses = [] gen_clauses = 0 while gen_clauses < num_clauses: clause = [] if min_lits == num_vars: num_lits = num_vars else: num_lits = random.randint(min_lits, max_lits) (num_ex_lits, num_un_lits) = _generate_num_lits(num_lits, ratio) clause = _generate_clause(num_ex_lits, num_un_lits) assert(len(clause) > 0) if _options.sort: clause = _sort_by_scope(clause) if _options.reduce: _forall_reduce(clause) if len(clause) > 0: if _options.allow_unit or len(clause) > 1: clauses.append(clause) gen_clauses += 1 else: assert(len(clause) == 1) _update_occs_cnt(abs(clause[0]), -1) assert(len(clauses) > 0) assert(len(clauses) == num_clauses) return clauses"}
{"text_id": "9757", "text": "docstring: def generate_module(src): if not src: return UNKNOWN_MODULE filename, ext = splitext(urlsplit(src).path) if filename.endswith(\".min\"): filename = filename[:-4] tokens = filename.split(\"/\") for idx, token in enumerate(tokens): if VERSION_RE.match(token): return \"/\".join(tokens[idx + 1 :]) return CLEAN_MODULE_RE.sub(\"\", filename) or UNKNOWN_MODULE"}
{"text_id": "9758", "text": "docstring: def touchgdoc(group, account, drive, folder, permissions, title, list): pass"}
{"text_id": "9759", "text": "docstring: def _assert_strict_valid_tags_for_blog_post( self, expected_error_substring, tags): with self.assertRaisesRegexp( utils.ValidationError, expected_error_substring): blog_domain.BlogPost.require_valid_tags(tags, True)"}
{"text_id": "9760", "text": "docstring: def _call(self) -> HTTPResponse: return HttpCall.call( method=self.http_method, url=self.endpoint_url, fields=self.fields, headers=self.headers, body=self.http_body )"}
{"text_id": "9761", "text": "docstring: def _remove_definition(self, definition): global_callback._call_library_remove_definition(self, definition) definition._library = None"}
{"text_id": "9762", "text": "docstring: def _first_sunday(year): first_day = datetime.date(year, 3, 1) if first_day.weekday() == 6: return first_day elif first_day.weekday() <= 2: return first_day - pandas.tseries.offsets.Week(weekday=6) else: return first_day + pandas.tseries.offsets.Week(weekday=6)"}
{"text_id": "9763", "text": "docstring: def wait_for_running( max_timeout=60 ): logger.info(\"waiting for the condor daemon ...\") while( max_timeout >= 0 ): try: import htcondor collector = htcondor.Collector() schedd = htcondor.Schedd() security = htcondor.SecMan() return True except Exception as e: print( e ) logger.debug(\"still waiting for the condor daemon to come online\") time.sleep( 5 ) max_timeout -= 5 return False"}
{"text_id": "9764", "text": "docstring: def create_subscriptions(client, sub, topic): project = 'projects/{}'.format(utils.get_project_id()) dest_sub = project + '/subscriptions/' + sub dest_topic = project + '/topics/' + topic body = {'topic': dest_topic} logging.info(\"Create topic %sub on topic %s in project %s\",sub, topic, project) def _do_get_request(): return client.projects().subscriptions().get( subscription=dest_sub).execute() @backoff.on_exception( backoff.expo, HttpError, max_tries=3, giveup=utils.fatal_code) def _do_create_request(): res = client.projects().subscriptions().create( name=dest_sub, body=body).execute() logging.debug(res) try: _do_get_request() except Exception as e: if e.resp.status == 404: logging.error(e) _do_create_request() else: logging.error(e) raise PubSubException(e)"}
{"text_id": "9765", "text": "docstring: def run_false_jwt_login(self): self.start_threaded_attack(self.false_jwt.run)"}
{"text_id": "9766", "text": "docstring: def _parallel_downloader(voxforge_url, archive_dir, total, counter): def download(d): (i, file) = d download_url = voxforge_url + '/' + file c = counter.increment() print('Downloading file {} ({}/{})...'.format(i+1, c, total)) base.maybe_download(filename_of(download_url), archive_dir, download_url) return download"}
{"text_id": "9767", "text": "docstring: def theo_CL_freq_resp(k, x_ea_perc, x_fh_perc): df, ddf = j * k, -k ** 2 CLa_qs, CLda_qs, CLb_qs, CLdb_qs, void, void, void = qs_derivs(x_ea_perc, x_fh_perc) CLqs = np.array([ CLa_qs + CLda_qs * df, CLa_qs * df, CLb_qs + CLdb_qs * df, ]) CLda_nc, CLdda_nc, CLdb_nc, CLddb_nc, void, void, void, void \\ = nc_derivs(x_ea_perc, x_fh_perc) df, ddf = j * k, -k ** 2 CLun = np.array([ CLda_nc * df + CLdda_nc * ddf, CLda_nc * ddf, CLdb_nc * df + CLddb_nc * ddf ]) Y = theo_fun(k) * CLqs + CLun Y[1] = -Y[1] return Y"}
{"text_id": "9768", "text": "docstring: def egress_nat_rule_ids(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]: return pulumi.get(self, \"egress_nat_rule_ids\")"}
{"text_id": "9769", "text": "docstring: def _find_step_function(request, step, scenario, encoding): name = step.name try: return request.getfixturevalue(get_step_fixture_name(name, step.type, encoding)) except pytest_fixtures.FixtureLookupError: try: name = find_argumented_step_fixture_name(name, step.type, request._fixturemanager, request) if name: return request.getfixturevalue(name) raise except pytest_fixtures.FixtureLookupError: raise exceptions.StepDefinitionNotFoundError( u\"\"\"Step definition is not found: {step}.\"\"\" \"\"\" Line {step.line_number} in scenario \"{scenario.name}\" in the feature \"{feature.filename}\"\"\".format( step=step, scenario=scenario, feature=scenario.feature ) )"}
{"text_id": "9770", "text": "docstring: def downgrade(): if context.get_context().dialect.name in ['oracle', 'mysql', 'postgresql']: create_primary_key('CONFIGS_HISTORY_PK', 'configs_history', ['section', 'opt', 'updated_at'])"}
{"text_id": "9771", "text": "docstring: def conclude_user_session( self, session_state ): session_state.old_amplitude_events = dict(session_state.amplitude_events)"}
{"text_id": "9772", "text": "docstring: def str_datetime(dt, formats=None): if isinstance(dt, str): return dt if isinstance(formats, str): return dt.strftime(formats) if formats is None: formats = DATETIME_FORMATS return dt.strftime(formats[0])"}
{"text_id": "9773", "text": "docstring: def on_dismiss(self, event): value = self.list.is_value_waiting() canceled = self.list.is_canceled() self.list.clear_canceled() if canceled: self.list.set_waiting_value(-1) return elif value >= 0: self.GetTextCtrl().SetSelection(0, 0) self.list.select_item(value) self.GetTextCtrl().SetSelection(0, len(self.GetTextCtrl().GetValue())) self.list.set_waiting_value(-1) return event.Skip()"}
{"text_id": "9774", "text": "docstring: def check_gausslog_fileset(file_set, good_vibes_check, results_dict): total_react_charge = 0 ts_charge = np.nan ts_index = None total_product_charge = 0 multiplicities = np.full([len(file_set)], np.nan) solvent = None func = None basis = None gauss_ver = None react_stoich_dict = {} ts_stoich_dict = {} prod_stoich_dict = {} reading_reactants = True for index, fname in enumerate(file_set): base_name = os.path.basename(fname) if fname == REACT_PROD_SEP: ts_index = index reading_reactants = False if index == 0: raise InvalidDataError(\"Did not expect to read find reactant/product separator ('{}') as the first \" \"entry\".format(REACT_PROD_SEP)) else: multiplicities[index] = multiplicities[index - 1] continue gauss_result = results_dict[base_name][HARTREE_OUT] freq_vals = results_dict[base_name][FREQS] stoich = results_dict[base_name][STOICH] if freq_vals[0] < 0 and freq_vals[1] < 0: raise InvalidDataError(\"The first two frequencies are both imaginary in file: {}\".format(fname)) if freq_vals[0] < 0: if not reading_reactants: raise InvalidDataError(\"In each set of files, only one file with an imaginary frequency is expected\\n\" \" (or none if reactant/product separator is used). Unexpectedly found an \" \"imaginary frequency in\\n file: {}\\n after already finding the TS to be: \" \"{}\".format(fname, file_set[ts_index])) reading_reactants = False ts_index = index ts_charge = gauss_result.getCharge() ts_stoich_dict = parse_stoich(stoich) elif reading_reactants: total_react_charge += gauss_result.getCharge() if len(react_stoich_dict) == 0: react_stoich_dict = parse_stoich(stoich) else: react_stoich_dict = parse_stoich(stoich, add_to_dict=react_stoich_dict) else: total_product_charge += gauss_result.getCharge() if len(prod_stoich_dict) == 0: prod_stoich_dict = parse_stoich(stoich) else: prod_stoich_dict = parse_stoich(stoich, add_to_dict=prod_stoich_dict) multiplicities[index] = int(gauss_result.getMult()) file_gauss_ver = None i = 0 with open(fname) as f: for line in f: s_line = line.strip() if GAUSS_VER_PAT.match(s_line): file_gauss_ver = s_line.split()[:3] break i += 1 if i > 160: break if index == 0: solvent = str(results_dict[base_name][SOLV]).lower() func = str(gauss_result.getFunctional()).lower() if func.startswith(\"r\") or func.startswith(\"u\"): func = func[1:] basis = str(gauss_result.getBasisSet()).lower() gauss_ver = file_gauss_ver else: if str(gauss_result.getSolvent()).lower() != solvent: raise InvalidDataError(\"Different solvents ({}, {}) found for file set: \" \"{}\".format(solvent, gauss_result.getSolvent(), file_set)) current_func = str(gauss_result.getFunctional()).lower() if current_func.startswith(\"u\") or current_func.startswith(\"r\"): current_func = current_func[1:] if current_func != func: raise InvalidDataError(\"Different functionals ({}, {}) found for file set: \" \"{}\".format(func, gauss_result.getFunctional(), file_set)) if str(gauss_result.getBasisSet()).lower() != basis: raise InvalidDataError(\"Different basis sets ({}, {}) found for file set: \" \"{}\".format(basis, gauss_result.getBasisSet(), file_set)) if gauss_ver != file_gauss_ver: warning(\"Different Gaussian versions ({}, {}) found for file set: {}\". format(gauss_ver, file_gauss_ver, file_set)) file_set_str = \"\\n \".join([\"\"] + [os.path.relpath(f) for f in file_set]) if len(ts_stoich_dict) > 0: if react_stoich_dict != ts_stoich_dict: raise InvalidDataError(\"Check stoichiometries of reactant(s) and transition state for set: {}\\n\" \"reactants: {}, products: {}\".format(file_set_str, react_stoich_dict, ts_stoich_dict)) if total_react_charge != ts_charge: raise InvalidDataError(\"Check charge of reactant(s) and transition state for set: {}\\n\" \"Found {} and {}, respectively\".format(file_set_str, total_react_charge, ts_charge)) if len(prod_stoich_dict) > 0: if react_stoich_dict != prod_stoich_dict: raise InvalidDataError(\"Check stoichiometries of reactant(s) and product(s) for set: {}\\n\" \"reactants: {}, products: {}\".format(file_set_str, react_stoich_dict, prod_stoich_dict)) if total_react_charge != total_product_charge: raise InvalidDataError(\"Check charge of reactant(s) and product(s) for set: {}\\nFound {} and {}, \" \"respectively\".format(file_set_str, total_react_charge, total_product_charge)) mult_check = np.sum(multiplicities - multiplicities[0]) if mult_check != 0: raise InvalidDataError(\"Check multiplicities in set: {}\\nFound: {}\".format(file_set, multiplicities)) if good_vibes_check: if solvent: file_set = file_set + [\"-c\", \"1\"] if REACT_PROD_SEP in file_set: file_set.remove(REACT_PROD_SEP) with capture_stdout(gaussian_wrangler.goodvibes_hm.main, file_set + [\"--check\"]) as output: vibes_out = output.split(\"\\n\") for line in vibes_out: if GOODVIBES_ERROR_PAT.match(line): if 'Different charge and multiplicity' in line: continue raise InvalidDataError(\"See GoodVibes error checking report: 'Goodvibes_output.dat'\") return solvent, ts_index"}
{"text_id": "9775", "text": "docstring: def sweObject(obj, jd): sweObj = SWE_OBJECTS[obj] sweList, flg = swisseph.calc_ut(jd, sweObj) return { 'id': obj, 'lon': sweList[0], 'lat': sweList[1], 'lonspeed': sweList[3], 'latspeed': sweList[4] }"}
{"text_id": "9776", "text": "docstring: def print_result(date_printer, choosen_record): message = \"Punched in on %s.\" end_date_str = date_printer.full_date_string(choosen_record.span.start) print(message % (end_date_str))"}
{"text_id": "9777", "text": "docstring: def sphere(particle): _sum = 0.0 for _x in particle.x: _sum = _sum + _x ** 2 return _sum"}
{"text_id": "9778", "text": "docstring: def astra_fbp_cuda(self, proj_type='cuda'): n_d = self.sinogram.shape[1] vol_geom = astra.create_vol_geom(self.n, self.n) proj_geom = astra.create_proj_geom('parallel', 1.0, n_d, self.angs) proj_id = astra.create_projector(proj_type, proj_geom, vol_geom) sino_id = astra.data2d.create('-sino', proj_geom, data=self.sinogram) rec_id = astra.data2d.create('-vol', vol_geom) cfg = astra.astra_dict('FBP_CUDA') cfg['ReconstructionDataId'] = rec_id cfg['ProjectionDataId'] = sino_id alg_id = astra.algorithm.create(cfg) cfg['option'] = {} cfg['option']['FilterType'] = self.filter astra.algorithm.run(alg_id, 1) rec = astra.data2d.get(rec_id) if self.circle: astra.extrautils.clipCircle(rec) return rec.astype(float)"}
{"text_id": "9779", "text": "docstring: def _schedule_coroutine(self, coro, done_callback): fid = self.next_future_id future = self.event_loop.create_task(coro) def cb(f): done_callback(f) del self.pending_futures[fid] self.pending_futures[fid] = future future.add_done_callback(cb) self.next_future_id += 1 return future"}
{"text_id": "9780", "text": "docstring: async def read_body(self, receive): body = b'' more_body = True while more_body: message = await receive() body += message.get('body', b'') more_body = message.get('more_body', False) return body"}
{"text_id": "9781", "text": "docstring: def _callback(self, transformation): if self.callback is not None: self.callback(transformation, self.n_iter_) self.n_iter_ += 1"}
{"text_id": "9782", "text": "docstring: def configure_puppet_agent(): configure_puppet_external_facts() bootstrap_puppet_agent_config()"}
{"text_id": "9783", "text": "docstring: def is_matched(string: str): mapper = {'(': ')', '{': '}', '[': ']'} stack = [] for char in string: if char in mapper: stack.append(mapper[char]) elif char not in mapper.values(): continue elif not (stack and char == stack.pop()): return False return not stack"}
{"text_id": "9784", "text": "docstring: def title(self) -> str: title = self.__py_view__.title if title is None: return title else: return str(title)"}
{"text_id": "9785", "text": "docstring: def parameter_dict(self): return dict( v_0=self.v_0 / (nu.km / nu.s), v_esc=self.v_esc / (nu.km / nu.s), rho_dm=self.rho_dm / (nu.GeV / nu.c0 ** 2 / nu.cm ** 3), )"}
{"text_id": "9786", "text": "docstring: def usage(self, console, line): if not line: line = \"usage\" line = line.lower() if line == \"usage\": console.msg(\"Usage: usage <command>\") elif line in self._commands.keys(): usage = \"Usage: \" + self._commands[line].USAGE console.msg(usage) else: console.msg(\"usage: Unknown command: \" + line) return False return True"}
{"text_id": "9787", "text": "docstring: def silence_comment_subscription(user, media_entry): cn = get_comment_subscription(user.id, media_entry.id) if cn: cn.notify = False cn.send_email = False cn.save()"}
{"text_id": "9788", "text": "docstring: def rename(name, source, force=False, makedirs=False): name = os.path.expanduser(name) source = os.path.expanduser(source) ret = { 'name': name, 'changes': {}, 'comment': '', 'result': True} if not name: return _error(ret, 'Must provide name to file.rename') if not os.path.isabs(name): return _error( ret, 'Specified file {0} is not an absolute path'.format(name)) if not os.path.lexists(source): ret['comment'] = ('Source file \"{0}\" has already been moved out of ' 'place').format(source) return ret if os.path.lexists(source) and os.path.lexists(name): if not force: ret['comment'] = ('The target file \"{0}\" exists and will not be ' 'overwritten'.format(name)) ret['result'] = False return ret elif not __opts__['test']: try: __salt__['file.remove'](name) except (IOError, OSError): return _error( ret, 'Failed to delete \"{0}\" in preparation for ' 'forced move'.format(name) ) if __opts__['test']: ret['comment'] = 'File \"{0}\" is set to be moved to \"{1}\"'.format( source, name ) ret['result'] = None return ret dname = os.path.dirname(name) if not os.path.isdir(dname): if makedirs: __salt__['file.makedirs'](name) else: return _error( ret, 'The target directory {0} is not present'.format(dname)) try: if os.path.islink(source): linkto = os.readlink(source) os.symlink(linkto, name) os.unlink(source) else: shutil.move(source, name) except (IOError, OSError): return _error( ret, 'Failed to move \"{0}\" to \"{1}\"'.format(source, name)) ret['comment'] = 'Moved \"{0}\" to \"{1}\"'.format(source, name) ret['changes'] = {name: source} return ret"}
{"text_id": "9789", "text": "docstring: def create_project_dir(project): outdir = project if os.path.exists(outdir): if not os.path.isdir(outdir): raise Exception( \"Output directory %s already exists but is not a directory\" % outdir ) else: os.mkdir(outdir) return \"%s/%s\" % (outdir, project)"}
{"text_id": "9790", "text": "docstring: def _join_layer_strings(lyrs, count_sep='*', sep=';'): def _s(count, lyr): if count > 1 and lyr: ret = ('{:d}' + count_sep + '{:s}').format(count, lyr) elif lyr: ret = lyr else: ret = sep * (count - 1) return ret counts, lyrs = zip(*[ (len(list(g)), lyr) for lyr, g in itertools.groupby(lyrs)]) lyr = sep.join([_s(count, lyr) for count, lyr in zip(counts, lyrs)]) return lyr"}
{"text_id": "9791", "text": "docstring: def smooth_elevation(self, index: int): df_segment = self.get_segment(index) elevation = df_segment.ele.to_numpy() n = int(np.ceil(df_segment.shape[0]*0.05)) elevation_ma = self._moving_average(elevation, n) smooth_elevation = np.concatenate( (np.array([elevation[0] + i*(elevation_ma[0]-elevation[0])/n for i in range(1, n)]), elevation_ma) ) df_segment = df_segment.drop(columns=['ele']) df_segment['ele'] = smooth_elevation self.df_track.loc[self.df_track['segment'] == index] = df_segment"}
{"text_id": "9792", "text": "docstring: def assign_roles( self, world_state: stp.rc.WorldState, ) -> None: used_robots = set() for tactic in self.prioritized_tactics: robots_for_tactic = [] for cost_fn, role in tactic.role_requests: min_cost = 1e9 cheapest_robot = None numOfInvisibleRobots = 0 for robot in world_state.our_robots: if robot in used_robots or robot in robots_for_tactic: continue if not robot.visible: numOfInvisibleRobots += 1 continue cost = cost_fn(robot, world_state) if cost < min_cost: min_cost = cost cheapest_robot = robot if cheapest_robot is None: print(f\"RoleRequest ({role}, {cost_fn}) was not assigned\") robots_for_tactic = None break robots_for_tactic.append(cheapest_robot) if robots_for_tactic is not None: used_robots.update(robots_for_tactic) tactic.set_assigned_robots(robots_for_tactic) self.approved_prioritized_tactics.append(tactic) tactic.init_roles(world_state) else: numRobotsAvailable = ( len(world_state.our_robots) - len(used_robots) - numOfInvisibleRobots ) print( f\"Tactic {tactic} denied: {len(tactic.role_requests)} requested roles, but only {numRobotsAvailable} robots available\" ) for robot in world_state.our_robots: if robot not in used_robots: self.unassigned_roles.append(unassigned_role.UnassignedRole(robot))"}
{"text_id": "9793", "text": "docstring: def as_function(self, name=\"layer_function\", compile=False): graph = Graph.build(inputs=None, outputs=self) return graph.as_function(name=name, compile=compile)"}
{"text_id": "9794", "text": "docstring: def _get_kticks(self, ax): high_sym_points = self.kpoints.kpts kpts_labels = np.array([f'${k}$' if k != 'G' else '$\\\\Gamma$' for k in self.kpoints.labels]) all_kpoints = self.vasprun.actual_kpoints index = [0] for i in range(len(high_sym_points) - 2): if high_sym_points[i + 2] != high_sym_points[i + 1]: index.append(i) index.append(len(high_sym_points) - 1) kpts_loc = np.isin(all_kpoints, high_sym_points).all(1) kpoints_index = np.where(kpts_loc == True)[0] kpts_labels = kpts_labels[index] kpoints_index = list(kpoints_index[index]) for i in range(len(kpoints_index)): if 0 < i < len(kpoints_index) - 1: kpoints_index[i] = kpoints_index[i] + 0.5 for k in kpoints_index: ax.axvline(x=k, color='black', alpha=0.7, linewidth=0.5) ax.set_xticks(kpoints_index) ax.set_xticklabels(kpts_labels)"}
{"text_id": "9795", "text": "docstring: def _create_params_from_java(self): java_params = list(self._java_obj.params()) from pyspark.ml.param import Param for java_param in java_params: java_param_name = java_param.name() if not hasattr(self, java_param_name): param = Param(self, java_param_name, java_param.doc()) setattr(param, \"created_from_java_param\", True) setattr(self, java_param_name, param) self._params = None"}
{"text_id": "9796", "text": "docstring: def GetLineRaw(*args, **kwargs): return _stc.StyledTextCtrl_GetLineRaw(*args, **kwargs)"}
{"text_id": "9797", "text": "docstring: def dot(self, v): return dot(self._vxyz, v.vxyz)"}
{"text_id": "9798", "text": "docstring: def add_object(self, new_obj: 'PhysObj') -> None: self.objects.append(new_obj)"}
{"text_id": "9799", "text": "docstring: def PitchBend(pitch_bend, *, channel=None): return PitchBend(pitch_bend=pitch_bend, channel=channel)"}
{"text_id": "9800", "text": "docstring: def __get_prep_basis_op(dic, basis): keys = sorted(dic.keys()) tups = [dic[k] for k in keys] return reduce(lambda acc, b: np.kron(basis[b[0]][b[1]], acc), tups, [1])"}
{"text_id": "9801", "text": "docstring: def int_from_text(text, default=0): g = re.search(r'\\d+', str(text)) return default if g is None else int(g.group(0))"}
{"text_id": "9802", "text": "docstring: def message_error(self, text, title=\"Error\"): if (sys.version_info >= (3, 0)): messagebox.showerror(title, text) else: tkMessageBox.showerror(title, text)"}
{"text_id": "9803", "text": "docstring: def D(self, repeat): editor = self._widget.editor() cursor = editor.textCursor() cursor.movePosition(QTextCursor.EndOfLine, QTextCursor.KeepAnchor) cursor.movePosition(QTextCursor.Down, QTextCursor.KeepAnchor, repeat - 1) editor.setTextCursor(cursor) editor.cut() self._widget.update_vim_cursor()"}
{"text_id": "9804", "text": "docstring: def which_versions_ok(required_feeds: List[FileInfo], current_modtimes: Dict[str, str]) -> Set[str]: ok_versions: Set[str] = set() for i in required_feeds: curr_modtime = current_modtimes.get(i.version, \"\") if curr_modtime == i.modtime: ok_versions.add(i.version) return ok_versions"}
{"text_id": "9805", "text": "docstring: def _initialize_data(self): self.chopper_name = \"C01\" self.frequency_reference = 50 self.frequency_setpoint = 0 self.frequency = 0 self.phase_setpoint = 0 self.phase = 0 self.phase_status_is_ok = False self.magnetic_bearing_is_on = False self.magnetic_bearing_status_is_ok = False self.drive_is_on = False self.drive_mode_is_start = False self.drive_l1_current = 0 self.drive_l2_current = 0 self.drive_l3_current = 0 self.drive_direction_is_cw = False self.drive_temperature = 0 self.phase_outage = 0 self.master_chopper = \"C1\" self.logging_is_on = False self.dsp_status_is_ok = False self.interlock_er_status_is_ok = False self.interlock_vacuum_status_is_ok = False self.interlock_frequency_monitoring_status_is_ok = False self.interlock_magnetic_bearing_amplifier_temperature_status_is_ok = False self.interlock_magnetic_bearing_amplifier_current_status_is_ok = False self.interlock_drive_amplifier_temperature_status_is_ok = False self.interlock_drive_amplifier_current_status_is_ok = False self.interlock_ups_status_is_ok = False self.error_on_set_frequency = None self.error_on_set_phase = None self.error_on_set_magnetic_bearing = None self.error_on_set_drive_mode = None self.disconnected = False"}
{"text_id": "9806", "text": "docstring: def _handle_command(self, message: dict) -> None: command = Command.from_message(message) if command is None: return for handler in self._command_registry[command.name]: self.executor.submit(self._execute_catching_error, handler, command)"}
{"text_id": "9807", "text": "docstring: def runMethod(name: str, filters: dict, param1=None): if name == 'getLocations' : return onc.getLocations(filters) elif name == 'getLocationHierarchy' : return onc.getLocationHierarchy(filters) elif name == 'getDeployments' : return onc.getDeployments(filters) elif name == 'getDevices' : return onc.getDevices(filters) elif name == 'getDeviceCategories' : return onc.getDeviceCategories(filters) elif name == 'getProperties' : return onc.getProperties(filters) elif name == 'getDataProducts' : return onc.getDataProducts(filters) elif name == 'getDirectScalar' : return onc.getDirectScalar(filters, param1) elif name == 'getDirectByLocation' : return onc.getDirectByLocation(filters, param1) elif name == 'getDirectByDevice' : return onc.getDirectByDevice(filters, param1) elif name == 'getDirectRawByLocation' : return onc.getDirectRawByLocation(filters, param1) elif name == 'getDirectRawByDevice' : return onc.getDirectRawByDevice(filters, param1) elif name == 'getListByLocation' : return onc.getListByLocation(filters, param1) elif name == 'getListByDevice' : return onc.getListByDevice(filters, param1)"}
{"text_id": "9808", "text": "docstring: def mid_box_initialization(dual_vars, clbs, cubs): xt = [] zt = [] for lay_idx, layer in enumerate(dual_vars.sum_beta): init_value = ((clbs[lay_idx] + cubs[lay_idx]) / 2).unsqueeze(1) xt.append(init_value.expand_as(dual_vars.sum_beta[lay_idx]).clone()) if lay_idx > 0: zt.append(0.5 * torch.ones_like(dual_vars.sum_beta[lay_idx])) return PrimalVars(xt, zt)"}
{"text_id": "9809", "text": "docstring: def runtime(self): if self._get_label() == 'custom': return self.get_options().runtime_spec else: runtime_name = scala_build_info.get(self._get_label()).runtime_name return [getattr(self, runtime_name)]"}
{"text_id": "9810", "text": "docstring: def placeOrder(self, order: Order): for vehicle in self.vehicles: if vehicle.isAvailable: self.orders.append(order) vehicle.isAvailable = False return \"Your order was placed successfully!\" return \"There is no available vehicle to deliver an order.\""}
{"text_id": "9811", "text": "docstring: def _get_files(self): parts = self.result.nodeid.split(\"[\") return [\"\"] if len(parts) == 1 else [os.path.basename(parts[1][:-1])]"}
{"text_id": "9812", "text": "docstring: def make_apply(self, name: str, args: List[Node]) -> Node: prod = self.get_function_production_or_raise(name) return self.make_node(prod.id, args)"}
{"text_id": "9813", "text": "docstring: def hashdeep_audit(inputPath,manifestPath,_type=None): _uuid = os.path.basename(inputPath) package = os.path.join(inputPath,_uuid) if _type == 'SIP': target = package elif _type == 'objects': target = os.path.join(package,'objects') command = ['hashdeep','-rvval','-j0','-k',manifestPath,'.'] here = os.getcwd() os.chdir(target) try: hashaudit = subprocess.run(command,stdout=subprocess.PIPE,stderr=subprocess.PIPE) out = hashaudit.stdout.splitlines() result = \"\" error = False for line in out: if line.decode().startswith(\"hashdeep: Audit\"): outcome = line.decode() print(outcome) if outcome == 'hashdeep: Audit failed': status = False elif outcome == 'hashdeep: Audit passed': status = True else: status = False error = True print(\"INCONCLUSIVE AUDIT. SIP NOT VERIFIED.\") result = [out,hashaudit.stderr.decode()] if not error: for line in out: result += line.decode()+\"\\n\" except: print( \"there was a problem with the hashdeep audit. \" \"package NOT verified.\" ) status = False result = \"hashdeep error\" os.chdir(here) return result,status"}
{"text_id": "9814", "text": "docstring: def node_security_group(self) -> Optional[pulumi.Input['pulumi_aws.ec2.SecurityGroup']]: return pulumi.get(self, \"node_security_group\")"}
{"text_id": "9815", "text": "docstring: def update_user_password( user_id: int, user_passwords: UserUpdatePassword, db: Session = Depends(get_db), current_user: DBUser = Depends(get_current_active_user)): db_user = DBUsers.get_user(db, user_id=user_id) if db_user is None: raise HTTPException( status_code=status.HTTP_404_NOT_FOUND, detail=\"User not found\") if db_user != current_user: raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail=\"Can only update its own password\" ) if user_passwords.old_password == user_passwords.new_password: raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail=\"New password cannot be the same as the old one\") if not verify_password(user_passwords.old_password, db_user.password): raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail=\"Incorrect old password\") DBUsers.update_user_password( db=db, user_id=user_id, new_password=user_passwords.new_password) return db_user"}
{"text_id": "9816", "text": "docstring: def check_if_operations_completed(self): prod_order = frappe.get_doc(\"Production Order\", self.production_order) for d in prod_order.get(\"operations\"): total_completed_qty = flt(self.fg_completed_qty) + flt(prod_order.produced_qty) if total_completed_qty > flt(d.completed_qty): frappe.throw(_(\"Row #{0}: Operation {1} is not completed for {2} qty of finished goods in Production Order # {3}. Please update operation status\") .format(d.idx, d.operation, total_completed_qty, self.production_order), OperationsNotCompleteError)"}
{"text_id": "9817", "text": "docstring: def extract(pack) -> bytearray: header_index = ATOM.locate(pack) ATOM_SIZE = bytearray(pack[header_index:header_index + 4]) ATOM_SIZE = struct.unpack('>I', ATOM_SIZE)[0] pack = bytearray(pack[header_index:header_index + ATOM_SIZE]) return pack"}
{"text_id": "9818", "text": "docstring: def isFrameReady(self): return len(self.__buffer) > 1"}
{"text_id": "9819", "text": "docstring: def proportions(data): poms = np.mean(pd.isnull(data), axis=0) pobs = np.mean(np.logical_not(pd.isnull(data)), axis=0) proportions_dict = dict(poms=poms, pobs=pobs) proportions_ = _index_output(proportions_dict, data.columns) return proportions_"}
{"text_id": "9820", "text": "docstring: def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer): if isinstance(example, PaddingInputExample): return InputFeatures( input_ids=[0] * max_seq_length, input_mask=[0] * max_seq_length, segment_ids=[0] * max_seq_length, label_id=0, is_real_example=False) label_map = {} for (i, label) in enumerate(label_list): label_map[label] = i tokens_a, mapping_a = tokenizer.tokenize(example.text_a) tokens_b = None if example.text_b: tokens_b = tokenizer.tokenize(example.text_b) if tokens_b: _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3) else: if len(tokens_a) > max_seq_length - 2: tokens_a = tokens_a[0:(max_seq_length - 2)] Where \"type_ids\" are used to indicate whether this is the first sequence or the second sequence. The embedding vectors for `type=0` and `type=1` were learned during pre-training and are added to the wordpiece embedding vector (and position vector). This is not *strictly* necessary since the [SEP] token unambiguously separates the sequences, but it makes it easier for the model to learn the concept of sequences. For classification tasks, the first vector (corresponding to [CLS]) is used as the \"sentence vector\". Note that this only makes sense because the entire model is fine-tuned. tokens = [] segment_ids = [] tokens.append(\"[CLS]\") segment_ids.append(0) for token in tokens_a: tokens.append(token) segment_ids.append(0) tokens.append(\"[SEP]\") segment_ids.append(0) if tokens_b: for token in tokens_b: tokens.append(token) segment_ids.append(1) tokens.append(\"[SEP]\") segment_ids.append(1) input_ids = tokenizer.convert_tokens_to_ids(tokens) The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to. input_mask = [1] * len(input_ids) Zero-pad up to the sequence length. while len(input_ids) < max_seq_length: input_ids.append(0) input_mask.append(0) segment_ids.append(0) assert len(input_ids) == max_seq_length assert len(input_mask) == max_seq_length assert len(segment_ids) == max_seq_length loc, mas, e1_mas, e2_mas = prepare_extra_data(mapping_a, example.locations, FLAGS.max_distance) label_id = [label_map[label] for label in example.labels] label_id = label_id + [0] * (FLAGS.max_num_relations - len(label_id)) cls_mask = [1] * example.num_relations + [0] * (FLAGS.max_num_relations - example.num_relations) np.set_printoptions(edgeitems=15) if ex_index < 5: tf.logging.info(\"*** Example ***\") tf.logging.info(\"guid: %s\" % (example.guid)) tf.logging.info(\"tokens: %s\" % \" \".join( [tokenization.printable_text(x) for x in tokens])) tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids])) tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask])) tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids])) tf.logging.info(\"loc:\") tf.logging.info(\"\\n\" + str(loc)) tf.logging.info(\"mas:\") tf.logging.info(\"\\n\" + str(mas)) tf.logging.info(\"e1_mas:\") tf.logging.info(\"\\n\" + str(e1_mas)) tf.logging.info(\"e2_mas:\") tf.logging.info(\"\\n\" + str(e2_mas)) tf.logging.info(\"cls_mask:\") tf.logging.info(\"\\n\" + str(cls_mask)) tf.logging.info(\"labels: %s\" % \" \".join([str(x) for x in label_id])) feature = InputFeatures( input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, loc=loc.flatten(), mas=mas.flatten(), e1_mas=e1_mas.flatten(), e2_mas=e2_mas.flatten(), cls_mask=cls_mask, label_id=label_id) return feature"}
{"text_id": "9821", "text": "docstring: def find_best_and_worst_queries(queries : List[query.Query], dataset : statistics.Dataset, n_queries : int, with_timeouts : bool = False ) -> Tuple[List[Tuple[query.Query, float, int, int]], List[Tuple[query.Query, float, int, int]], List[Tuple[query.Query, float, int, int]], List[Tuple[query.Query, float, int, int]]]: best_queries_gc : List[Tuple[query.Query, float, int, int]] = [] worst_queries_gc : List[Tuple[query.Query, float, int, int]] = [] best_queries_fo : List[Tuple[query.Query, float, int, int]] = [] worst_queries_fo : List[Tuple[query.Query, float, int, int]] = [] for parameter_size, actual_size, query_id, gc, fo in dataset: gc_time = gc.cpu_time fo_time = fo.cpu_time query = queries[query_id] if with_timeouts or not gc.timeout: _update_query_list_normalized(worst_queries_gc, best_queries_gc, query, gc_time, n_queries, parameter_size, actual_size) _update_query_list_normalized(worst_queries_fo, best_queries_fo, query, fo_time, n_queries, parameter_size, actual_size) return (best_queries_gc, worst_queries_gc, best_queries_fo, worst_queries_fo)"}
{"text_id": "9822", "text": "docstring: def _load_state(self, context): try: state = cookie_to_state( context.cookie, self.config[\"COOKIE_STATE_NAME\"], self.config[\"STATE_ENCRYPTION_KEY\"], ) except SATOSAStateError as e: state = State() finally: context.state = state msg = \"Loaded state {state} from cookie {cookie}\".format(state=state, cookie=context.cookie) logline = lu.LOG_FMT.format(id=lu.get_session_id(context.state), message=msg) logger.info(logline)"}
{"text_id": "9823", "text": "docstring: def remove_where(frame, col, values): rows = [] for row in range(frame.shape[0]): curr = frame.iloc[row][col] if curr not in values: rows.append(row) frame = frame.iloc[rows] frame = renumber_index(frame) return frame"}
{"text_id": "9824", "text": "docstring: def free(self, key): key = self._encode_key(key) if self._use_rocksdb: self._db.delete(key) else: del(self._db[key])"}
{"text_id": "9825", "text": "docstring: def add_bias_col(self, X: np.ndarray): ones_vector = np.ones((X.shape[0], 1)) X = np.hstack((ones_vector, X)) return X"}
{"text_id": "9826", "text": "docstring: def doTimedPause(port_name, n_pause): if port_name is not None: while n_pause > 0: if n_pause > 750: time_delay = 750 else: time_delay = n_pause if time_delay < 1: time_delay = 1 ebb_serial.command(port_name, 'SM,{0},0,0\\r'.format(time_delay)) n_pause -= time_delay"}
{"text_id": "9827", "text": "docstring: def rmi_lower_bound(self, labels_4D, probs_4D): assert labels_4D.size() == probs_4D.size() p, s = self.rmi_pool_size, self.rmi_pool_stride if self.rmi_pool_stride > 1: if self.rmi_pool_way == 0: labels_4D = F.max_pool2d(labels_4D, kernel_size=p, stride=s, padding=self.kernel_padding) probs_4D = F.max_pool2d(probs_4D, kernel_size=p, stride=s, padding=self.kernel_padding) elif self.rmi_pool_way == 1: labels_4D = F.avg_pool2d(labels_4D, kernel_size=p, stride=s, padding=self.kernel_padding) probs_4D = F.avg_pool2d(probs_4D, kernel_size=p, stride=s, padding=self.kernel_padding) elif self.rmi_pool_way == 2: shape = labels_4D.size() new_h, new_w = shape[2] // s, shape[3] // s labels_4D = F.interpolate(labels_4D, size=(new_h, new_w), mode='nearest') probs_4D = F.interpolate(probs_4D, size=(new_h, new_w), mode='bilinear', align_corners=True) else: raise NotImplementedError(\"Pool way of RMI is not defined!\") label_shape = labels_4D.size() n, c = label_shape[0], label_shape[1] la_vectors, pr_vectors = self.map_get_pairs(labels_4D, probs_4D, radius=self.rmi_radius, is_combine=0) if self.is_cuda: la_vectors = la_vectors.view([n, c, self.half_d, -1]).type(torch.cuda.DoubleTensor).requires_grad_(False) pr_vectors = pr_vectors.view([n, c, self.half_d, -1]).type(torch.cuda.DoubleTensor) else: la_vectors = la_vectors.view([n, c, self.half_d, -1]).type(torch.DoubleTensor).requires_grad_(False) pr_vectors = pr_vectors.view([n, c, self.half_d, -1]).type(torch.DoubleTensor) diag_matrix = torch.eye(self.half_d).unsqueeze(dim=0).unsqueeze(dim=0) la_vectors = la_vectors - la_vectors.mean(dim=3, keepdim=True) la_cov = torch.matmul(la_vectors, la_vectors.transpose(2, 3)) pr_vectors = pr_vectors - pr_vectors.mean(dim=3, keepdim=True) pr_cov = torch.matmul(pr_vectors, pr_vectors.transpose(2, 3)) pr_cov_inv = self.inverse(pr_cov + diag_matrix.type_as(pr_cov) * _POS_ALPHA) la_pr_cov = torch.matmul(la_vectors, pr_vectors.transpose(2, 3)) appro_var = la_cov - torch.matmul(la_pr_cov.matmul(pr_cov_inv), la_pr_cov.transpose(-2, -1)) rmi_now = 0.5 * self.log_det_by_cholesky(appro_var + diag_matrix.type_as(appro_var) * _POS_ALPHA) rmi_per_class = rmi_now.view([-1, self.num_classes]).mean(dim=0).float() rmi_per_class = torch.div(rmi_per_class, float(self.half_d)) rmi_loss = torch.sum(rmi_per_class) if _IS_SUM else torch.mean(rmi_per_class) return rmi_loss"}
{"text_id": "9828", "text": "docstring: def _chan_from_id(self, deviceid, expected_type=None): if deviceid not in self.device.channels.keys(): msg = \"Unknown channel {}\".format(deviceid) self.log.error(msg) raise ValueError(msg) if expected_type is not None and self.device.channels[deviceid].channel_type != expected_type: msg = \"Unexpected channel type for {} (expected {}, was {})\"\\ .format(deviceid, expected_type, self.device.channels[deviceid].channel_type) self.log.error(msg) raise ValueError(msg) return self.device.channels[deviceid]"}
{"text_id": "9829", "text": "docstring: def uniquify_reference_names(comp): py_typecheck.check_type(comp, building_blocks.ComputationBuildingBlock) name_generator = building_block_factory.unique_name_generator(None) class _RenameNode(transformation_utils.BoundVariableTracker): def __init__(self, name, value): super(_RenameNode, self).__init__(name, value) py_typecheck.check_type(name, str) self.new_name = next(name_generator) def __str__(self): return 'Value: {}, name: {}, new_name: {}'.format(self.value, self.name, self.new_name) def _transform(comp, context_tree): if isinstance(comp, building_blocks.Reference): payload = context_tree.get_payload_with_name(comp.name) if payload is None: return comp, False new_name = payload.new_name return building_blocks.Reference(new_name, comp.type_signature, comp.context), True elif isinstance(comp, building_blocks.Block): new_locals = [] for name, val in comp.locals: context_tree.walk_down_one_variable_binding() new_name = context_tree.get_payload_with_name(name).new_name new_locals.append((new_name, val)) return building_blocks.Block(new_locals, comp.result), True elif isinstance(comp, building_blocks.Lambda): if comp.parameter_type is None: return comp, False context_tree.walk_down_one_variable_binding() new_name = context_tree.get_payload_with_name( comp.parameter_name).new_name return building_blocks.Lambda(new_name, comp.parameter_type, comp.result), True return comp, False symbol_tree = transformation_utils.SymbolTree(_RenameNode) return transformation_utils.transform_postorder_with_symbol_bindings( comp, _transform, symbol_tree)"}
{"text_id": "9830", "text": "docstring: def dp_maximum(data, candidates=None, lower=None, upper=None, mechanism=\"Automatic\", privacy_usage=None, **kwargs): return Component( \"DPMaximum\", arguments={ 'data': Component.of(data), 'candidates': Component.of(candidates), 'lower': Component.of(lower), 'upper': Component.of(upper) }, options={ 'mechanism': mechanism, 'privacy_usage': serialize_privacy_usage(privacy_usage) }, constraints=kwargs)"}
{"text_id": "9831", "text": "docstring: def check_goal(self): if self.robotX == self.GoalX and self.robotY == self.GoalY: return True return False"}
{"text_id": "9832", "text": "docstring: def update_metrics(self): self.checkpoint() if ( self.divergence_monitoring_steps and self.steps >= self.divergence_monitoring_steps and self.mean_reward <= self.best_reward ): self.plateau_count += 1 if self.plateau_count >= self.plateau_reduce_patience: current_lr, new_lr = None, None for model in self.output_models: current_lr = model.optimizer.learning_rate new_lr = current_lr * self.plateau_reduce_factor self.display_message( f'Learning rate reduced {current_lr.numpy()} ' f'-> {new_lr.numpy()}' ) current_lr.assign(new_lr) self.plateau_count = 0 self.early_stop_count += 1 self.frame_speed = (self.steps - self.last_reset_step) / ( perf_counter() - self.last_reset_time ) self.last_reset_step = self.steps self.mean_reward = np.around( np.mean(self.total_rewards), self.display_precision )"}
{"text_id": "9833", "text": "docstring: def size(self, size): if not type(size) is int: raise TypeError('size must be an integer') if size < 0: raise ValueError('size must be >= 0') self.__size = size"}
{"text_id": "9834", "text": "docstring: def turn_off(self, **kwargs): self.robot.pause_cleaning() time.sleep(1) self.robot.send_to_base()"}
{"text_id": "9835", "text": "docstring: def has_ports(self): return self._port_count > 0"}
{"text_id": "9836", "text": "docstring: def _options(self, resource): query = resource.get_query() rfilter = resource.rfilter if rfilter: join = rfilter.get_joins() left = rfilter.get_joins(left = True) else: join = left = None rfield = S3ResourceField(resource, self.field) field = rfield.field row = current.db(query).select(field.min(), field.max(), join = join, left = left, ).first() minimum = row[field.min()] maximum = row[field.max()] return minimum, maximum"}
{"text_id": "9837", "text": "docstring: def start_response_aux(status, headers, exc_info, response=None): response.status = str(status).split(' ',1)[0] response.headers = dict(headers) return lambda *args, **kargs: response.write(escape=False,*args,**kargs)"}
{"text_id": "9838", "text": "docstring: def tearDown(self): self.bq_client.delete_dataset( dataset=self.dataset_ref, delete_contents=True )"}
{"text_id": "9839", "text": "docstring: def publish_message(self): if self._message_number == 0: self._first_message_time = datetime.datetime.now() if self._stopping: logging.debug(\"Script is stopping, cannot publish message\") return if not self._connection.is_open: logging.debug(\"Connection is not open, cannot publish message\") return message = {'sequence':(self._message_number+1)} message_id = uuid.uuid4() if self._persistent: properties = pika.BasicProperties( app_id=os.path.basename(__file__), content_type='application/json', delivery_mode=2, message_id=(str(message_id)), timestamp=int(time.time())) else: properties = pika.BasicProperties( app_id=os.path.basename(__file__), content_type='application/json', message_id=(str(message_id)), timestamp=int(time.time())) self._channel.basic_publish(self._exchange, self._routing_key, json.dumps(message, ensure_ascii=False), properties) self._message_number += 1 self._delivery_tag += 1 self._deliveries.append(self._delivery_tag) logging.debug('Published message # %i, tag %i, id %s', self._message_number, self._delivery_tag, message_id) if self._message_number < self._max_messages: self.schedule_next_message() else: message_processing_time = datetime.datetime.now() - self._first_message_time logging.info(\"%s messages sent in %s (%s/s)\", self._max_messages, message_processing_time, round(self._max_messages/message_processing_time.total_seconds(), 2)) self.stop()"}
{"text_id": "9840", "text": "docstring: def print_running_jobs(self): print(\"Running jobs:\") now = time.time() for job in self.running_jobs: print(job, \"running for %i seconds\" % (now - job.start_time))"}
{"text_id": "9841", "text": "docstring: async def authenticate(self, request): state = self.check(request, allow_session=True) if state is None: return AuthCredentials([]), UnauthenticatedUser() return state.credentials, state.authenticated_user"}
{"text_id": "9842", "text": "docstring: def _write_message_with_feedback_service(self, notification, devices, chunk_size): self.validate_notification_and_chunck_size(notification, chunk_size) sent_count = 0 deactivated_count = 0 chunks = self.split_devices_into_chunks(devices, chunk_size) error_msg = '' for chunk_num, chunk in enumerate(chunks, 1): try: chunk_sent_count, chunk_deactivated_count = self.send_chunk(chunk=chunk, notification=notification) sent_count += chunk_sent_count deactivated_count += chunk_deactivated_count except Exception as error: logger.error(\"Error sending push notification.\", exc_info=sys.exc_info()) error_msg += \"Notification chunk #%s has failed. \\n\\n %s.\\n\\n\" % (chunk_num, error) self.set_last_sent_time(notification) service = FeedbackService.objects.get(apn_service=notification.service) try: num_deactivated = service.call() deactivated_count += num_deactivated except Exception as error: logger.error(\"Error sending push notification.\", exc_info=sys.exc_info()) error_msg += \"The final feedback service has failed. \\n\\n %s\" % error return sent_count, deactivated_count, error_msg"}
{"text_id": "9843", "text": "docstring: def electrolyzer_ratio_rule(model, t): expr = 0 expr += model.flow[busses[self.bus_el], self.model_th, t] expr += - model.flow[busses[self.bus_el], self.model_h2, t] return (expr == 0)"}
{"text_id": "9844", "text": "docstring: def run(args): sim = SequentialLife(args.width, args.height) if args.file: sim.load(args.file) else: sim.randomize(args.seed) runner = sim.animate if args.animate else sim.run runner = sprofile(runner) if args.profile else runner runner(steps=args.steps)"}
{"text_id": "9845", "text": "docstring: def student_home(): error = None if request.method == 'POST': _instr = request.form['submit_button'] if _instr == 'view_my': id = session['user_id'] return redirect(url_for(\"view\", id = id)) elif _instr == 'sign_up': return redirect(url_for(\"login\")) elif _instr == 'view_daily': return redirect(url_for(\"login\")) elif _instr == 'view_agg': return redirect(url_for(\"login\")) else: error = \"Invalid selection\" return render_template(\"student_home.html\", error = error) else: return render_template(\"student_home.html\", error = error)"}
{"text_id": "9846", "text": "docstring: def forward(self, x): out = self.layers(x) if self.shortcut is not None: out += self.shortcut(x) else: out += x return ( out if self.last_nonlinearity is None else self.last_nonlinearity(out) )"}
{"text_id": "9847", "text": "docstring: def notes(): session.forget(response) init() Books = BOOKS() ViewSettings = VIEWSETTINGS(Books) ViewSettings.initState() Note = NOTE(Books) return Note.page(ViewSettings)"}
{"text_id": "9848", "text": "docstring: def slope_function(slope, intercept, x): result = slope * x + intercept return result"}
{"text_id": "9849", "text": "docstring: def format_bibliography(json_data): json_data = _uniqueify(_flatten(json_data)) bib_source = CiteProcJSON(json_data) style_path = get_style_filepath('apa') bib_style = CitationStylesStyle(style_path, validate=False) bibliography = CitationStylesBibliography( bib_style, bib_source, formatter.html) for c in json_data: bibliography.register(Citation([CitationItem(c['id'])])) items = [] for item in bibliography.bibliography(): items.append(str(item)) return items"}
{"text_id": "9850", "text": "docstring: def can_stop_any_time_i_want_to(self) -> bool: if self.net_winnings < 0 and abs(self.net_winnings) > self.max_loss: self.history.append(\"lost more than max_loss\") return True if self.net_winnings > self.stop_at: self.history.append(\"won goal\") return True if self.tickets_played > self.max_tickets_bought: self.history.append(\"played enough tickets\") return True return False"}
{"text_id": "9851", "text": "docstring: def update_vision_pirate(self, pirate, offsets, delta): a_row, a_col = pirate.loc vision = self.vision[pirate.owner] for v_row, v_col in offsets: vision[a_row+v_row][a_col+v_col] += delta"}
{"text_id": "9852", "text": "docstring: def xlsx_write(self, worksheet, row, col): values = [self.id_no, datetime.strftime(self.orig_date, '%Y.%m.%d'), datetime.strftime(self.pay_due, '%Y.%m.%d'), self.total_sum] positions = config['invo_header_ident']['value'] row, col = list2row(worksheet, row, col, values, positions) return row, col"}
{"text_id": "9853", "text": "docstring: def command_for(self, *args: Any) -> str: if not len(args): raise TypeError(\"missing handler name as the first argument\") return self.handler_for(args[0]).command_for(*args)"}
{"text_id": "9854", "text": "docstring: def handle(infiles, tables, user_input_path, **kwargs): return handle_variables( metadata_path=user_input_path, tables=tables, table=TABLE, infiles=infiles, raw_variables=RAW_VARIABLES, write_data=write_data, outvar_name=VAR_NAME, outvar_units=VAR_UNITS, serial=kwargs.get('serial'), levels=LEVELS, logdir=kwargs.get('logdir'), simple=kwargs.get('simple'), outpath=kwargs.get('outpath'))"}
{"text_id": "9855", "text": "docstring: def fill_index(writer, recipes): for recipe in recipes: author = recipe.author.name if recipe.author else '' categories = ' '.join(c.name for c in recipe.categories) synonyms = ' '.join(s.name for s in recipe.synonyms) tags = '' for tag in recipe.tags: tags += tag.name + ' ' for synonym in tag.synonyms: tags += synonym.name + ' ' writer.update_document(category=categories, description=recipe.description, id=str(recipe.id), info=recipe.info, ingredients=recipe.ingredients, tags=tags, title=recipe.title+' '+synonyms, titletags=tags+recipe.title+' '+synonyms, rated=(recipe.rating is not -1), rating=recipe.rating, author=author) writer.commit()"}
{"text_id": "9856", "text": "docstring: def addRounds(self, numRounds=1): self.totalRounds += numRounds return self.totalRounds"}
{"text_id": "9857", "text": "docstring: def main(): kwds_query = [[(''.join((random.choice(string.ascii_lowercase) for _ in range(16)))) for _ in range(10)] for _ in range(NUM_QUERIES)] docs_published = [[(''.join((random.choice(string.ascii_lowercase) for _ in range(16)))) for _ in range(100)] for _ in range(NUM_DOCS)] mspsi_client = MSPSIClient(CURVE) mspsi_server = MSPSIServer(CURVE) pr = cProfile.Profile() pr.enable() (secret_server, published) = mspsi_server.publish(docs_published) pr.disable() pr.print_stats() queries = [] pr = cProfile.Profile() pr.enable() for i in range(NUM_QUERIES): queries.append(mspsi_client.query(kwds_query[i])) pr.disable() pr.print_stats() replies = [] pr = cProfile.Profile() pr.enable() for i in range(NUM_QUERIES): replies.append(mspsi_server.reply(secret_server, queries[i][1])) pr.disable() pr.print_stats() pr = cProfile.Profile() pr.enable() for i in range(NUM_QUERIES): mspsi_client.compute_cardinalities(queries[i][0], replies[i], published) pr.disable() pr.print_stats()"}
{"text_id": "9858", "text": "docstring: def __get_resource_undo_users(self, this_resource): if __debug__: assert isinstance(this_resource, BaseResource) if not self.user.is_active: raise PermissionDenied(\"Requesting user is not active\") candidates = UserResourcePrivilege.get_undo_users(resource=this_resource, grantor=self.user) if this_resource.raccess.owners.count() == 1: users_to_exclude = User.objects.filter(is_active=True, u2urp__resource=this_resource, u2urp__privilege=PrivilegeCodes.OWNER)\\ .values('pk') return candidates.exclude(pk__in=Subquery(users_to_exclude)) else: return candidates"}
{"text_id": "9859", "text": "docstring: def y_datalist(self) -> list: result = [] for _id in sorted(self.__elements.keys()): if self.__elements[_id] is not None: if self.__elements[_id].is_y: result.append(self.__elements[_id].data) return result"}
{"text_id": "9860", "text": "docstring: def stop(self, wait=True, wait_other_notice=False, gently=True): marks = [] if self.is_running(): if wait_other_notice: marks = [(node, node.mark_log()) for node in list(self.cluster.nodes.values()) if node.is_live() and node is not self] self._update_jmx_pid() if self._process_jmx and self._process_scylla: if gently: try: self._process_jmx.terminate() except OSError as e: pass try: self._process_scylla.terminate() except OSError as e: pass else: try: self._process_jmx.kill() except OSError as e: pass try: self._process_scylla.kill() except OSError as e: pass else: signal_mapping = {True: signal.SIGTERM, False: signal.SIGKILL} for pid in [self.jmx_pid, self.pid]: try: os.kill(pid, signal_mapping[gently]) except OSError: pass if wait_other_notice: for node, mark in marks: node.watch_log_for_death(self, from_mark=mark) else: time.sleep(.1) still_running = self.is_running() if still_running and wait: wait_time_sec = 1 for i in xrange(0, 7): time.sleep(wait_time_sec) if not self.is_running(): return True wait_time_sec *= 2 raise NodeError(\"Problem stopping node %s\" % self.name) else: return True else: return False"}
{"text_id": "9861", "text": "docstring: def print_and_log(templatePath, pdbDir, pdbPaths): print(\"-------------------\") print(\"## Superimposing ##\") print(\"-------------------\") print(\"\\nTemplate: \") print(\"\\t\" + templatePath) print(\"\\nSuperimposing \" + str(len(pdbPaths)) + \" conformations from directory: \" + pdbDir) date_str = time.strftime(\"%H:%M:%S\") time_str = time.strftime(\"%d/%m/%Y\") with open(pdbDir + \"/superimposed.log\", \"w\") as log_file: log_file.write(\"#\") log_file.write(\"## of superimposed conformations\\n\") log_file.write(\"#\") log_file.write(\"Date & Time: \" + date_str + \" -- \" + time_str + \"\\n\\n\") log_file.write(\"Template used:\\n\") log_file.write(\"\\t\" + templatePath + \"\\n\\n\") log_file.write(\"Conformations superimposed:\\n\") for pdb in pdbPaths: log_file.write(\"\\t\" + basename(pdb) + \"\\n\")"}
{"text_id": "9862", "text": "docstring: def handleLabels( cmdList ): trimmedCmdList = [] for i in range( len( cmdList ) ): cmd = cmdList[ i ] if cmd[ 0 ] == '(': label = cmd[ 1 : - 1 ] addr = i - len( knownAddresses_ProgramMemory ) knownAddresses_ProgramMemory[ '@{}'.format( label ) ] = '@{}'.format( addr ) else: trimmedCmdList.append( cmd ) return trimmedCmdList"}
{"text_id": "9863", "text": "docstring: def format_worksheets(worksheets): output = '-- Worksheets w/ Datasources %s\\n' % ('-'*(LINE_BIG-29)) for worksheet in worksheets: output += '-- %s\\n' % worksheet for source in worksheets[worksheet]: output += ' -- %s\\n' % source output += '\\n' output += '\\n'*2 return output"}
{"text_id": "9864", "text": "docstring: def _process_mor_objects_queue(self, instance): i_key = self._instance_key(instance) self.mor_cache.init_instance(i_key) if not self.mor_objects_queue.contains(i_key): self.log.debug(\"Objects queue is not initialized yet for instance {}, skipping processing\".format(i_key)) return for resource_type in RESOURCE_TYPE_METRICS: query_specs = [] batch_size = self.batch_morlist_size or self.mor_objects_queue.size(i_key, resource_type) while self.mor_objects_queue.size(i_key, resource_type): for _ in xrange(batch_size): mor = self.mor_objects_queue.pop(i_key, resource_type) if mor is None: self.log.debug(\"No more objects of type '{}' left in the queue\".format(resource_type)) break mor_name = str(mor['mor']) mor['interval'] = REAL_TIME_INTERVAL if mor['mor_type'] in REALTIME_RESOURCES else None self.mor_cache.set_mor(i_key, mor_name, mor) query_spec = vim.PerformanceManager.QuerySpec() query_spec.entity = mor[\"mor\"] query_spec.intervalId = mor[\"interval\"] query_spec.maxSample = 1 query_specs.append(query_spec) if query_specs: self.pool.apply_async(self._process_mor_objects_queue_async, args=(instance, query_specs))"}
{"text_id": "9865", "text": "docstring: def _fetch_all(self): if self._result_cache is None: self._result_cache = list(self.iterator()) if not isinstance(self, ValuesListQuerySet): for x in self._result_cache: self._set_query_time(x) if self._prefetch_related_lookups and not self._prefetch_done: self._prefetch_related_objects()"}
{"text_id": "9866", "text": "docstring: def to_dict(self) -> Dict: _dict = {} if hasattr(self, 'cscc') and self.cscc is not None: _dict['cscc'] = self.cscc if hasattr(self, 'lscc') and self.lscc is not None: _dict['lscc'] = self.lscc if hasattr(self, 'escc') and self.escc is not None: _dict['escc'] = self.escc if hasattr(self, 'vscc') and self.vscc is not None: _dict['vscc'] = self.vscc if hasattr(self, 'qscc') and self.qscc is not None: _dict['qscc'] = self.qscc return _dict"}
{"text_id": "9867", "text": "docstring: def account_history(brokerage: str = USER_BROKERAGE, access_token: str = USER_ACCESS_TOKEN, account_number: str = USER_ACCOUNT_NUMBER) -> dict: if brokerage == \"Tradier Inc.\": url = TR_BROKERAGE_API_URL elif brokerage == \"miscpaper\": url = TR_SANDBOX_BROKERAGE_API_URL else: logger.error('Oops! An error Occurred \u26a0\ufe0f') raise InvalidBrokerage response = requests.get(\"{}/v1/accounts/{}/history\".format( url, account_number), headers=tr_get_headers(access_token)) if response: return response.json() if response.status_code == 400: logger.error('Oops! An error Occurred \u26a0\ufe0f') raise BadRequest(response.text) if response.status_code == 401: logger.error('Oops! An error Occurred \u26a0\ufe0f') raise InvalidCredentials(response.text)"}
{"text_id": "9868", "text": "docstring: def six(d, Nnode, MaxSize=1e99): head = random.randint(0, Nnode - 1) path = [head] while True: nexts = list(d.successors(head)) next = random.choice(nexts) if next in path: i = path.index(next) if len(path[i:]) >= MaxSize: return [] return path[i:] path.append(next) head = next"}
{"text_id": "9869", "text": "docstring: def rivers_with_station(stations): output_set = set() for station in stations: output_set.add(station.river) return output_set"}
{"text_id": "9870", "text": "docstring: def matrix(m, specs): lines = [] prefix = \"\" fontsize = specs['fontsize'] if fontsize is not None: prefix += \"\\\\fontsize{%f}{%f}\\selectfont \" % (fontsize, fontsize * 1.2) for r in range(m.shape[0]): lines.append(\" & \".join( [value(el, specs, mathmode=True) for el in m[r, :]])) if specs['brackets']: return prefix + \"$ \\\\begin{pmatrix}\\n\" + \\ \" \\\\\\\\ \\n\".join(lines) + \"\\n \\end{pmatrix} $\\n\" else: return prefix + \"$ \\\\begin{pmatrix}\\n\" + \\ \" \\\\\\\\ \\n\".join(lines) + \"\\n \\end{pmatrix} $\\n\""}
{"text_id": "9871", "text": "docstring: def file_dir_map(self): return self._outdir"}
{"text_id": "9872", "text": "docstring: def associated_route_table_id(self) -> Optional[pulumi.Input[str]]: return pulumi.get(self, \"associated_route_table_id\")"}
{"text_id": "9873", "text": "docstring: def clustering_analysis(visualization=0, grade=False, trace=None): if trace is None: trace = prepare_trace() w = NotifyProgress('Clustering') w.show() try: try: if not trace.constant_propagation: trace = optimization_const_propagation(trace) if not trace.stack_addr_propagation: trace = optimization_stack_addr_propagation(trace) except: pass w.pbar_update(30) vr = find_virtual_regs(deepcopy(trace)) w.pbar_update(20) cluster = repetition_clustering(deepcopy(trace)) w.pbar_update(25) if visualization == 0: v0 = ClusterViewer(cluster, create_bb_diff, trace.ctx_reg_size, save_func=save) w.pbar_update(24) v0.Show() prev_ctx = defaultdict(lambda: 0) stack_changes = defaultdict(lambda: 0) for line in cluster: if isinstance(line, Traceline): prev_ctx = line.ctx else: stack_changes = create_cluster_gist(line, trace.ctx_reg_size, prev_ctx, stack_changes) prev_ctx = line[-1].ctx sorted_result = sorted(stack_changes.keys()) sorted_result.reverse() w.close() v1 = StackChangeViewer(vr, sorted_result, stack_changes) v1.Show() else: w.close() visualize_cli(cluster) except: w.close()"}
{"text_id": "9874", "text": "docstring: def timeline_home(self, max_id=None, since_id=None, limit=None): return self.timeline('home', max_id=max_id, since_id=since_id, limit=limit)"}
{"text_id": "9875", "text": "docstring: def real_space(self): for editor in ['erode/dil', 'open/close', 'blackhat/tophat']: self.opeditorscale[editor].config(state=tk.NORMAL, bg='red') self.itereditorscale[editor].config(state=tk.NORMAL, bg='green') self.editorlabels[editor].config()"}
{"text_id": "9876", "text": "docstring: def convert_to_binary(embedding_path): f = codecs.open(embedding_path + \".txt\", 'r', encoding='utf-8') wv = [] with codecs.open(embedding_path + \".vocab\", \"w\", encoding='utf-8') as vocab_write: count = 0 for line in f: if count == 0: count +=1 elif count>0 and count <300000: splitlines = line.split() if hasNumbers(splitlines[0].strip())== True: vocab_write.write(splitlines[0].strip()) vocab_write.write(\"\\n\") wv.append([float(val) for val in splitlines[1:]]) count +=1 else: break np.save(embedding_path + \".npy\", np.array(wv))"}
{"text_id": "9877", "text": "docstring: def url(self, query_name): return '{0}{1}'.format( self.mock_field('base_uri'), self.mock_field('api_urls').get(query_name), )"}
{"text_id": "9878", "text": "docstring: def QuadraticBezierCurves(*points): if len(points) > 1: points = np.asarray(points, dtype=np.float64) else: points = np.asarray(points[0], dtype=np.float64) assert len(points) > 2, \"QuadraticBezierCurves expects at least 3 points\" assert (len(points) % 2) == 1, (\"QuadraticBezierCurves expects an odd \" \"number of points\") return Join([ (1., QuadraticBezier(points[i], points[i+1], points[i+2])) for i in range(0, len(points)-2, 2) ])"}
{"text_id": "9879", "text": "docstring: def with_blob(self, blob): content = json.loads(blob.content) self.partition_id = content[\"partition_id\"] self.owner = content[\"owner\"] self.token = content[\"token\"] self.epoch = content[\"epoch\"] self.offset = content[\"offset\"] self.sequence_number = content[\"sequence_number\"] self.event_processor_context = content.get(\"event_processor_context\")"}
{"text_id": "9880", "text": "docstring: def predict( self, data=None, group_membership=True, probability=False, logit=False, add_to_data=False, ): name_memb = f'{self.dependent_variable} (predicted)' name_prob = f'{self.dependent_variable} (predicted prob.)' name_logit = f'{self.dependent_variable} (predicted logit)' all_columns = [name_memb, name_prob, name_logit] columns_to_show = [] if group_membership: columns_to_show.append(name_memb) if probability: columns_to_show.append(name_prob) if logit: columns_to_show.append(name_logit) cutoff = self.classification_cutoff if data is None: data_init = self._data.copy() logit = self._model.fittedvalues prob = logit.apply(lambda x: np.exp(x) / (1 + np.exp(x))) memb = prob.apply(lambda x: 1 if x >= cutoff else 0).map(self._inv_mapper) result = pd.DataFrame(index=self._observations_idx, columns=all_columns) result[name_memb] = memb result[name_prob] = prob result[name_logit] = logit result = result[columns_to_show] if add_to_data: return pd.concat([data_init, result], axis=1) else: return result else: aux_model = logit(self.formula, data).fit(**self._model_params) aux_data_idx = aux_model.fittedvalues.index aux_data_cols = aux_model.model.exog_names aux_data_cols = [BinaryLogisticRegression._translate_from_patsy_notation(x)\\ for x in aux_data_cols] aux_data = pd.DataFrame(aux_model.model.exog, index=aux_data_idx, columns=aux_data_cols) aux_X = add_constant(aux_data[self.variables_included].copy()) aux_y = aux_model.model.endog.copy() aux_model = Logit(aux_y, aux_X, missing='drop').fit(**self._model_params) logit = aux_model.fittedvalues prob = logit.apply(lambda x: np.exp(x) / (1 + np.exp(x))) memb = prob.apply(lambda x: 1 if x >= cutoff else 0).map(self._inv_mapper) result = pd.DataFrame(index=aux_data_idx, columns=all_columns) result[name_memb] = memb result[name_prob] = prob result[name_logit] = logit result = result[columns_to_show] if add_to_data: return pd.concat([data, result], axis=1) else: return result"}
{"text_id": "9881", "text": "docstring: def websocket_node_metadata(hass, connection, msg): manager = hass.data[DOMAIN][MANAGER] node = manager.get_instance(msg[OZW_INSTANCE]).get_node(msg[NODE_ID]) connection.send_result( msg[ID], { \"metadata\": node.meta_data, NODE_ID: node.node_id, OZW_INSTANCE: msg[OZW_INSTANCE], }, )"}
{"text_id": "9882", "text": "docstring: def update_experiment_id(self, experiment_id: str) -> None: self.ca.caput(self._simrbpv, experiment_id) self.ca.caput(self._daerbpv, experiment_id) names = [] surnames = [] orgs = [] if not self._db.experiment_exists(experiment_id): self.ca.caput(self._simnames, self.encode_for_return(names)) self.ca.caput(self._surnamepv, self.encode_for_return(surnames)) self.ca.caput(self._orgspv, self.encode_for_return(orgs)) raise Exception(\"error finding the experiment: %s\" % experiment_id) if self._db is not None: teammembers = self._db.get_team(experiment_id) for member in teammembers: fullname = six.text_type(member[0]) org = six.text_type(member[1]) role = six.text_type(member[2]) if not role == \"Contact\": surnames.append(self._get_surname_from_fullname(fullname)) orgs.append(org) name = User(fullname, org, role.lower()) names.append(name.__dict__) orgs = list(set(orgs)) self.ca.caput(self._simnames, self.encode_for_return(names)) self.ca.caput(self._surnamepv, self.encode_for_return(surnames)) self.ca.caput(self._orgspv, self.encode_for_return(orgs)) self.ca.caput(self._daenamespv, ExpData.make_name_list_ascii(surnames))"}
{"text_id": "9883", "text": "docstring: def log_check_warnings(self): self._log_check_warnings_object(self._info) self._log_check_warnings_object(self._tags) self._log_check_warnings_object(self._schemes) self._log_check_warnings_object(self._paths) self._log_check_warnings_object(self._securityDefinitions) self._log_check_warnings_object(self._definitions) pass"}
{"text_id": "9884", "text": "docstring: def _update_optional_commontypes_pack_dependencies(packs_found_from_incident_fields_or_types: set) -> list: common_types_pack_dependency = False if COMMON_TYPES_PACK in packs_found_from_incident_fields_or_types: packs_found_from_incident_fields_or_types.remove(COMMON_TYPES_PACK) common_types_pack_dependency = True pack_dependencies_data = PackDependencies._label_as_optional(packs_found_from_incident_fields_or_types) if common_types_pack_dependency: pack_dependencies_data.extend(PackDependencies._label_as_mandatory({COMMON_TYPES_PACK})) return pack_dependencies_data"}
{"text_id": "9885", "text": "docstring: def isnan(x, name=None): if in_dygraph_mode(): return _C_ops.final_state_isnan( x ) if _in_legacy_dygraph(): return _C_ops.isnan_v2(x) helper = LayerHelper(\"isnan_v2\", **locals()) check_variable_and_dtype(x, 'x', ['float16', 'float32', 'float64', 'int32', 'int64'], 'isnan') out = helper.create_variable_for_type_inference(dtype='bool') helper.append_op(type=\"isnan_v2\", inputs={\"X\": x}, outputs={\"Out\": out}) return out"}
{"text_id": "9886", "text": "docstring: def mousePressEvent(self, event: QGraphicsSceneMouseEvent): self._viewroot.clearSelectionsIfActiveTool() self.unsetActiveVirtualHelixItem() return QGraphicsItem.mousePressEvent(self, event)"}
{"text_id": "9887", "text": "docstring: def update_part(doc: str): stock_entry = frappe.get_doc(\"Stock Entry\", doc) service_orders_affected = [] for line in stock_entry.items: if line.part_list: transferred_qty = frappe.db.sql(f\"\"\"\\ select ifnull(sum(transfer_qty), 0) total from `tabStock Entry Detail` where docstatus = 1 and part_list = '{line.part_list}';\"\"\")[0][0] part_list = frappe.get_doc(\"Part List\", line.part_list) part_list.released_qty = transferred_qty if part_list.released_qty == part_list.qty: part_list.status = \"Released\" elif part_list.released_qty > 0: part_list.status = \"Partially Released\" else: part_list.status = \"Requested\" part_list.save(ignore_permissions=True) service_order = frappe.get_doc(\"Service Order\", part_list.parent) service_order.update_modified() if transferred_qty: comment = f\"has released {transferred_qty} {part_list.part}: {part_list.part_name}.\" else: comment = f\"has reversed all transfers of {part_list.part}: {part_list.part_name} for this order.\" service_order = frappe.get_doc(\"Service Order\", part_list.parent) service_order.add_comment( comment_type=\"Info\", text=comment, link_doctype=\"Material Request\", link_name=part_list.request ) if part_list.parent not in service_orders_affected: service_orders_affected.append(part_list.parent) if stock_entry.docstatus == 1: frappe.publish_realtime( event=\"msgprint\", message=f\"{line.transfer_qty} {part_list.part} are ready for collection.\", user=part_list.owner ) return service_orders_affected"}
{"text_id": "9888", "text": "docstring: def acosh(z): return AlComplex.from_python_complex(cm.acosh(z.to_python_complex()))"}
{"text_id": "9889", "text": "docstring: def ensure(self, method, retry=None, recoverable_error_callback=None, error_callback=None, timeout_is_error=True): current_pid = os.getpid() if self._initial_pid != current_pid: LOG.warn(\"Process forked after connection established! \" \"This can result in unpredictable behavior. \" \"See: http://docs.openstack.org/developer/\" \"oslo_messaging/transport.html\") self._initial_pid = current_pid if retry is None: retry = self.max_retries if retry is None or retry < 0: retry = None def on_error(exc, interval): LOG.debug(_(\"Received recoverable error from kombu:\"), exc_info=True) recoverable_error_callback and recoverable_error_callback(exc) interval = (self.driver_conf.kombu_reconnect_delay + interval if self.driver_conf.kombu_reconnect_delay > 0 else interval) info = {'err_str': exc, 'sleep_time': interval} info.update(self.connection.info()) if 'Socket closed' in six.text_type(exc): LOG.error(_LE('AMQP server %(hostname)s:%(port)d closed' ' the connection. Check login credentials:' ' %(err_str)s'), info) else: LOG.error(_LE('AMQP server on %(hostname)s:%(port)d is ' 'unreachable: %(err_str)s. Trying again in ' '%(sleep_time)d seconds.'), info) if self.driver_conf.kombu_reconnect_delay > 0: time.sleep(self.driver_conf.kombu_reconnect_delay) def on_reconnection(new_channel): self._set_current_channel(new_channel) self.consumer_num = itertools.count(1) for consumer in self.consumers: consumer.reconnect(new_channel) LOG.info(_LI('Reconnected to AMQP server on ' '%(hostname)s:%(port)d'), {'hostname': self.connection.hostname, 'port': self.connection.port}) def execute_method(channel): self._set_current_channel(channel) method() recoverable_errors = (self.connection.recoverable_channel_errors + self.connection.recoverable_connection_errors) try: autoretry_method = self.connection.autoretry( execute_method, channel=self.channel, max_retries=retry, errback=on_error, interval_start=self.interval_start or 1, interval_step=self.interval_stepping, on_revive=on_reconnection, ) ret, channel = autoretry_method() self._set_current_channel(channel) return ret except recoverable_errors as exc: LOG.debug(_(\"Received recoverable error from kombu:\"), exc_info=True) error_callback and error_callback(exc) self._set_current_channel(None) msg = _('Unable to connect to AMQP server on ' '%(hostname)s:%(port)d after %(retry)d ' 'tries: %(err_str)s') % { 'hostname': self.connection.hostname, 'port': self.connection.port, 'err_str': exc, 'retry': retry} LOG.error(msg) raise exceptions.MessageDeliveryFailure(msg) except Exception as exc: error_callback and error_callback(exc) raise"}
{"text_id": "9890", "text": "docstring: def heapSortNonAscending(self, A, n): self.buildHeap(A, n) for i in range(n): self.H[1], self.H[self.size] = self.H[self.size], self.H[1] A[i] = self.H[self.size] self.size -= 1 self.siftDown(1)"}
{"text_id": "9891", "text": "docstring: def handle_MissingOrInvalidParametersError(f): @functools.wraps(f) def wrapper(self, req, resp, **kwargs): try: return f(self, req, resp, **kwargs) except MissingOrInvalidParametersError as e: resp.status = falcon.HTTP_400 notifications = [] if e.missing_parameters: notifications.append(\"Missing required parameters: {}\".format( \", \".join(e.missing_parameters))) if e.invalid_parameters: notifications.append( \"Invalid values for parameters: {}\".format( \", \".join(e.invalid_parameters))) resp.body = \"\\n\\n\".join(notifications) return wrapper"}
{"text_id": "9892", "text": "docstring: def list_repositories(user_name, page_link=None): auth = get_auth() result = requests.get( page_link or f'https://api.github.com/users/{user_name}/repos', auth=auth ) rate_limit_update(result.headers) check_for_errors(result) if 'link' in result.headers.keys(): links = links_from_header.extract(result.headers['link']) else: links = {} return result.json(), links"}
{"text_id": "9893", "text": "docstring: def expand_aliases(self, ds_filter=None): new_base = self.base.replace_layers_with_aliases(self.op_label_aliases) new_fidpairs = _collections.OrderedDict() for coords, (prep, meas) in self.fidpairs.items(): prep2 = prep.replace_layers_with_aliases(self.op_label_aliases) meas2 = meas.replace_layers_with_aliases(self.op_label_aliases) if ds_filter is None or prep2 + new_base + meas2 in ds_filter: new_fidpairs[coords] = (prep2, meas2) return FiducialPairPlaquette(new_base, new_fidpairs, self.num_rows, self.num_cols, op_label_aliases=None)"}
{"text_id": "9894", "text": "docstring: def data_ready(self): self._send_command(self.DATA_READY, cmd_delay=0.001) self._read_reply(self._buffer, 3) return not ((self._buffer[0] & 0x03 == 0) and (self._buffer[1] == 0))"}
{"text_id": "9895", "text": "docstring: def ExampleSerialClassForUrl(url): return url, Serial"}
{"text_id": "9896", "text": "docstring: def wait_until_live(self): status = False while not status: try: status = self.server_live() except exceptions.ConnectionError: time.sleep(1)"}
{"text_id": "9897", "text": "docstring: def post_sys_policies_egp_name(self, name, **kwargs): \"\"\"Read, Modify, or Delete an access control policy. This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.post_sys_policies_egp_name(name, async_req=True) >>> result = thread.get() :param async_req bool :param str name: The name of the policy. Example: \\\"ops\\\" (required) :param Body77 body: :return: None If the method is called asynchronously, returns the request thread. \"\"\" kwargs['_return_http_data_only'] = True if kwargs.get('async_req'): return self.post_sys_policies_egp_name_with_http_info(name, **kwargs) else: (data) = self.post_sys_policies_egp_name_with_http_info(name, **kwargs) return data"}
{"text_id": "9898", "text": "docstring: def _getDecoderFPropMetrics(self, params): dec = params.Instantiate() src_seq_len = 5 src_enc = tf.random.normal([src_seq_len, 2, 8], seed=982774838, dtype=py_utils.FPropDtype(params)) src_enc_padding = tf.constant( [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 1.0], [1.0, 1.0]], dtype=py_utils.FPropDtype(params)) encoder_outputs = py_utils.NestedMap( encoded=src_enc, padding=src_enc_padding) target_ids = tf.transpose( tf.constant([[0, 1, 2, 3], [1, 2, 3, 4], [10, 11, 12, 15], [5, 6, 7, 8], [10, 5, 2, 5]], dtype=tf.int32)) target_labels = tf.transpose( tf.constant([[0, 1, 2, 3], [1, 2, 3, 4], [10, 11, 12, 13], [5, 7, 8, 10], [10, 5, 2, 4]], dtype=tf.int32)) target_paddings = tf.transpose( tf.constant([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [1, 1, 1, 0]], dtype=py_utils.FPropDtype(params))) target_transcripts = tf.constant(['abcd', 'bcde', 'klmp', 'fghi', 'kfcf']) target_weights = 1.0 - target_paddings targets = py_utils.NestedMap({ 'ids': target_ids, 'labels': target_labels, 'weights': target_weights, 'paddings': target_paddings, 'transcripts': target_transcripts, }) decoder_outputs = dec.FPropDefaultTheta(encoder_outputs, targets) return decoder_outputs.metrics, decoder_outputs.per_sequence['loss']"}
{"text_id": "9899", "text": "docstring: def read_ecosystems_from_bucket(self, bucket_name): bucket = self.s3_resource.Bucket(self.full_bucket_name(bucket_name)) result = bucket.meta.client.list_objects(Bucket=bucket.name, Delimiter='/') names = [o.get('Prefix') for o in result.get('CommonPrefixes')] return [name[:-1] for name in names]"}
{"text_id": "9900", "text": "docstring: def save(self): self.store.animationAttrs( self.source.getKeys(), self.node ) snapshot = self.source.getSnapshot() self.sourceKeyTransforms() self.store.storeCoreData( self.node, snapshot, self.snapshotName )"}
{"text_id": "9901", "text": "docstring: def load_models(module): modules = _import_submodules(module) return {m for module in modules for m in filter( _check_model, (getattr(module, name) for name in dir(module)) )}"}
{"text_id": "9902", "text": "docstring: def new_entr(self): self.invo_list[-1] += 1"}
{"text_id": "9903", "text": "docstring: def register(): form = RegistrationForm() if form.validate_on_submit(): user = User(email = form.email.data, username =form.username.data, password=form.password.data) db.session.add(user) db.session.commit() mail_message('Welcome to Pitcher','email/welcome_user',user.email,user=user) return redirect(url_for('auth.login')) title = \"New Account\" return render_template('/auth/register.html', registration_form =form, title=title)"}
{"text_id": "9904", "text": "docstring: def load_dbd_derived(fn): import pandas as pd df = pd.read_csv(fn, sep='\\t', encoding='latin1', comment='#', na_filter=False) group = None for idx, info in df.sort_values(by=['promptgroup', 'prompt']).iterrows(): field = info['field'] if 'DBF_NOACCESS' in info['type']: continue prompt = info['prompt'] short_desc = prompt attr_name = short_desc.lower() attr_name = re.sub('[^A-Za-z_0-9]', '_', attr_name) attr_name = re.sub('_+', '_', attr_name) attr_name = re.sub('^_', '', attr_name) attr_name = re.sub('_$', '', attr_name) group = info['promptgroup'].replace('GUI_', '') group = group.lower() if 'SPC_NOMOD' in info['special']: cls = 'EpicsSignalRO' else: cls = 'EpicsSignal' yield dict(cls=cls, field=field, short_desc=short_desc, attr_name=attr_name, doc=prompt, group='\"{}\"'.format(group), type=info['type'], )"}
{"text_id": "9905", "text": "docstring: def error(self, lines, xml=None): if xml is not None: lines.extend(xml.xml(indent = u'yes', omitXmlDeclaration = u'yes').split('\\n')) raise TranslationError('\\n'.join(lines))"}
{"text_id": "9906", "text": "docstring: def totalcounts_pertime(self): self.has_constant_totalcounts_pertime key = list(self.keys())[0] totalcountspertime = self[key].totalcounts_per_timestep return totalcountspertime"}
{"text_id": "9907", "text": "docstring: def auth_cookie(test_config): try: get_auth() except RuntimeError: init_auth( test_config.metador.site, test_config.orcid, test_config.metador.data_dir ) cookies = {} cookies[\"session_id\"] = get_auth().new_session(MOCK_TOKEN) return cookies"}
{"text_id": "9908", "text": "docstring: def min_interval_ms(self) -> float: return MIN_ADVERTISING_INTERVAL_MS"}
{"text_id": "9909", "text": "docstring: def accumulatedTorque(self): return (self.msg.content[7] << 8) | self.msg.content[6]"}
{"text_id": "9910", "text": "docstring: def add_animals(self, animal_dist): animal_classes = {'cat': Cat, 'dinosaur': Dinosaur, 'hippo': Hippo, 'rabbit': Rabbit, 'snail': Snail} self.entities.remove(self.animals.sprites()) self.animals.empty() for animal, count in animal_dist.iteritems(): for i in xrange(count): d = animal_classes[animal](self) d.scale_image(.115) self.add_entity(d)"}
{"text_id": "9911", "text": "docstring: def to_dict(self, registry: Optional[\"Registry\"] = None) -> Dict[str, Any]: if registry is None: registry = st.registry nodes = {} for node in self.graph.nodes: data = self.graph.nodes[node] type_serializer = registry.get_type_serializer(data[\"type\"]) state_dict = None if data[\"state\"]: state_dict = data[\"state\"].to_dict(registry) del state_dict[\"state\"][\"output_type\"] nodes[node] = { \"data\": data[\"value\"], \"type\": type_serializer.serialize(data[\"type\"]), \"state\": state_dict, \"depends_on\": list(self.graph.pred[node]), } return nodes"}
{"text_id": "9912", "text": "docstring: def connect(self, address, conntype=None): host, port = address if conntype is None: conntype = _the_interface.TCP_MODE if not _the_interface.socket_connect(self._socknum, host, port, conn_mode=conntype): raise RuntimeError(\"Failed to connect to host\", host) self._buffer = b''"}
{"text_id": "9913", "text": "docstring: def _EnsureSSHKeyExistsForUser(self, fetcher, user): public_key = self.keys.GetPublicKey() should_upload = True try: user_info = fetcher.LookupUser(user) except user_client.UserException: owner_email = gaia.GetAuthenticatedGaiaEmail(self.http) fetcher.CreateUser(user, owner_email) user_info = fetcher.LookupUser(user) for remote_public_key in user_info.publicKeys: if remote_public_key.key.rstrip() == public_key: expiration_time = remote_public_key.expirationTimestamp if expiration_time and time_util.IsExpired(expiration_time): fetcher.RemovePublicKey( user_info.name, remote_public_key.fingerprint) else: should_upload = False break if should_upload: fetcher.UploadPublicKey(user, public_key) return True"}
{"text_id": "9914", "text": "docstring: def ensure_volume_access_right_absent( user_email, volume_name ): client = connect_syndicate() return syndicate_provisioning.ensure_volume_access_right_absent( client, user_email, volume_name )"}
{"text_id": "9915", "text": "docstring: def redraw_path(self, wp_vertices=None): if wp_vertices is None: wp_vertices = self.pathpatch.get_path().wp_vertices if len(wp_vertices) == 0: raise IOError(\"mscolab session expired\") vertices = self.pathpatch.get_path().vertices else: x, y = list(zip(*wp_vertices)) lons, lats = self.map(x, y, inverse=True) x, y = self.map.gcpoints_path(lons, lats) vertices = list(zip(x, y)) self.line.set_data(list(zip(*vertices))) wp_heights = [(wp.flightlevel * 0.03048) for wp in self.waypoints_model.all_waypoint_data()] wp_times = [wp.utc_time for wp in self.waypoints_model.all_waypoint_data()] if self.tangent_lines is not None: self.tangent_lines.remove() if self.show_tangent_points: assert self.remote_sensing is not None self.tangent_lines = self.remote_sensing.compute_tangent_lines( self.map, wp_vertices, wp_heights) self.ax.add_collection(self.tangent_lines) else: self.tangent_lines = None if self.solar_lines is not None: self.solar_lines.remove() if self.show_solar_angle is not None: assert self.remote_sensing is not None self.solar_lines = self.remote_sensing.compute_solar_lines( self.map, wp_vertices, wp_heights, wp_times, self.show_solar_angle) self.ax.add_collection(self.solar_lines) else: self.solar_lines = None if self.wp_scatter is not None: self.wp_scatter.remove() x, y = list(zip(*wp_vertices)) if self.map.projection == \"cyl\": x = np.array(x) x[x < self.map.llcrnrlon] += 360 x[x > self.map.urcrnrlon] -= 360 self.wp_scatter = self.ax.scatter( x, y, color=self.markerfacecolor, s=20, zorder=3, animated=True, visible=self.show_marker) label_offset = self.appropriate_epsilon(px=5) for wp_label in self.wp_labels: wp_label.remove() self.wp_labels = [] wpd = self.waypoints_model.all_waypoint_data() for i in range(len(wpd)): textlabel = str(i) if wpd[i].location != \"\": textlabel = f\"{wpd[i].location:}\" t = self.ax.text( x[i] + label_offset, y[i] + label_offset, textlabel, bbox={\"boxstyle\": \"round\", \"facecolor\": \"white\", \"alpha\": 0.6, \"edgecolor\": \"none\"}, fontweight=\"bold\", zorder=4, animated=True, clip_on=True, visible=self.showverts and self.label_waypoints) self.wp_labels.append(t) if self.background: self.canvas.restore_region(self.background) try: self.ax.draw_artist(self.pathpatch) except ValueError as error: logging.debug(\"ValueError Exception '%s'\", error) self.ax.draw_artist(self.line) self.ax.draw_artist(self.wp_scatter) for t in self.wp_labels: self.ax.draw_artist(t) if self.show_tangent_points: self.ax.draw_artist(self.tangent_lines) if self.show_solar_angle is not None: self.ax.draw_artist(self.solar_lines) self.canvas.blit(self.ax.bbox)"}
{"text_id": "9916", "text": "docstring: def load(self): self.mock_data = \"droplets/single.json\" droplets = self.get_data(\"droplets/%s\" % self.id) droplet = droplets['droplet'] for attr in droplet.keys(): setattr(self, attr, droplet[attr]) for net in self.networks['v4']: if net['type'] == 'private': self.private_ip_address = net['ip_address'] if net['type'] == 'public': self.ip_address = net['ip_address'] if self.networks['v6']: self.ip_v6_address = self.networks['v6'][0]['ip_address'] return self"}
{"text_id": "9917", "text": "docstring: def check_adv(self, x, label): y_pred = self.dknn.classify(x).argmax(1) return torch.tensor((y_pred != label).astype(np.float32)).to(self.device)"}
{"text_id": "9918", "text": "docstring: def create_root_folders(project_path): folders = ['inputs', 'outputs', 'metadata'] [os.mkdir(project_path + ifolder) for ifolder in folders]"}
{"text_id": "9919", "text": "docstring: def from_dict(cls, _dict: Dict) -> 'TransitConnection': args = {} if 'base_connection_id' in _dict: args['base_connection_id'] = _dict.get('base_connection_id') if 'created_at' in _dict: args['created_at'] = string_to_datetime(_dict.get('created_at')) else: raise ValueError('Required property \\'created_at\\' not present in TransitConnection JSON') if 'id' in _dict: args['id'] = _dict.get('id') else: raise ValueError('Required property \\'id\\' not present in TransitConnection JSON') if 'local_bgp_asn' in _dict: args['local_bgp_asn'] = _dict.get('local_bgp_asn') if 'local_gateway_ip' in _dict: args['local_gateway_ip'] = _dict.get('local_gateway_ip') if 'local_tunnel_ip' in _dict: args['local_tunnel_ip'] = _dict.get('local_tunnel_ip') if 'mtu' in _dict: args['mtu'] = _dict.get('mtu') if 'name' in _dict: args['name'] = _dict.get('name') else: raise ValueError('Required property \\'name\\' not present in TransitConnection JSON') if 'network_account_id' in _dict: args['network_account_id'] = _dict.get('network_account_id') if 'network_id' in _dict: args['network_id'] = _dict.get('network_id') if 'network_type' in _dict: args['network_type'] = _dict.get('network_type') else: raise ValueError('Required property \\'network_type\\' not present in TransitConnection JSON') if 'remote_bgp_asn' in _dict: args['remote_bgp_asn'] = _dict.get('remote_bgp_asn') if 'remote_gateway_ip' in _dict: args['remote_gateway_ip'] = _dict.get('remote_gateway_ip') if 'remote_tunnel_ip' in _dict: args['remote_tunnel_ip'] = _dict.get('remote_tunnel_ip') if 'request_status' in _dict: args['request_status'] = _dict.get('request_status') if 'status' in _dict: args['status'] = _dict.get('status') else: raise ValueError('Required property \\'status\\' not present in TransitConnection JSON') if 'transit_gateway' in _dict: args['transit_gateway'] = TransitGatewayReference.from_dict(_dict.get('transit_gateway')) else: raise ValueError('Required property \\'transit_gateway\\' not present in TransitConnection JSON') if 'updated_at' in _dict: args['updated_at'] = string_to_datetime(_dict.get('updated_at')) if 'zone' in _dict: args['zone'] = ZoneReference.from_dict(_dict.get('zone')) return cls(**args)"}
{"text_id": "9920", "text": "docstring: def open_website(entry_id) -> None: bmrb_entry_url = get_bmrb_url('v2', 'entry', entry_id) headers = {'Application': 'nightshift'} with requests.get(bmrb_entry_url, headers=headers) as webpage_request: if not _got_bad_code(webpage_request): webbrowser.open(f'https://bmrb.io/data_library/summary/index.php?bmrbId={entry_id}')"}
{"text_id": "9921", "text": "docstring: def pama_translate(source: str, filename: str = '<string>'): if source is None: with open(filename) as f: source = f.read() scanner = syntax_support.TextScanner(filename, source) code = scanner.get_text() match_module = scanner.get_match_code() return code, match_module"}
{"text_id": "9922", "text": "docstring: def add_field_alias(self, field, alias): if alias in self.fields: raise KeyError(\"Field with name '%s' already exists!\" % alias) self.fields[alias] = field"}
{"text_id": "9923", "text": "docstring: async def commit_sending(self, id: str, data: bytes = None) -> dict: commit_tx_hash = None try: if self.private_key and not self.isUnlocked(self.minter): if data: transaction = self.contract.functions \\ .interledgerCommit(Web3.toInt(text=id), data) \\ .buildTransaction({'from': self.minter}) else: transaction = self.contract.functions \\ .interledgerCommit(Web3.toInt(text=id)) \\ .buildTransaction({'from': self.minter}) transaction.update({'nonce': self.web3.eth.getTransactionCount(self.minter)}) signed_tx = self.web3.eth.account.signTransaction(transaction, self.private_key) commit_tx_hash = self.web3.eth.sendRawTransaction(signed_tx.rawTransaction) elif self.password is not None: unlock = self.web3.geth.personal.unlockAccount(self.minter, self.password, 0) if not unlock: return {\"status\": False, \"error_code\": ErrorCode.TRANSACTION_FAILURE, \"message\": \"Wrong password\", \"commit_tx_hash\": None} if data: commit_tx_hash = self.contract.functions \\ .interledgerCommit(Web3.toInt(text=id), data) \\ .transact({'from': self.minter}) else: commit_tx_hash = self.contract.functions \\ .interledgerCommit(Web3.toInt(text=id)) \\ .transact({'from': self.minter}) self.web3.geth.personal.lockAccount(self.minter) else: if data: commit_tx_hash = self.contract.functions \\ .interledgerCommit(Web3.toInt(text=id), data) \\ .transact({'from': self.minter}) else: commit_tx_hash = self.contract.functions \\ .interledgerCommit(Web3.toInt(text=id)) \\ .transact({'from': self.minter}) tx_receipt = await asyncio.get_event_loop().run_in_executor( None, functools.partial(self.web3.eth.waitForTransactionReceipt, commit_tx_hash, timeout=self.timeout)) if tx_receipt['status']: return {\"commit_status\": True, \"commit_tx_hash\": commit_tx_hash} else: return {\"commit_status\": False, \"commit_error_code\": ErrorCode.TRANSACTION_FAILURE, \"commit_message\": \"Error in the transaction\", \"commit_tx_hash\": commit_tx_hash} except web3.exceptions.TimeExhausted as e: return {\"commit_status\": False, \"commit_error_code\": ErrorCode.TIMEOUT, \"commit_message\": \"Timeout after sending the transaction\", \"commit_tx_hash\": commit_tx_hash, \"exception\": e} except ValueError as e: d = eval(e.__str__()) return {\"commit_status\": False, \"commit_error_code\": ErrorCode.TRANSACTION_FAILURE, \"commit_message\": d['message'], \"commit_tx_hash\": commit_tx_hash, \"exception\": e}"}
{"text_id": "9924", "text": "docstring: def cleanup_resources_from_task_run( self, task_run: \"TaskRun\", server_url: str ) -> None: requester = cast(\"MTurkRequester\", task_run.get_requester()) session = self.datastore.get_session_for_requester(requester._requester_name) run_row = self.datastore.get_run(task_run.db_id) delete_sns_topic(session, run_row[\"arn_id\"])"}
{"text_id": "9925", "text": "docstring: def name(self): assert not self.is_tuple n = self._name_nodes[-1] assert n.type == token.NAME, repr(n) return n.value"}
{"text_id": "9926", "text": "docstring: def _write_cron_lines(user, lines): path = salt.utils.mkstemp() with salt.utils.fopen(path, 'w+') as fp_: fp_.writelines(lines) if __grains__['os'] == 'Solaris' and user != \"root\": __salt__['cmd.run']('chown {0} {1}'.format(user, path)) ret = __salt__['cmd.run_all'](_get_cron_cmdstr(user, path)) os.remove(path) return ret"}
{"text_id": "9927", "text": "docstring: def exchange_token(self, code): url = '%s%s/oauth2/token' % (self.scheme, self.host) options = { 'grant_type': 'authorization_code', 'redirect_uri': self._redirect_uri(), 'client_id': self.options.get('client_id'), 'client_secret': self.options.get('client_secret'), 'code': code, } options.update({ 'verify_ssl': self.options.get('verify_ssl', True), 'proxies': self.options.get('proxies', None) }) self.token = wrapped_resource( make_request('post', url, options)) self.access_token = self.token.access_token return self.token"}
{"text_id": "9928", "text": "docstring: def _tokenize_one(self, input_text): sep = self.sep out = [] if sep == \"\" or sep is None: for char in input_text: out.append(char) else: for part in input_text.split(sep): out.append(part) return out"}
{"text_id": "9929", "text": "docstring: def _flip(self): self.d = -self.d self.M = -self.M self.delta[:] = np.flipud(self.delta) self.rho[:] = np.flipud(self.rho) self.v[:] = np.flipud(self.v) self.u[:] = -np.flipud(self.u) self.U[:] = -np.flipud(self.U) self.p[:] = np.flipud(self.p) self.lamda_1[:] = np.flipud(self.lamda_1) self.lamda_2[:] = np.flipud(self.lamda_2) self.r_1[:] = np.flipud(self.r_1) self.r_2[:] = np.flipud(self.r_2) self.dr_1_drho[:] = np.flipud(self.dr_1_drho) self.dr_1_dp[:] = np.flipud(self.dr_1_dp) self.dr_1_dlamda_1[:] = np.flipud(self.dr_1_dlamda_1) self.dr_1_dlamda_2[:] = np.flipud(self.dr_1_dlamda_2) self.dr_2_drho[:] = np.flipud(self.dr_2_drho) self.dr_2_dp[:] = np.flipud(self.dr_2_dp) self.dr_2_dlamda_1[:] = np.flipud(self.dr_2_dlamda_1) self.dr_2_dlamda_2[:] = np.flipud(self.dr_2_dlamda_2) self.drho_dx[:] = -np.flipud(self.drho_dx) self.dv_dx[:] = -np.flipud(self.dv_dx) self.du_dx[:] = np.flipud(self.du_dx) self.dp_dx[:] = -np.flipud(self.dp_dx) self.dlamda_1_dx[:] = -np.flipud(self.dlamda_1_dx) self.dlamda_2_dx[:] = -np.flipud(self.dlamda_2_dx)"}
{"text_id": "9930", "text": "docstring: def _draw_page_header(self) -> None: self.pdf.setFont(\"Courier\", 10) self._draw_label(\"Printed By:\", color=\"blue\") today = timezone.now().date() us_format_date_string = today.strftime(\"%m/%d/%Y\") self._draw_label(label=\"Print Date:\", data=us_format_date_string) self.row_number += 1 self._draw_label(\"ATLANTA POLICE DEPARTMENT\", centered=True) self._draw_label(\"Offense Report\", centered=True) self._draw_label(f\"INCIDENT NUMBER: {self.incident.incident_number}\", centered=True) self.pdf.drawString(525, TOP_ALIGN_Y, f\"PAGE: {self.page_number}\")"}
{"text_id": "9931", "text": "docstring: def update_cached_validation_state(self, cert_chain: CertChain) -> None: self._cache[cert_chain.digest] = cert_chain.successful_validation"}
{"text_id": "9932", "text": "docstring: async def begin_migrate_mongo_db_database_to_autoscale( self, resource_group_name: str, account_name: str, database_name: str, **kwargs: Any ) -> AsyncLROPoller[\"_models.ThroughputSettingsGetResults\"]: api_version = kwargs.pop('api_version', \"2022-02-15-preview\") polling = kwargs.pop('polling', True) cls = kwargs.pop('cls', None) lro_delay = kwargs.pop( 'polling_interval', self._config.polling_interval ) cont_token = kwargs.pop('continuation_token', None) if cont_token is None: raw_result = await self._migrate_mongo_db_database_to_autoscale_initial( resource_group_name=resource_group_name, account_name=account_name, database_name=database_name, api_version=api_version, cls=lambda x,y,z: x, **kwargs ) kwargs.pop('error_map', None) def get_long_running_output(pipeline_response): response = pipeline_response.http_response deserialized = self._deserialize('ThroughputSettingsGetResults', pipeline_response) if cls: return cls(pipeline_response, deserialized, {}) return deserialized if polling is True: polling_method = AsyncARMPolling(lro_delay, **kwargs) elif polling is False: polling_method = AsyncNoPolling() else: polling_method = polling if cont_token: return AsyncLROPoller.from_continuation_token( polling_method=polling_method, continuation_token=cont_token, client=self._client, deserialization_callback=get_long_running_output ) return AsyncLROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"text_id": "9933", "text": "docstring: def _xml_error_count(self): return len([test for test in self._test_list if test.xml_error_elements])"}
{"text_id": "9934", "text": "docstring: def _boxelize(pixels, dims, palette=None): pixel_unit = dims.pixels_per_box if palette: pixels, pixel_indices = _palettize(pixels, palette) else: pixel_indices = np.asarray([], dtype=np.uint8) pixels.setflags(write=1) for i in xrange(dims.output_dims[0]): cols = pixels[:, i * pixel_unit:(i + 1) * pixel_unit] if (cols.shape[1] != 0): sections = np.resize( cols, (math.ceil(cols.shape[0]/pixel_unit), pixel_unit * pixel_unit, 3)) avg = np.percentile(sections, 50, axis=1, interpolation='lower') out = np.repeat(avg, pixel_unit * pixel_unit, axis=0) out_expand = np.resize(out, cols.shape) pixels[:, i * pixel_unit:(i + 1) * pixel_unit] = out_expand out = Image.fromarray(pixels, 'RGB') if palette: idxs = pixel_indices[0::dims.pixels_per_box, 0::dims.pixels_per_box] idxs.setflags(write=1) idxs[np.where(idxs >= palette.num_of_colors)] = palette.num_of_colors - 1 else: idxs = np.asarray([]) return out, idxs"}
{"text_id": "9935", "text": "docstring: def StationaryBootstrap(data: np.ndarray, m, sampleLength)-> np.ndarray: accept = 1/m lenData = data.shape[0] sampleIndex = np.random.randint(0,high =lenData,size=1); sample = np.zeros((sampleLength,1)) for iSample in range(sampleLength): if np.random.uniform(0,1,1)>=accept: sampleIndex += 1 if sampleIndex >= lenData: sampleIndex=0 else: sampleIndex = np.random.randint(0,high = lenData,size=1) sample[iSample,0] = data[sampleIndex] return sample"}
{"text_id": "9936", "text": "docstring: def insert(self, tenantid, serverid, data): windowsize = self.get_windowsize(tenantid) if isinstance(windowsize, list) and len(windowsize) == 1: windowsize = int(windowsize[0]) if isinstance(data, list) and len(data) == len(fact_attributes): self.r.rpush(tenantid + \".\" + serverid, data) self.r.ltrim(tenantid + \".\" + serverid, -int(windowsize), -1) else: return \"error\""}
{"text_id": "9937", "text": "docstring: def main(grid_size, discount, n_improvements = 5): wind = 0.3 trajectory_length = 3*grid_size gw = gridworld.Gridworld(grid_size, wind, discount) ground_r = np.array([gw.reward(s) for s in range(gw.n_states)]) prob_optimal = 0.0 rewards = [] policies = [] for i in range(n_improvements): policy = [gw.optimal_policy_improving(s,prob_optimal) for s in range(gw.n_states)] r = linear_irl.irl(gw.n_states, gw.n_actions, gw.transition_probability, policy, gw.discount, 1, 5) rewards.append(r) policies.append(policy) print(prob_optimal) prob_optimal += np.float(1/n_improvements) return rewards, policies"}
{"text_id": "9938", "text": "docstring: def plaqs4x4( self, x: Optional[Array] = None, wloops4x4: Optional[Array] = None ) -> Array: if wloops4x4 is None: if x is None: raise ValueError('One of `x` or `wloops` must be specified.') wloops4x4 = self.wilson_loops4x4(x) return self._plaqs4x4(wloops4x4)"}
{"text_id": "9939", "text": "docstring: def transform_name_to_id(name: str, lower: bool = True) -> str: duppl = False study_id = \"\" for c in name: if ( (c >= \"a\" and c <= \"z\") or (c >= \"A\" and c <= \"Z\") or (c >= \"0\" and c <= \"9\") or c == \"_\" or c == \"-\" or c == \"(\" or c == \")\" or c == \",\" or c == \"&\" or c == \" \" ): study_id += c duppl = False else: if not duppl: study_id += \" \" duppl = True study_id_stripped = study_id.strip() if lower: return study_id_stripped.lower() return study_id_stripped"}
{"text_id": "9940", "text": "docstring: def SelectNode(self, filts, top: CfgTopology, nl: Dict[str, Node]): if len(filts[1]) == 0: self.logger.error('No candidate nodes found!') return '' needsGpu = False for p in top.proc_groups: if len(p.group_gpus) > 0: needsGpu = True break if not needsGpu: for n in filts[1]: if nl[n].GetTotalGPUs() == 0: self.logger.info(f'Found node {n} without GPUs to pair with a CPU-only pod') return n node = filts[1][0] return node"}
{"text_id": "9941", "text": "docstring: def vp_start_gui(): global val, w, root root = tk.Tk() top = Toplevel1 (root) lms_support.init(root, top) root.mainloop()"}
{"text_id": "9942", "text": "docstring: def redis(self): try: from django_redis import get_redis_connection from redis.exceptions import ConnectionError except ImportError as e: logger.exception(e) return False, f\"redis module import fail, error: {str(e)}\" try: get_redis_connection(\"default\").ping() except ConnectionError as e: logger.exception(e) return False, f\"redis ping test fail, error: {str(e)}\" return True, \"ok\""}
{"text_id": "9943", "text": "docstring: def paintEvent(self, event): del event assert threading.currentThread().getName() == \"MainThread\" self.__colorsLock.lockForWrite() try: if not self.__colors: return colors = self.__colors finally: self.__colorsLock.unlock() painter = QtGui.QPainter(self) painter.save() try: rect = self.contentsRect() ratio = float(rect.height())/len(colors) shift = self.__sourceTree.verticalScrollBar().value() * ratio offPage = self.__sourceTree.verticalScrollBar().maximum() * ratio painter.drawPixmap(self.contentsRect(), self.__background, self.__background.rect()) pen = QtGui.QPen(self.__colorInvalid) pen.setWidth(0) painter.setPen(pen) painter.setBrush(self.__brushPattern) painter.drawRect(rect.adjusted(2, shift, -2, -offPage + shift)) finally: painter.restore() painter.end() del painter"}
{"text_id": "9944", "text": "docstring: def _get_message_range( lines: Sequence[bytes], after: Tuple[int, int], ) -> Mapping[str, Tuple[int, int]]: end_pos = _find_next_sentence(lines, *after) return {\"start\": after, \"stop\": end_pos}"}
{"text_id": "9945", "text": "docstring: def _involuted_reverse(self): def inv_is_top(si): return (si.stride == 1 and self._lower_bound == StridedInterval._modular_add(self._upper_bound, 1, self.bits) ) o = self.copy() o._reversed = not o._reversed if o.bits == 8: return o.copy() if inv_is_top(o): si = o.copy() return si else: lb = o._lower_bound ub = o._upper_bound rounded_bits = ((o.bits + 7) // 8) * 8 lb_r = [] ub_r = [] for i in range(0, rounded_bits, 8): if i != 0: lb = lb >> 8 ub = ub >> 8 lb_r.append(lb & 0xff) ub_r.append(ub & 0xff) si_lb = None si_ub = None for b in lb_r: if si_lb is None: si_lb = b else: si_lb <<= 8 si_lb |= b for b in ub_r: if si_ub is None: si_ub = b else: si_ub <<= 8 si_ub |= b si = StridedInterval(bits=o.bits, lower_bound=si_lb, upper_bound=si_ub, stride=o._stride, uninitialized=o.uninitialized) si._reversed = o._reversed if not o.is_integer: logger.warning('Reversing a real strided-interval %s is bad', self) return si"}
{"text_id": "9946", "text": "docstring: def plot_fwd_flow_density(model, ax, test_grid, n_pts, batch_size): xx, yy, zz = test_grid data = zz.reshape(-1, 2) n_batches = data.shape[0] // batch_size probs = [] for b in range(n_batches): batch = data[b*batch_size:(b+1)*batch_size] log_prob = model.log_prob(batch).cpu().double() prob = torch.exp(log_prob).detach() probs.append(prob) probs = torch.cat(probs) ax.pcolormesh(xx, yy, probs.reshape((n_pts,n_pts))) ax.set_facecolor(plt.cm.jet(0.)) ax.set_title('Predicted density')"}
{"text_id": "9947", "text": "docstring: def RunCommand(self): if not HAVE_OPENSSL: raise CommandException( 'The signurl command requires the pyopenssl library (try pip ' 'install pyopenssl or easy_install pyopenssl)') method, expiration, content_type, passwd = self._ParseAndCheckSubOpts() storage_urls = self._EnumerateStorageUrls(self.args[1:]) key = None client_email = None try: key, client_email = _ReadJSONKeystore(open(self.args[0], 'rb').read(), passwd) except ValueError: if not passwd: passwd = getpass.getpass('Keystore password:') try: key, client_email = _ReadKeystore( open(self.args[0], 'rb').read(), passwd) except ValueError: raise CommandException('Unable to parse private key from {0}'.format( self.args[0])) print 'URL\\tHTTP Method\\tExpiration\\tSigned URL' for url in storage_urls: if url.scheme != 'gs': raise CommandException('Can only create signed urls from gs:// urls') if url.IsBucket(): gcs_path = url.bucket_name if method == 'RESUMABLE': raise CommandException('Resumable signed URLs require an object ' 'name.') else: gcs_path = '{0}/{1}'.format(url.bucket_name, urllib.quote(url.object_name.encode(UTF8))) final_url = _GenSignedUrl(key, client_email, method, '', content_type, expiration, gcs_path, self.logger) expiration_dt = datetime.fromtimestamp(expiration) print '{0}\\t{1}\\t{2}\\t{3}'.format(url.url_string.encode(UTF8), method, (expiration_dt .strftime('%Y-%m-%d %H:%M:%S')), final_url.encode(UTF8)) response_code = self._ProbeObjectAccessWithClient( key, client_email, gcs_path, self.logger) if response_code == 404: if url.IsBucket() and method != 'PUT': raise CommandException( 'Bucket {0} does not exist. Please create a bucket with ' 'that name before a creating signed URL to access it.' .format(url)) else: if method != 'PUT' and method != 'RESUMABLE': raise CommandException( 'Object {0} does not exist. Please create/upload an object ' 'with that name before a creating signed URL to access it.' .format(url)) elif response_code == 403: self.logger.warn( '%s does not have permissions on %s, using this link will likely ' 'result in a 403 error until at least READ permissions are granted', client_email, url) return 0"}
{"text_id": "9948", "text": "docstring: def prettyPrint(self, thing): import pprint; pp = pprint.PrettyPrinter(indent=4) pp.pprint(thing)"}
{"text_id": "9949", "text": "docstring: def append_rpath_within_wheel( lib_name: str, rpath: str, wheel_base_dir: str, patcher: ElfPatcher ) -> None: if not isabs(lib_name): lib_name = abspath(lib_name) lib_dir = dirname(lib_name) if not isabs(wheel_base_dir): wheel_base_dir = abspath(wheel_base_dir) def is_valid_rpath(rpath: str) -> bool: return _is_valid_rpath(rpath, lib_dir, wheel_base_dir) old_rpaths = patcher.get_rpath(lib_name) rpaths = filter(is_valid_rpath, old_rpaths.split(\":\")) rpath_set = OrderedDict([(old_rpath, \"\") for old_rpath in rpaths]) rpath_set[rpath] = \"\" patcher.set_rpath(lib_name, \":\".join(rpath_set))"}
{"text_id": "9950", "text": "docstring: def read_csv_file(file_path): csv_iterator = pd.read_csv(file_path) num_rows, num_cols = csv_iterator.shape print(f\"the number of rows found in file: {num_rows}\") print(\"column headings from raw dataset: \", list(csv_iterator.columns.values)) return csv_iterator"}
{"text_id": "9951", "text": "docstring: def run(self,Input): x0 = Input.get('x0',0.0) y0 = Input.get('y0',0.0) v0 = Input.get('v0',1.0) ang = Input.get('angle',45.)*np.pi/180. self.x0 = x0 self.y0 = y0 self.v0 = v0 self.ang = ang ts = np.linspace(0,0.1,10) vx0 = np.cos(ang)*v0 vy0 = np.sin(ang)*v0 r = prange(v0,ang,y0) self.x = np.zeros(len(ts)) self.y = np.zeros(len(ts)) self.r = np.zeros(len(ts)) for i,t in enumerate(ts): self.x[i] = x_pos(x0,vx0,t) self.y[i] = y_pos(y0,vy0,t) self.r[i] = r self.time = ts"}
{"text_id": "9952", "text": "docstring: def train_model_pipeline(self, input_pipeline, training_epochs= 100, verbose= True, verbosity_ival= 1, excessive= False): self.input_pipeline = input_pipeline if not self.model_built: raise AttributeError(\"The computation graph must be built before the model\"\\ \" can be trained\") if not self.input_is_pipeline: raise AttributeError(\"Model was constructed to accept locally-stored data,\"\\ \"either use 'train_model' method or rebuild model \"\\ \"with the 'build_model_pipeline' method.\") with tf.compat.v1.Session(graph= self.graph) as sess: sess.run(self.init) if verbose: print(\"Model initialised\") print() for epoch in range(training_epochs): count = 0 run_loss = 0 for feed_data in input_pipeline: if self.additional_data is None: if not isinstance(feed_data, pd.DataFrame): raise TypeError(\"Input data must be in a DataFrame\") na_loc = feed_data.notnull().astype(bool).values feedin = {self.X: feed_data.values, self.na_idx: na_loc} else: if not isinstance(feed_data, list): raise TypeError(\"Input should be a list of two DataFrames, with \"\\ \"index 0 containing the target imputation data, and\"\\ \" the data at index 1 containing additional data\") if len(feed_data) != 2: raise TypeError(\"Input should be a list of two DataFrames, with \"\\ \"index 0 containing the target imputation data, and\"\\ \" the data at index 1 containing additional data\") if not isinstance(feed_data[0], pd.DataFrame): raise TypeError(\"Input data must be in a DataFrame\") if not isinstance(feed_data[1], pd.DataFrame): raise TypeError(\"Additional data must be in a DataFrame\") na_loc = feed_data[0].notnull().astype(bool).values feedin = {self.X: feed_data[0].fillna(0).values, self.X_add: feed_data[1].fillna(0).values, self.na_idx: na_loc} if np.sum(na_loc) == 0: continue loss, _ = sess.run([self.joint_loss, self.train_step], feed_dict= feedin) if excessive: print(\"Current cost:\", loss) count +=1 if not np.isnan(loss): run_loss += loss if verbose: if epoch % verbosity_ival == 0: print('Epoch:', epoch, \", loss:\", str(run_loss/count)) if verbose: print(\"Training complete. Saving file...\") save_path = self.saver.save(sess, self.savepath) if verbose: print(\"Model saved in file: %s\" % save_path) return self"}
{"text_id": "9953", "text": "docstring: def E_Advective_Dispersion(t, Pe): if isinstance(t, list): t[t == 0] = 10**(-50) return (Pe/(4*np.pi*t))**(0.5)*np.exp((-Pe*((1-t)**2))/(4*t))"}
{"text_id": "9954", "text": "docstring: def _string_image(string, font_path=None): grayscale = 'L' lines = string.split('\\\\n') large_font = 1000 font_path = font_path or _DEFAULT_FONT try: font = PIL.ImageFont.truetype(font_path, size=large_font) except IOError: font = None if font is None: if font_path == _DEFAULT_FONT: raise RuntimeError('Unable to load built-in font ({})'.format(_DEFAULT_FONT)) else: raise ValueError('Unable to load provided font ({})'.format(font_path)) pt2px = lambda pt: int(round(pt * 96.0 / 72)) max_width_line = max(lines, key=lambda s: font.getsize(s)[0]) test_string = 'abcdefghijklmnopqrstuvwxyz' max_height = pt2px(font.getsize(test_string)[1]) max_width = pt2px(font.getsize(max_width_line)[0]) height = max_height * len(lines) width = int(round(max_width + 40)) image = PIL.Image.new(grayscale, (width, height), color=_PIXEL_OFF) draw = PIL.ImageDraw.Draw(image) vertical_position = 5 horizontal_position = 5 line_spacing = int(round(max_height * 0.65)) for line in lines: draw.text((horizontal_position, vertical_position), line, fill=_PIXEL_ON, font=font) vertical_position += line_spacing c_box = PIL.ImageOps.invert(image).getbbox() image = image.crop(c_box) return image"}
{"text_id": "9955", "text": "docstring: def make_mask_colors(n_parts, cmap=plt.cm.inferno): colors = cmap(np.linspace(0, 1, n_parts), alpha=False, bytes=False)[:, :3] return colors"}
{"text_id": "9956", "text": "docstring: def update_adjusted_run_time_from_event(self): _LOGGER.info(\"updated_adjusted_run_time_from_event called.\") result = self.calculate_water_budget_and_adjusted_run_time( self.bucket, self.type ) _LOGGER.info( \"updated_adjusted_run_time_from_event: got result: {}. Setting attributes of daily adjusted run time (including result['wb']) and state == result['art']\".format( result ) ) art_entity_id = self.coordinator.entities[TYPE_ADJUSTED_RUN_TIME] attr = self.get_attributes_for_daily_adjusted_run_time( self.bucket, result[\"wb\"], result[\"art\"] ) self.hass.states.set( art_entity_id, result[\"art\"], attr, ) sun_state = self.hass.states.get(\"sun.sun\") if sun_state is not None: sun_rise = sun_state.attributes.get(\"next_rising\") if sun_rise is not None: try: sun_rise = datetime.datetime.strptime(sun_rise, \"%Y-%m-%dT%H:%M:%S.%f%z\") except(ValueError): sun_rise = datetime.datetime.strptime(sun_rise, \"%Y-%m-%dT%H:%M:%S%z\") _LOGGER.info(\"sun_rise: {}\".format(sun_rise)) async_track_point_in_time( self.hass, self._fire_start_event, point_in_time=sun_rise ) time_to_fire = sun_rise - datetime.timedelta(seconds=result[\"art\"]) event_to_fire = f\"{self.coordinator.name}_{EVENT_IRRIGATE_START}\" _LOGGER.info(\"{} will fire at {}\".format(event_to_fire, time_to_fire))"}
{"text_id": "9957", "text": "docstring: def to_dict(self) -> Dict: _dict = {} if hasattr(self, 'collection_type') and self.collection_type is not None: _dict['collection_type'] = self.collection_type if hasattr(self, 'collection_total') and self.collection_total is not None: _dict['collection_total'] = self.collection_total return _dict"}
{"text_id": "9958", "text": "docstring: def sync(self): available = self.count sleep(0.0005) if available > 0: available = available + 2 buf = self.read_keypad(available) for raw in buf: evt = KeyEvent(_seesaw_key((raw >> 2) & 0x3F), raw & 0x3) if ( evt.number < _NEO_TRELLIS_NUM_KEYS and self.callbacks[evt.number] is not None ): self.callbacks[evt.number](evt)"}
{"text_id": "9959", "text": "docstring: def step(self, actions): self.current_task = self.df_tasks.loc[self.current_task_id] self.current_time_slot = int( self.df_tasks.loc[self.current_task_id, \"arrive_time\"]) self.next_time_slot = int( self.df_tasks.loc[self.current_task_id + 1, \"arrive_time\"]) if self.verbose: print(f\"current time slot = {self.current_time_slot}\") print(f\"next task's time slot = {self.next_time_slot}\") bids_list = [] max_usage_time_list = [] start_time_list = [] relative_start_time_list = [] for node_id, action in enumerate(actions): max_usage_time, relative_start_time = self.find_max_usage_time( node_id) start_time = int( self.df_tasks.loc[ self.current_task_id, 'arrive_time'] + relative_start_time + 1) bids_list.append(action * self.df_tasks.loc[ self.current_task_id, 'valuation_coefficient']) max_usage_time_list.append(max_usage_time) start_time_list.append(start_time) relative_start_time_list.append(relative_start_time) if self.verbose: print(\"bid prices:\") print(bids_list) print(\"max usage times:\") print(max_usage_time_list) print(\"start times:\") print(start_time_list) print(\"relative start times:\") print(relative_start_time_list) (winner_index, winner_usage_time, winner_utility, max_utility, sw_increase) = self.reverse_auction(bids_list, max_usage_time_list, start_time_list, verbose=self.verbose, auction_type=self.auction_type) self.winner_id = winner_index if self.verbose: print(f\"winner ID = {self.winner_id}\") if winner_index is not None: winner_start_time = start_time_list[winner_index] winner_relative_start_time = relative_start_time_list[winner_index] winner_finish_time = (winner_start_time + winner_usage_time - 1) if winner_usage_time is not None and winner_usage_time > 0: self.allocation_scheme.loc[self.current_task_id] = [ winner_index, winner_start_time, winner_finish_time] else: self.allocation_scheme.loc[self.current_task_id] = [None, None, None] self.update_resource_occupency(winner_index, winner_usage_time, winner_relative_start_time) if self.verbose: print(f\"allocation scheme:\") print(f\"{self.allocation_scheme.loc[self.current_task_id]}\") print(\"idle resource capacities of winner node:\") print(self.idle_resource_capacities[self.winner_id][:, 0:5]) self.future_occup = ( 1 - np.divide(self.idle_resource_capacities[:, :, self.next_time_slot + 1: (self.next_time_slot + 11)], self.full_resource_capacities[:, :, self.next_time_slot + 1: (self.next_time_slot + 11)])) future_occup_len = len(self.future_occup[0][0]) if future_occup_len < 10: z = np.zeros((self.n_nodes, 3, 10 - future_occup_len)) self.future_occup = np.concatenate((self.future_occup, z), axis=2) if self.verbose: print(\"occupancy of future 10 time steps:\") print(self.future_occup) self.current_task_value = self.current_task['valuation_coefficient'] * \\ self.current_task['usage_time'] if self.verbose: print(f\"current task ID = {self.current_task_id}\") print(f\"current task's value = {self.current_task_value}\") print(\"current task info:\") print(self.current_task) self.state = [] self.rewards = [] self.current_task_id += 1 self.rewards.append(sw_increase - self.current_task_value) if self.current_task_id >= self.max_steps: done = True task_info = None self.state = None else: done = False task_info = self.df_tasks_normalised.iloc[ self.current_task_id].to_numpy() for i in range(self.n_nodes): future_occup = self.future_occup[i].flatten(order=\"F\") agent_state = np.concatenate((task_info, future_occup), axis=None) self.state.append(agent_state) self.sw_increase = sw_increase if self.verbose: print(\"tasks normalised:\") print(self.df_tasks_normalised.head()) print(f\"next task ID = {self.current_task_id}\") print(\"The info of the next task\") print(task_info) print(\"social welfare increase\") print(self.sw_increase) print(\"next global observation\") print(self.state) print(f\"rewards for agents = {self.rewards}\") print(f\"Is the episode over? {done}\") print(\"\\n\\n\") self.total_social_welfare += sw_increase return self.state, self.rewards, done, sw_increase"}
{"text_id": "9960", "text": "docstring: def config_salt(): process = subprocess.Popen(['salt-call', '--local', 'tls.create_self_signed_cert']) process.wait() with open('/etc/salt/master', 'w') as _file: yaml.dump(SALT_MASTER_DEFAULT_CONFIG, _file, default_flow_style=False) create_api_user()"}
{"text_id": "9961", "text": "docstring: def s3_get_foreign_key(field, m2m=True): ftype = str(field.type) multiple = False if ftype[:9] == \"reference\": key = ftype[10:] elif m2m and ftype[:14] == \"list:reference\": key = ftype[15:] multiple = True else: key = current.s3db.virtual_reference(field) if not key: return (None, None, None) if \".\" in key: rtablename, key = key.split(\".\") else: rtablename = key rtable = current.s3db.table(rtablename) if rtable: key = rtable._id.name else: key = None return (rtablename, key, multiple)"}
{"text_id": "9962", "text": "docstring: def verify_with_ccomp( ccomp: str, file: Path, flags: str, compcert_timeout: int ) -> bool: with CCompEnv() as tmpdir: cmd = [ ccomp, str(file), \"-interp\", \"-fall\", ] if flags: cmd.extend(flags.split()) res = True try: utils.run_cmd( cmd, additional_env={\"TMPDIR\": str(tmpdir)}, timeout=compcert_timeout, ) res = True except subprocess.CalledProcessError: res = False except subprocess.TimeoutExpired: res = False logging.debug(f\"CComp returncode {res}\") return res"}
{"text_id": "9963", "text": "docstring: def parse_backup_edge_pool_opt_per_az(az): edge_pool_opts = az.backup_edge_pool res = [] for edge_pool_def in edge_pool_opts: split = edge_pool_def.split(':') try: (edge_type, edge_size, minimum_pooled_edges, maximum_pooled_edges) = split[:4] except ValueError: raise n_exc.Invalid(_(\"Invalid edge pool format for availability\" \" zone %s\") % az.name) if edge_type not in vcns_const.ALLOWED_EDGE_TYPES: msg = (_(\"edge type '%(edge_type)s' is not allowed, \" \"allowed types: %(allowed)s for availability zone \" \"%(name)s\") % {'edge_type': edge_type, 'allowed': vcns_const.ALLOWED_EDGE_TYPES, 'name': az.name}) LOG.error(msg) raise n_exc.Invalid(msg) edge_size = edge_size or nsxv_constants.COMPACT if edge_size not in vcns_const.ALLOWED_EDGE_SIZES: msg = (_(\"edge size '%(edge_size)s' is not allowed, \" \"allowed types: %(allowed)s for availability zone \" \"%(name)s\") % {'edge_type': edge_size, 'allowed': vcns_const.ALLOWED_EDGE_SIZES, 'name': az.name}) LOG.error(msg) raise n_exc.Invalid(msg) res.append({'edge_type': edge_type, 'edge_size': edge_size, 'minimum_pooled_edges': int(minimum_pooled_edges), 'maximum_pooled_edges': int(maximum_pooled_edges)}) edge_pool_dicts = {} for edge_type in vcns_const.ALLOWED_EDGE_TYPES: edge_pool_dicts[edge_type] = {} for r in res: edge_pool_dict = edge_pool_dicts[r['edge_type']] if r['edge_size'] in edge_pool_dict.keys(): raise n_exc.Invalid(_(\"Duplicate edge pool configuration for \" \"availability zone %s\") % az.name) else: edge_pool_dict[r['edge_size']] = { 'minimum_pooled_edges': r['minimum_pooled_edges'], 'maximum_pooled_edges': r['maximum_pooled_edges']} return edge_pool_dicts"}
{"text_id": "9964", "text": "docstring: def bind_location( self, poi_id, device_id=None, uuid=None, major=None, minor=None): data = { \"poi_id\": poi_id } if device_id: data[\"device_identifier\"] = { \"device_id\": device_id } else: data[\"device_identifier\"] = { \"uuid\": uuid, \"major\": major, \"minor\": minor } url = 'https://api.weixin.qq.com/shakearound/device/bindlocation' json_data = self._send_request('post', url, data=data) return json_data"}
{"text_id": "9965", "text": "docstring: def handle_log_line(self, log_line): self._text_box.append(log_line)"}
{"text_id": "9966", "text": "docstring: def _serialize_input( self, data: Union[ Dict, str, List, np.ndarray, pd.core.series.Series, pd.core.frame.DataFrame, \"torch.Tensor\", ], ): try: data_type = type(data) if isinstance(data, torch.Tensor): buffer = BytesIO() torch.save(data, buffer) return { \"data\": base64.b64encode(buffer.getvalue()).decode(\"utf-8\"), \"data_type\": str(data_type), } return _serialize_input_helper(data, data_type=data_type) except: raise TypeError( \"The supported data types are Dict, str, list, \" \"numpy.ndarray, pd.core.series.Series, \" \"pd.core.frame.DataFrame, torch.Tensor. Please \" \"convert to the supported data types first. \" )"}
{"text_id": "9967", "text": "docstring: def wait_key(message = ''): if message != '': print (message) result = None if os.name == 'nt': import msvcrt result = msvcrt.getch() else: import termios fd = sys.stdin.fileno() oldterm = termios.tcgetattr(fd) newattr = termios.tcgetattr(fd) newattr[3] = newattr[3] & ~termios.ICANON & ~termios.ECHO termios.tcsetattr(fd, termios.TCSANOW, newattr) try: result = sys.stdin.read(1) except IOError: pass finally: termios.tcsetattr(fd, termios.TCSAFLUSH, oldterm) return result"}
{"text_id": "9968", "text": "docstring: def _bulk_fill_hprobs(self, array_to_fill, layout, pr_array_to_fill, deriv1_array_to_fill, deriv2_array_to_fill): blkSize1 = layout.param_dimension_blk_sizes[0] blkSize2 = layout.param_dimension_blk_sizes[1] atom_resource_alloc = layout.resource_alloc('atom-processing') param_resource_alloc = layout.resource_alloc('param-processing') param2_resource_alloc = layout.resource_alloc('param2-processing') atom_resource_alloc.host_comm_barrier() host_param_slice = None host_param2_slice = None global_param_slice = layout.global_param_slice global_param2_slice = layout.global_param2_slice for atom in layout.atoms: if pr_array_to_fill is not None: self._bulk_fill_probs_atom(pr_array_to_fill[atom.element_slice], atom, atom_resource_alloc) if blkSize1 is None and blkSize2 is None: if deriv1_array_to_fill is not None: self._bulk_fill_dprobs_atom(deriv1_array_to_fill[atom.element_slice, :], host_param_slice, atom, global_param_slice, param_resource_alloc) if deriv2_array_to_fill is not None: if deriv1_array_to_fill is not None and global_param_slice == global_param2_slice: deriv2_array_to_fill[atom.element_slice, :] = deriv1_array_to_fill[atom.element_slice, :] else: self._bulk_fill_dprobs_atom(deriv2_array_to_fill[atom.element_slice, :], host_param2_slice, atom, global_param2_slice, param2_resource_alloc) self._bulk_fill_hprobs_atom(array_to_fill[atom.element_slice, :, :], host_param_slice, host_param2_slice, atom, global_param_slice, global_param2_slice, param2_resource_alloc) else: assert(blkSize1 is not None and blkSize2 is not None), \\ \"Both (or neither) of the Hessian block sizes must be specified!\" Np1 = _slct.length(global_param_slice) Np2 = _slct.length(global_param2_slice) nBlks1 = int(_np.ceil(Np1 / blkSize1)) nBlks2 = int(_np.ceil(Np2 / blkSize2)) blocks1 = _mpit.slice_up_range(Np1, nBlks1) blocks2 = _mpit.slice_up_range(Np2, nBlks2) for block1 in blocks1: host_param_slice_part = block1 global_param_slice_part = _slct.shift(block1, global_param_slice.start) if deriv1_array_to_fill is not None: self._bulk_fill_dprobs_atom(deriv1_array_to_fill[atom.element_slice, :], host_param_slice_part, atom, global_param_slice_part, param_resource_alloc) for block2 in blocks2: host_param2_slice_part = block2 global_param2_slice_part = _slct.shift(block2, global_param2_slice.start) self._bulk_fill_hprobs_atom(array_to_fill[atom.element_slice, :], host_param_slice_part, host_param2_slice_part, atom, global_param_slice_part, global_param2_slice_part, param2_resource_alloc) if deriv2_array_to_fill is not None: if deriv1_array_to_fill is not None and global_param_slice == global_param2_slice: deriv2_array_to_fill[atom.element_slice, :] = deriv1_array_to_fill[atom.element_slice, :] else: for block2 in blocks2: host_param2_slice_part = block2 global_param2_slice_part = _slct.shift(block2, global_param2_slice.start) self._bulk_fill_dprobs_atom(deriv2_array_to_fill[atom.element_slice, :], host_param2_slice_part, atom, global_param2_slice_part, param_resource_alloc) atom_resource_alloc.host_comm_barrier()"}
{"text_id": "9969", "text": "docstring: def send_confirmation_block_notifications(*, payload, sender_account_number, recipient_account_numbers): channel_layer = channels.layers.get_channel_layer() for account_number in (*recipient_account_numbers, sender_account_number): async_to_sync(channel_layer.group_send)( ConfirmationBlockConsumer.group_name(account_number), { 'type': 'send.confirmation.block', 'message': standardize_notification( notification_type=CONFIRMATION_BLOCK_NOTIFICATION, payload=payload ) } )"}
{"text_id": "9970", "text": "docstring: def fresnel(n1, n2, k1x, k2x): r12_s = (k1x - k2x) / (k1x + k2x) t12_s = 2*k1x / (k1x + k2x) r12_p = (n1*n1*k2x - n2*n2*k1x) / (n1*n1*k2x + n2*n2*k1x) t12_p = (2*n1*n1*k2x) / (n1*n1*k2x + n2*n2*k1x) return r12_s, t12_s, r12_p, t12_p"}
{"text_id": "9971", "text": "docstring: def add_validation_info(self, epoch: int, accuracy: float): self.validation_results = self.validation_results.append({\"Epoch\":epoch, \"Accuracy\":accuracy}, ignore_index=True)"}
{"text_id": "9972", "text": "docstring: def _replace_template_variable(self, template, variable_name, variable_value): return_template = template.replace('{{' + variable_name + '}}', variable_value) return return_template"}
{"text_id": "9973", "text": "docstring: def create_slave(self, kwargs): self.logger.debug('master in create_slave ') ret = self._take_snapshot() for position in ret.keys(): master_log_file = ret[position]['binfile'] master_log_pos = ret[position]['position'] mysqldump_path = ret[position]['mysqldump_path'] try: kwargs = self._slave_get_params(kwargs) for key in kwargs: slave = kwargs[key] self._register_slave(str(slave['ip'])) from conpaas.services.mysql.agent import client client.setup_slave(str(slave['ip']), slave['port'], key, self.my_ip, master_log_file, master_log_pos, mysqldump_path) self.logger.debug('Created slave %s' % str(slave['ip'])) return HttpJsonResponse() except AgentException as e: return HttpErrorResponse(e.message)"}
{"text_id": "9974", "text": "docstring: def filter_user(user_ref): if user_ref: user_ref = user_ref.copy() user_ref.pop('password', None) user_ref.pop('tenants', None) user_ref.pop('groups', None) user_ref.pop('domains', None) try: user_ref['extra'].pop('password', None) user_ref['extra'].pop('tenants', None) except KeyError: pass return user_ref"}
{"text_id": "9975", "text": "docstring: def close_block(self, blank_line=True, **kwargs): self.set_indent(offset=-1) if blank_line: self.writeln(**kwargs) return"}
{"text_id": "9976", "text": "docstring: def measure( self, func=None, disp2=None, multiple_results=False, trigger=True, **mode_params ): if disp2 is not None: if disp2.upper() not in [\"TH\", \"Q\", \"D\"]: raise ParameterError(\"Display 2 must be either 'TH', 'Q' or 'D'\") self._write(\"DISP2 {}\".format(disp2)) if func is not None: if func.upper() not in [\"L\", \"C\", \"R\", \"Z\", \"ESR\"]: raise ParameterError(\"Func must be either 'L', 'C', 'R', 'Z' or 'ESR'\") self._write(\"FUNC {}\".format(func)) return self._read_measurement(multiple_results)"}
{"text_id": "9977", "text": "docstring: def copyToHdfs(self, fileobj, destination): src = FileObjInputStream(fileobj) self.j_smvPyClient.copyToHdfs(src, destination)"}
{"text_id": "9978", "text": "docstring: def parse_hdr_key_group(hdr, prefix='F'): values = {} for k, v in hdr.items(): if k[:len(prefix)] == prefix: try: i = int(k[len(prefix):])-1 except ValueError: continue values[i] = v return [values[i] for i in range(max(values.keys())+1)]"}
{"text_id": "9979", "text": "docstring: def _get_batch_job_manager_class(cls): return InteractionAnswerSummariesMRJobManager"}
{"text_id": "9980", "text": "docstring: def generate_conditions(input_tiff, initial_angle, wave_period,init_wave_height): g = 9.81 initial_angle = np.deg2rad(initial_angle) wave_length = g * (wave_period ** 2) / (2 * np.pi) initial_celerity = wave_length / wave_period gdal_ds = gdal.Open(input_tiff) gdal_band = gdal_ds.GetRasterBand(1) nodataval = gdal_band.GetNoDataValue() grid_size = 50 data_array = rot270(gdal_ds.ReadAsArray().astype(np.float)) rows, cols = data_array.shape data_array = data_array[ 0 : (rows // grid_size * grid_size), 0 : (rows // grid_size * grid_size) ] if np.any(data_array == nodataval): data_array[data_array == nodataval] = np.nan out_full_path, file_name = generate_output_name(\"png\") fig = plt.figure(figsize=(25, 20)) plt.show() plt.contour(data_array, cmap=\"viridis\", levels=list(range(-1200, 0, 50))) plt.colorbar() rows, cols = data_array.shape depths = np.average( np.split( np.average( np.split(data_array, math.ceil(cols / grid_size), axis=1), axis=-1 ), math.ceil(rows / grid_size), axis=1, ), axis=-1, ) celerity = np.sqrt(g * depths) wave_directions = np.arcsin((celerity * np.sin(initial_angle)) / initial_celerity) wave_directions = np.where( np.isnan(wave_directions), initial_angle, wave_directions ) plt.show() U = np.sin(wave_directions) V = np.cos(wave_directions) X, Y = np.meshgrid(np.arange(0, rows, grid_size), np.arange(0, cols, grid_size)) q = plt.quiver(X, Y, U, V) plt.quiverkey(q, X=0.3, Y=1.1, U=10, label=\"Quiver key, length = 10\", labelpos=\"E\") plt.gca().set_aspect(\"equal\", adjustable=\"box\") plt.savefig(out_full_path, bbox_inches=\"tight\") plt.show() depth_val = \"Respective Grid Points for the Wave Heights: \" return file_name,depth_val,depths"}
{"text_id": "9981", "text": "docstring: def margin_size(self, value: int) -> None: if not isinstance(value, int): raise TypeError(\"The margin size must be an integer\") margin_spacing = (2 * value) + (2 * self._border_thickness) if margin_spacing >= self.widget_width: raise ValueError( \"The size of the borders and margins combined can total the same or more\" \"than the widget's width.\" ) if margin_spacing >= self.widget_height: raise ValueError( \"The size of the borders and margins combined can total the same or more\" \"than the widget's height.\" ) self._margin_size = value self._set_progress(self._progress)"}
{"text_id": "9982", "text": "docstring: def expand(self, pi_det: typing.Dict[str, typing.Tuple[int, float]])\\ -> typing.Tuple[int, Node]: info_state = self.state.information_state_string() actions_det = [act_prob[0] for act_prob in pi_det[info_state]] actions_det = [ action for action in actions_det if action not in self.children.keys() ] rand_action = np.random.choice(actions_det) self.children[rand_action] = Node(self.state.child(rand_action), self, self.k) return self.children[rand_action]"}
{"text_id": "9983", "text": "docstring: def _pretty_hex(self, data): if data is None: return \"<none>\" if type(data) is int: data = [data] if len(data) == 0: return \"<none>\" if len(data) == 1: value = \"{:02x}\".format(data[0]) if len(value) % 2: value = \"0\" + value return \"0x\" + value return \"[\" + \", \".join(\"0x{:02x}\".format(byte) for byte in data) + \"]\""}
{"text_id": "9984", "text": "docstring: def estimate_bg_elliptical_annulus(cutout, ellipticity=0, r_50=50, pa=0, width=20, factor=10, return_mask=False): a_in = r_50 * factor b_in = a_in * (1 - ellipticity) a_out, b_out = a_in + width, b_in + width bg_mask = make_source_mask(cutout, nsigma=2, npixels=2, dilate_size=11) annulus = EllipticalAnnulus([cutout.shape[0] / 2, cutout.shape[1] / 2], a_in=a_in, a_out=a_out, b_in=b_in, b_out=b_out, theta=pa) annulus_mask = annulus.to_mask(method='center') annulus_mask = annulus_mask.to_image(shape=cutout.shape) annulus_mask[annulus_mask == 0] = np.nan bg_pixels = cutout * annulus_mask bg_pixels[bg_mask] = np.nan mean, median, std = np.nanmean(bg_pixels), np.nanmedian(bg_pixels), np.nanstd(bg_pixels) if return_mask: return mean, median, std, bg_pixels else: return mean, median, std"}
{"text_id": "9985", "text": "docstring: def find_players(self, name): possibilities = [(player, ratio(name, player.full_name)) for player in self._players] filtered_possibilities = filter(lambda pos: pos[1] >= self._search_threshold, possibilities) players = sorted(filtered_possibilities, key=lambda pos: -pos[1]) return list(map(lambda p: p[0], players))"}
{"text_id": "9986", "text": "docstring: def on_batch_end(self, iteration:int, smooth_loss:TensorOrNumber, **kwargs:Any)->None: if iteration==0 or smooth_loss < self.best_loss: self.best_loss = smooth_loss self.opt.lr = self.sched.step() if self.sched.is_done or (self.stop_div and smooth_loss > 4*self.best_loss): self.stop=True return True"}
{"text_id": "9987", "text": "docstring: def _strip_elements(jsn, elements): for tagstack, element in elements: if len(tagstack) == 0: jsn.pop(element.tag) else: current_node = jsn for tag in tagstack: current_node = current_node[tag] current_node.pop(element.tag)"}
{"text_id": "9988", "text": "docstring: def llc_facets_2d_to_compact(facets, extra_metadata): flatdata = np.array([]) for kfacet in range(len(facets)): if facets['facet' + str(kfacet)] is not None: tmp = np.reshape(facets['facet' + str(kfacet)].values, (-1)) flatdata = np.concatenate([flatdata, tmp]) return flatdata"}
{"text_id": "9989", "text": "docstring: def initialize(self, data_mgr, resource_mgr, config_mgr): self.event_mgr = None self.data_mgr = data_mgr self.resource_mgr = resource_mgr self.config_mgr = config_mgr self.host = self.config_mgr.get_platform_parameter('HOST') self.node_alloc_mode = self.config_mgr.get_platform_parameter('NODE_ALLOCATION_MODE') try: self.task_launch_cmd = self.config_mgr.get_platform_parameter('MPIRUN') except Exception: print('Error accessing platform parameter MPIRUN') raise"}
{"text_id": "9990", "text": "docstring: def add_args(parser): parser.add_argument(\"--bias\", action='store_true', help=\"use residual bias\") parser.add_argument(\"--residual\", action='store_true', help=\"use residual connection\") parser.add_argument(\"--in_channels\",default=7, help=\"number of input features\")"}
{"text_id": "9991", "text": "docstring: def testAdd(self): self.stack.add('cats') assert self.stack.remove() == 'cats', 'wrong item on the top of the stack'"}
{"text_id": "9992", "text": "docstring: def browse_edit(): if request.method == 'POST': class _BrowseEditForm(BrowseEditForm): pass deckid = request.form['hidden_deckid_field'] deck = Deck.query.get(deckid) setattr(_BrowseEditForm.deckname, 'default', deck.deckname) browse_edit_form = _BrowseEditForm() class _CardForm(CardForm): pass for card in Card.query.filter_by(deck_id=deckid).all(): _CardForm.term = card.term _CardForm.definition = card.definition browse_edit_form.cards.append_entry(_CardForm()) if browse_edit_form.validate_on_submit: return render_template('browse_edit.html', title='Enter a card', form=browse_edit_form) return redirect(url_for('browse_edit.html'))"}
{"text_id": "9993", "text": "docstring: def ite_dict(i, d, default): i = i.ast if type(i) is ASTCacheKey else i if len(d) < 4: return ite_cases([ (i == c, v) for c,v in d.items() ], default) keys = list(d.keys()) keys.sort() split_val = keys[len(keys)//2] dictLow = {c:v for c,v in d.items() if c <= split_val} dictHigh = {c:v for c,v in d.items() if c > split_val} valLow = ite_dict(i, dictLow, default) valHigh = ite_dict(i, dictHigh, default) return If(i <= split_val, valLow, valHigh)"}
{"text_id": "9994", "text": "docstring: def surface(current_data): from numpy import ma drytol = getattr(current_data.user, 'drytol', drytol_default) q = current_data.q h = q[:,:,0] eta = q[:,:,i_eta] water = ma.masked_where(h<=drytol, eta) return water"}
{"text_id": "9995", "text": "docstring: def make_kernel(trialkernel, T_start, T_stop, L_start, L_stop, coef = 1): L_start = (bin_size/0.005)*L_start L_start = floor(L_start) L_stop = (bin_size/0.005)*L_stop L_stop = ceil(L_stop) kernel_length = L_stop-L_start kernel = np.diag(np.ones(kernel_length))*coef trialkernel[T_start:T_stop, L_start:L_stop] = kernel return trialkernel"}
{"text_id": "9996", "text": "docstring: def init(name=None, http_host=DEFAULT_HTTP_HOST, http_port=DEFAULT_HTTP_PORT, metric_exporter=InMemoryExporter): if name is not None and not isinstance(name, str): raise TypeError(\"name must be a string.\") if not ray.is_initialized(): ray.init() global master_actor master_actor_name = format_actor_name(SERVE_MASTER_NAME, name) try: master_actor = ray.get_actor(master_actor_name) return except ValueError: pass ray.register_custom_serializer(Query, Query.ray_serialize, Query.ray_deserialize) ray.register_custom_serializer(RequestMetadata, RequestMetadata.ray_serialize, RequestMetadata.ray_deserialize) http_node_id = ray.state.current_node_id() master_actor = ServeMaster.options( name=master_actor_name, max_restarts=-1, max_task_retries=-1, ).remote(name, http_node_id, http_host, http_port, metric_exporter) block_until_http_ready( \"http://{}:{}/-/routes\".format(http_host, http_port), timeout=HTTP_PROXY_TIMEOUT)"}
{"text_id": "9997", "text": "docstring: def validate( schema: RawSchema, raw_items: RawItems, keys: pd.Index, fast: bool = False ) -> Result: validate_func = fast_validate if fast else full_validate errors = validate_func(schema, raw_items, keys) result = Result(\"JSON Schema Validation\") err_items = len(set(itertools.chain.from_iterable(errors.values()))) if errors: result.add_error( f\"{err_items} ({err_items/len(list(raw_items)):.0%}) items have {len(errors)} errors\", errors=errors, ) return result"}
{"text_id": "9998", "text": "docstring: def random_weights(nobjs, population_size): weights = [] if nobjs == 2: weights = [[1, 0], [0, 1]] weights.extend([(i/(population_size-1.0), 1.0-i/(population_size-1.0)) for i in range(1, population_size-1)]) else: candidate_weights = [] for i in range(population_size*50): random_values = [random.uniform(0.0, 1.0) for _ in range(nobjs)] candidate_weights.append([x/sum(random_values) for x in random_values]) for i in range(nobjs): weights.append([0]*i + [1] + [0]*(nobjs-i-1)) while len(weights) < population_size: max_index = -1 max_distance = -POSITIVE_INFINITY for i in range(len(candidate_weights)): distance = POSITIVE_INFINITY for j in range(len(weights)): temp = math.sqrt(sum([math.pow(candidate_weights[i][k]-weights[j][k], 2.0) for k in range(nobjs)])) distance = min(distance, temp) if distance > max_distance: max_index = i max_distance = distance weights.append(candidate_weights[max_index]) del candidate_weights[max_index] return weights"}
{"text_id": "9999", "text": "docstring: def syntax_branch_process(parent_node, branch): label = branch.node head = \"=H\" in label or \"-H\" in label tag = label.replace(\"=H\", \"\").replace(\"-H\", \"\") if \"|\" in label: ner = label.split(\"|\")[-1] else: ner = ner_tags.no_ner tag = tag.split(\"|\")[0] order = self.syntax_count new_constituent = self.add_constituent(node_id=\"C{0}\".format(order), sentence=syntactic_root, tag=tag, order=order, label=label) self.set_ner(new_constituent, ner) self.syntax_count += 1 children = [ self.iterate_syntax( syntactic_tree=child, parent=new_constituent, syntactic_root=syntactic_root) for child in branch] children.sort(key=itemgetter(\"ord\")) self.link_syntax_non_terminal(parent=parent_node, child=new_constituent) if head: self.set_head(parent_node, new_constituent) head_word = self.get_head_word(new_constituent) content_text = self.expand_node(children) new_constituent[\"tree\"] = branch new_constituent[\"label\"] = (\" | \".join((content_text, tag))) new_constituent[\"lemma\"] = content_text new_constituent[\"form\"] = content_text new_constituent[\"doc_type\"] = self.doc_type new_constituent[\"utterance\"] = head_word[\"utterance\"] new_constituent[\"quoted\"] = head_word[\"quoted\"] new_constituent[\"begin\"] = children[0][\"begin\"] new_constituent[\"end\"] = children[-1][\"end\"] new_constituent[\"ord\"] = (children[0][\"span\"][0], children[-1][\"span\"][1]) new_constituent[\"span\"] = new_constituent[\"ord\"] if constituent_tags.ner_constituent(tag): self.add_mention_of_named_entity(sentence=syntactic_root, mention=new_constituent) new_constituent[\"constituent\"] = new_constituent return new_constituent"}
